<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>论文笔记 | Attention Is All You Need</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=Roboto+Mono&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #005f73;
            --secondary-color: #0a9396;
            --background-color: #f8f9fa;
            --text-color: #212529;
            --heading-font: 'Noto Sans SC', sans-serif;
            --body-font: 'Noto Sans SC', sans-serif;
            --code-font: 'Roboto Mono', monospace;
            --border-color: #dee2e6;
            --card-bg: #ffffff;
            --shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        }

        body {
            font-family: var(--body-font);
            line-height: 1.8;
            color: var(--text-color);
            background-color: var(--background-color);
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 850px;
            margin: 0 auto;
            background-color: var(--card-bg);
            padding: 30px 50px;
            border-radius: 12px;
            box-shadow: var(--shadow);
        }

        .paper-title {
            font-family: var(--heading-font);
            font-weight: 700;
            font-size: 2.2em;
            color: var(--primary-color);
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 5px;
        }

        .paper-subtitle {
            font-family: var(--heading-font);
            font-size: 1.2em;
            color: #6c757d;
            margin-top: 0;
            margin-bottom: 40px;
        }

        h2 {
            font-family: var(--heading-font);
            font-size: 1.8em;
            color: var(--primary-color);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 8px;
            margin-top: 50px;
        }

        h3 {
            font-family: var(--heading-font);
            font-size: 1.4em;
            color: var(--secondary-color);
            margin-top: 30px;
        }

        h4 {
            font-family: var(--heading-font);
            font-size: 1em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        h5 {
            font-family: var(--heading-font);
            font-size: 1.0em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        p, li {
            font-size: 1em;
            text-align: justify;
        }
        
        strong {
            color: var(--primary-color);
        }

        /* 重点总结区域样式 */
        .summary-box {
            background-color: #eef7f8;
            border-left: 5px solid var(--secondary-color);
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .summary-box h3 {
            margin-top: 0;
            color: var(--primary-color);
        }

        .summary-box ul {
            padding-left: 20px;
        }

        .summary-box li {
            margin-bottom: 10px;
        }

        /* 代码/实现解析区域样式 */
        .code-analysis pre {
            background-color: #282c34;
            color: #abb2bf;
            font-family: var(--code-font);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.5;
            font-size: 0.9em;
        }
        
        .code-analysis code .token{
            color: #61afef; /* 蓝色，用于变量/标记 */
        }
        .code-analysis code .comment{
            color: #5c6370; /* 灰色，用于注释 */
        }
        .code-analysis code .value{
            color: #98c379; /* 绿色，用于值 */
        }

        /* 图像和表格说明文字样式 */
        .figure-caption, .table-caption {
            background-color: #f1f3f5;
            border: 1px solid var(--border-color);
            padding: 15px;
            margin: 20px auto;
            border-radius: 8px;
            font-style: italic;
            color: #495057;
            text-align: center;
        }
        .figure-caption img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .img-small {
            width: 30%; /* 图片宽度为容器的 30% */
            max-width: 100%; /* 安全措施，确保不会超出 */
        }

        .img-medium {
            width: 50%; /* 图片宽度为容器的 50% */
            max-width: 100%;
        }

        .img-large {
            width: 75%; /* 图片宽度为容器的 75% */
            max-width: 100%;
        }

        blockquote {
            border-left: 4px solid var(--border-color);
            padding-left: 20px;
            color: #6c757d;
            margin-left: 0;
        }

        .appendix-box {
            background-color: #f1f3f5; /* 浅灰色背景 */
            border-left: 5px solid #adb5bd; /* 中性灰色边框 */
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .appendix-box h3 {
            margin-top: 0;
            color: #495057; /* 深灰色标题，以在灰色背景上保持清晰 */
        }

        .data-table {
            width: 100%;
            border-collapse: collapse; /* 合并边框 */
            margin: 30px 0;
            font-size: 0.95em;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden; /* 确保圆角生效 */
        }

        .data-table caption {
            caption-side: bottom; /* 将标题放在表格下方，与图片说明保持一致 */
            margin-top: 10px;    /* 与表格的间距 */
            padding: 5px;
            font-size: 0.9em;
            font-style: italic;
            color: #6c757d;      /* 使用柔和的灰色 */
            text-align: center;
        }

        .data-table thead tr {
            background-color: var(--primary-color);
            color: #ffffff;
            text-align: left;
            font-weight: bold;
        }

        .data-table th, .data-table td {
            padding: 12px 15px;
            border-bottom: 1px solid var(--border-color);
        }

        .data-table tbody tr {
            background-color: #ffffff;
        }

        /* 斑马条纹，增加可读性 */
        .data-table tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }

        .data-table tbody tr:last-of-type {
            border-bottom: 2px solid var(--primary-color);
        }

        /* 任务组列加粗，突出显示 */
        .data-table td:first-child {
            font-weight: bold;
            color: var(--primary-color);
        }
    </style>
</head>
<body>

    <div class="container">
        
        <h1 class="paper-title">Attention Is All You Need</h1>
        <p class="paper-subtitle"></p>

        <!-- ====================================================================== -->
        <!-- PART 2: 论文重点总结 (Key Takeaways)                                   -->
        <!-- ====================================================================== -->
        <!-- 这是我为你总结的部分。未来你可以将自己的总结放在这里。 -->
        <section id="summary">
            <h2>论文重点总结</h2>
            <div class="summary-box">
                <h3>核心思想与贡献</h3>
                <p>RT-2的核心贡献是提出了一种<strong>视觉-语言-动作 (VLA)</strong> 模型，它通过一种简单而强大的方法，将预训练的视觉语言模型 (VLM) 的知识直接用于端到端的机器人底层控制。</p>
                <ul>
                    <li><strong>动作即文本 (Actions as Text):</strong> 论文最关键的创新是将机器人的连续动作（如末端执行器位移）离散化，并表示为文本标记 (text tokens)。这使得机器人动作可以和自然语言一样，被VLM直接处理和生成。</li>
                    <li><strong>知识迁移与涌现能力 (Knowledge Transfer & Emergent Abilities):</strong> 通过在海量网络数据和机器人轨迹数据上进行“协同微调”(co-fine-tuning)，RT-2 不仅学会了机器人技能，还继承了VLM的语义理解、推理和泛化能力。这使得机器人能够执行训练数据中从未见过的、需要语义和逻辑推理的任务（例如，将物品放到特定图标上，或拿起“临时锤子”）。</li>
                    <li><strong>端到端控制 (End-to-End Control):</strong> 与之前将LLM用作高级规划器的方法不同，RT-2直接输出底层的机器人动作指令，实现了从像素到动作 (pixels-to-action) 的端到端控制，同时享受大规模预训练的红利。</li>
                    <li><strong>性能验证:</strong> 在超过6000次的真实机器人实验中，RT-2在对新物体、新场景的泛化能力上，相比RT-1等基线模型实现了约2倍的性能提升，并展示了符号理解、关系推理和多步规划等多种涌现能力。</li>
                </ul>
            </div>
        </section>

        

        <!-- ====================================================================== -->
        <!-- PART 1: 论文全文翻译 (Full Translation)                                -->
        <!-- ====================================================================== -->
        <!-- 这是你提供的笔记内容，我已经帮你格式化好了。 -->
        <section id="translation">
            <h2>论文全文翻译</h2>

            <h3>Abstract</h3>
            <p>主流的序列转导模型都是基于复杂的循环或卷积神经网络，包含一个编码器和一个解码器。表现最好的模型还通过一个注意力机制来连接编码器和解码器。<strong>我们提出了一种新的、简单的网络架构——Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。</strong>在两个机器翻译任务上的实验表明，这些模型在质量上更优，同时具有更好的并行性，并且训练所需的时间显著减少。在 WMT 2014 英德翻译任务中，我们的模型取得了 28.4 的 BLEU 值，比以往包括集成模型在内的最佳结果还要高出 2.0 BLEU。在 WMT 2014 英法翻译任务中，我们的模型在八个 GPU 上训练了 3.5 天后，便创下了单一模型下 41.8 的 BLEU 值新纪录，而这只是文献中最佳模型训练成本的一小部分。我们还证明了 Transformer 能够很好地泛化到其他任务，并成功地将其应用于拥有大量和有限训练数据的英语成分句法分析任务中。</p>
            
            <h3>1. Introduction</h3>
            <p>循环神经网络（RNN），特别是长短期记忆（LSTM） 和门控循环（gated recurrent） 神经网络，已经在序列建模和如语言建模、机器翻译等转导问题中，被牢固地确立为最先进的方法。此后，大量的研究工作继续推动着循环语言模型和编码器-解码器架构的边界。</p>
            <p>循环模型通常沿着输入和输出序列的符号位置进行计算。通过将位置与计算时间中的步骤对齐，它们生成一系列隐藏状态 $h_t$，作为前一个隐藏状态 $h_{t-1}$ 和当前位置 t 输入的函数。<strong>这种固有的顺序性阻碍了训练样本内的并行化，这在序列长度较长时变得至关重要，因为内存限制了跨样本的批处理。</strong>最近的工作通过因式分解技巧和条件计算 在计算效率上取得了显著的改进，同时后者还提升了模型性能。然而，顺序计算这个根本性的制约依然存在。</p>
            <p>注意力机制已经成为各种任务中一个不可或缺的部分，它使得模型能够对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离。然而，在除了少数几个案例外，这种注意力机制都是与循环网络结合使用的。</p>
            <p>在这项工作中，我们提出了 Transformer，一种摒弃了循环结构的模型架构，并完全依赖于注意力机制来绘制输入和输出之间的全局依赖关系。Transformer 允许进行显著更多的并行化，并且在八个 P100 GPU 上仅训练十二小时后，就可以在翻译质量上达到新的技术水平。</p>
            
            <h3>2. Background</h3>
            <p>减少顺序计算的目标也构成了 Extended Neural GPU、ByteNet 和 ConvS2S 的基础，所有这些模型都使用卷积神经网络作为基本构建块，为所有输入和输出位置并行计算隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量，会随着位置间距离的增加而增长，对于 ConvS2S 是线性增长，对于 ByteNet 是对数增长。这使得学习远距离位置之间的依赖关系变得更加困难。在 Transformer 中，这个操作数量被减少到了常数级别，尽管这是以牺牲因注意力加权位置平均化而导致的有效分辨率降低为代价的，但我们通过第 3.2 节中描述的多头注意力机制来弥补了这一影响。</p>
            <p><strong>自注意力（Self-attention），有时也称为内部注意力（intra-attention），是一种关联单个序列不同位置的注意力机制，目的是为了计算序列的表示。</strong>自注意力已在多种任务中成功应用，包括阅读理解、摘要概括、文本蕴含以及学习与任务无关的句子表示。</p>
            <p>端到端的记忆网络是基于循环注意力机制而非序列对齐的循环，并已在简单语言问答和语言建模任务上表现良好。</p>
            <p>然而，据我们所知，Transformer 是第一个完全依赖自注意力来计算其输入和输出表示的转导模型，而不使用序列对齐的 RNN 或卷积。在接下来的章节中，我们将描述 Transformer，阐述使用自注意力的动机，并讨论其相对于 和 等模型的优势。</p>
            
            <h3>3. Model Architecture</h3>
            <p>大多数有竞争力的神经序列转导模型都具有编码器-解码器结构。在这里，编码器将一个由符号表示组成的输入序列 $(x_1, ..., x_n)$ 映射到一个连续表示序列 $z = (z_1, ..., z_n)$。给定 z，解码器随后逐个元素地生成一个输出序列 $(y_1, ..., y_m)$。在每一步中，模型都是自回归的，即在生成下一个符号时，将先前生成的符号作为额外输入。</p>
            <p>Transformer 遵循这种整体架构，使用堆叠的自注意力层和逐点全连接层来分别构建编码器和解码器，如图 1 的左半部分和右半部分所示。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./Transformer_figs/fig1.png" alt="Language-Table Performance" class="img-medium">
                    <figcaption>图1：Transformer模型架构</figcaption>
                </figure>
            </div>

            <h4>3.1. Encoder and Decoder Stacks</h4>
            <p><strong>编码器：</strong>编码器由 N = 6 个相同的层堆叠而成。每一层有两个子层。第一个是多头自注意力机制，第二个是一个简单的、按位置进行的全连接前馈网络。我们在每个子层周围都使用了一个残差连接，然后进行层归一化。也就是说，每个子层的输出是 LayerNorm(x + Sublayer(x))，其中 Sublayer(x) 是该子层本身实现的函数。为了方便这些残差连接，模型中所有的子层以及嵌入层，都产生维度为 dmodel = 512 的输出。</p>
            <p><strong>解码器：</strong>解码器同样由 N = 6 个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器栈的输出执行多头注意力。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器栈中的自注意力子层，以防止当前位置关注到后续位置。这种掩码机制，结合输出嵌入被偏移一个位置的事实，确保了对位置 i 的预测只能依赖于位置小于 i 的已知输出。</p>

            <h4>3.2. Attention</h4>
            <p>注意力函数可以被描述为将一个查询（query）和一组键值对（key-value pairs）映射到一个输出的过程，其中查询、键、值和输出都是向量。输出是根据值的加权和计算得出的，其中每个值的权重是通过查询与对应键的兼容性函数计算的。</p>
            
            <h5>3.2.1. Scaled Dot-Product Attention</h5>
            <p>我们将我们特定的注意力机制称为“缩放点积注意力”（图2）。输入由维度为 $d_k$ 的查询和键，以及维度为 $d_v$ 的值组成。我们计算查询与所有键的点积，然后将每个点积除以 $\sqrt{d_k}$，最后应用一个 softmax 函数来获得值的权重。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./Transformer_figs/fig2.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图 2：(左) 缩放点积注意力 (Scaled Dot-Product Attention)。(右) 多头注意力 (Multi-Head Attention) 由多个并行的注意力层组成</figcaption>
                </figure>
            </div>
            <p>在实践中，我们同时对一组查询计算注意力函数，并将它们打包成一个矩阵 Q。键和值也分别打包成矩阵 K 和 V。我们计算输出矩阵如下：</p>
            <p>$ Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p>
            <p>两种最常用的注意力函数是加法注意力 和点积（乘性）注意力。除了缩放因子 $\frac{1}{\sqrt{d_k}}$ 外，点积注意力与我们的算法是相同的。加法注意力使用一个带单个隐藏层的前馈网络来计算兼容性函数。虽然两者在理论上的复杂度相似，但点积注意力在实践中速度更快、空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p>
            <p>当 $d_k$ 的值较小时，两种机制的表现相似，但对于较大的 $d_k$ 值，加性注意力的表现优于没有缩放的点积注意力。我们怀疑，对于较大的 $d_k$ 值，点积的量级会变得很大，从而将 softmax 函数推向梯度极小的区域。为了抵消这个影响，我们用 $\frac{1}{\sqrt{d_k}}$ 来缩放点积。</p>
            <div class="appendix-box">
                <p>为了说明为什么点积会变大，假设 q 和 k 的分量是均值为 0、方差为 1 的独立随机变量。那么它们的点积 $q·k = \sum\limits_{i=1}^{d_k} q_i k_i$，均值为 0，方差为 $d_k$。</p>
            </div>
            
            <h5>3.2.2. Multi-Head Attention</h5>
            <p>我们发现，与其使用 $d_{model}$ 维度的键、值和查询来执行单次注意力函数，不如将查询、键和值分别通过 h 次不同的、学习到的线性投影，将它们投影到 $d_q$、$d_k$ 和 $d_v$ 维度。然后，在每个投影版本的查询、键和值上并行执行注意力函数，产生 h 个 $d_v$ 维的输出值。如图 2 所示，这些输出值被拼接起来，并再次进行投影，从而得到最终的值。</p>
            <p>多头注意力允许模型在不同位置共同关注来自不同表示子空间的信息。而对于单一注意力头，平均化过程会抑制这一点。</p>
            <p>$$MultiHead(Q, K, V)=Concat(head_1,...,head_h)W^O where head_i=Attention(QW^Q_i, QW^Q_i, QW^Q_i)$$</p>
            <p>其中，投影是参数矩阵 $Wᵢ^Q ∈ ℝ^(dmodel×dk), Wᵢ^K ∈ ℝ^(dmodel×dk), Wᵢ^V ∈ ℝ^(dmodel×dv)$ 和 $W⁰ ∈ ℝ^(h·dv×dmodel)$。</p>
            <p>在这项工作中，我们采用 h = 8 个并行的注意力层，或称为“头”（heads）。对于每个头，我们使用 $d_k = d_v = d_{model}/h = 64$。由于每个头的维度减小，总计算成本与具有完整维度的单头注意力相似。</p>

            <h5>3.2.3. Applications of Attention in our Model</h5>
            <p>Transformer 以三种不同的方式使用多头注意力：</p>
            <ul>
                <li>在“编码器-解码器注意力”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都能关注到输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如。</li>
                <li>编码器包含自注意力层。在一个自注意力层中，所有的键、值和查询都来自同一个地方，即编码器中前一层的输出。编码器中的每个位置都可以关注到编码器前一层的所有位置。
                <li>类似地，解码器中的自注意力层允许解码器中的每个位置关注到解码器中直到并包括该位置在内的所有位置。我们需要防止解码器中的信息向左流动，以保持其自回归特性。我们在缩放点积注意力内部通过掩码（masking）实现这一点，即在 softmax 的输入中，将所有对应非法连接的值设置为-∞。参见图2。</li>
            </ul>

            <h4>3.3. Position-wise Feed-Forward Networks</h4>
            <p>除了注意力子层，我们编码器和解码器的每一层都包含一个全连接的前馈网络，该网络独立且相同地应用于每个位置。该网络由两个线性变换组成，中间有一个 ReLU 激活函数。</p>
            <p>$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$</p>
            <p>虽然线性变换在不同位置上是相同的，但它们在不同层之间使用不同的参数。描述这一点的另一种方式是，它是两个核大小为 1 的卷积。输入和输出的维度是 $d_{model} = 512$，内层的维度是 $d_{ff} = 2048$。</p>
            
            <h4>3.4. Embeddings and Softmax</h4>
            <p>与其他序列转导模型类似，我们使用学习到的嵌入（embeddings）将输入词元（tokens）和输出词元转换为维度为 $d_{model}$ 的向量。我们还使用常规的学习线性变换和 softmax 函数将解码器的输出转换为预测的下一个词元概率。在我们的模型中，我们共享两个嵌入层和 pre-softmax 线性变换之间的权重矩阵，类似于。在嵌入层中，我们将这些权重乘以 $\sqrt {d_{model}}$。</p>

            <h4>3.5. Positional Encoding</h4>
            <p>由于我们的模型不包含循环和卷积，为了让模型能够利用序列的顺序，我们必须注入一些关于序列中词元相对或绝对位置的信息。为此，我们将“位置编码”（positional encodings）添加到编码器和解码器栈底部的输入嵌入中。位置编码与嵌入具有相同的维度 $d_{model}$，因此两者可以相加。位置编码有多种选择，可以是学习的也可以是固定的。</p>
            <p>在这项工作中，我们使用不同频率的正弦和余弦函数：</p>
            <p>$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$</p>
            <p>$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$</p>
            <p>其中 pos 是位置，i 是维度。也就是说，位置编码的每个维度都对应一个正弦曲线。波长形成一个从 2π 到 10000·2π 的几何级数。我们选择这个函数是因为我们假设它能让模型轻松地学习通过相对位置来关注，因为对于任何固定的偏移量 $k，PE_{pos+k}$ 都可以表示为 $PE_{pos}$ 的线性函数。</p>
            <p>我们也尝试了使用学习的位置嵌入，并发现两个版本产生了几乎相同的结果（见表3，行(E)）。我们选择了正弦版本，因为它可能允许模型外推到比训练期间遇到的序列更长的序列。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./Transformer_figs/tab3.png" alt="Language-Table Performance">
                    <figcaption>表 3：Transformer 架构的变体。未列出的值与基础模型相同。所有指标均在英德翻译开发集 newstest2013 上得出。列出的困惑度是基于我们的字节对编码的每个词片的困惑度，不应与每个词的困惑度进行比较。</figcaption>
                </figure>
            </div>

            <h3>4. Why Self-Attention</h3>
            <p>在本节中，我们将自注意力层的各个方面与循环层和卷积层进行比较，这些层通常用于将一个可变长度的符号表示序列 $(x_1, ..., x_n)$ 映射到另一个等长的序列 $(z_1, ..., z_n)$，其中 $x_i, z_i ∈ ℝᵈ$，例如在一个典型的序列转导编码器或解码器的隐藏层中。我们使用自注意力的动机主要考虑三个方面。</p>
            <p>一是每层的总计算复杂度。另一个是可以并行化的计算量，以所需的最小顺序操作数量来衡量。</p>
            <p>第三是网络中远距离依赖关系之间的路径长度。学习远距离依赖关系是许多序列转导任务中的一个关键挑战。影响学习这种依赖关系能力的一个关键因素是前向和后向信号在网络中必须传播的路径长度。输入和输出序列中任意位置组合之间的路径越短，学习远距离依赖关系就越容易。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。</p>
            
            <h4>4.1. RT-2 在任务上表现如何，更重要的是，在新对象、背景和环境中泛化情况如何？</h4>
            <p>为了评估在分布内的性能以及泛化能力，我们将 RT-2-PaLI-X 和 RT-2-PaLM-E 模型与前几节中列出的四个基线进行比较。对于“已见任务”类别，我们使用了与 RT-1 (Brohan et al., 2022) 相同的一套指令，该评估包括超过 200 个任务：36 个用于拾取对象，35 个用于敲击对象，35 个用于竖直放置物品，48 个用于移动对象，18 个用于打开或关闭各种抽屉，以及 36 个用于从抽屉中取出并放入物品。请注意，这些“分布内”的评估仍然会变化物体位置等因素，例如一天中的时间、机器人位置等，需要技能来泛化到环境中的现实变化。</p>
            <p>图 3 展示了示例泛化评估，其中分为未见过的类别（对象、背景和环境），并进一步细分为易类和难类。对于未见过的对象，困难案例包括难以掌握和独特的对象（例如玩具）。对于未见过的背景，困难情况包括更多样化的背景和新奇的对象。最后，对于未见过的环境，困难案例对应于具有显示器和附件的视觉上更明显的办公桌环境，而较容易的环境则为水槽。这些评估由超过 280 项任务组成，主要关注许多不同情况下抓取和放置技能。对未见过类别的指令列表在附录 F.2 中指定。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig3.png" alt="Language-Table Performance">
                    <figcaption>图3 | 图4和6b以及表4和6中用于评估的示例一般化场景。</figcaption>
                </figure>
            </div>
            <div class="appendix-box">
                <h3>附录F.2 Evaluation Instructions</h3>
                <p>表 2 列出了在对未见过的对象、背景和环境进行模型评估时使用的自然语言指令。每个指令都在 1-5 次之间运行，具体取决于该评估集中的总指令数。表 3 列出了用于评估定量涌现评估的自然语言指令。每个指令都运行了 5 次。</p>
                <table class="data-table">
                    <caption>表3 | 用于评估RT-2涌现能力的各类任务指令</caption>
                    <thead>
                        <tr>
                            <th>任务组</th>
                            <th>任务</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>符号理解：符号1</td>
                            <td>将可乐罐移到X附近，将可乐罐移到3附近，将可乐罐移到Y附近</td>
                        </tr>
                        <tr>
                            <td>符号理解：符号2</td>
                            <td>将苹果移到树（图标）旁，将苹果移到鸭子（图标）旁，将苹果移到苹果（图标）旁，将苹果移到匹配的卡片上</td>
                        </tr>
                        <tr>
                            <td>符号理解：符号3</td>
                            <td>把可乐罐放到狗（图标）旁边，把可乐罐推到心形（图标）上方，把可乐罐放到星星（图标）上方</td>
                        </tr>
                        <tr>
                            <td>推理：数学</td>
                            <td>将香蕉移到数字2旁，将香蕉移到二加一的总和附近，将香蕉移到三乘二的答案附近，将香蕉移到最小的数字旁</td>
                        </tr>
                        <tr>
                            <td>推理：商标</td>
                            <td>将杯子移到谷歌（商标）旁，将杯子移到安卓（商标）旁，将杯子移到油管（商标）旁，将杯子移到一个搜索引擎（图标）旁</td>
                        </tr>
                        <tr>
                            <td>推理：营养</td>
                            <td>给我拿一个健康的零食，拿一杯健康的饮料，拿起一杯甜饮料，将健康的零食移到健康的饮料旁，拿起一个咸味零食</td>
                        </tr>
                        <tr>
                            <td>推理：颜色与多语言</td>
                            <td>将苹果移到相同颜色的杯子旁，将苹果移到不同颜色的杯子旁，将绿色薯片移到匹配颜色的杯子旁，将苹果移动到绿色的杯子旁(西语), 将苹果移动到红色的杯子旁(德语), 将苹果移动到绿色的杯子旁(西语), 将绿色薯条移动到红色的杯子旁(法语)</td>
                        </tr>
                        <tr>
                            <td>人物识别：名人</td>
                            <td>将可乐罐移到泰勒·斯威夫特（照片）旁，将可乐罐移到汤姆·克鲁斯（照片）旁，将可乐罐移到史努比狗狗（照片）旁</td>
                        </tr>
                        <tr>
                            <td>人物识别：CelebA</td>
                            <td>将可乐罐移到戴眼镜的人旁边，将可乐罐移到白发男子旁边，将可乐罐移到黑发女士旁边</td>
                        </tr>
                    </tbody>
                </table>
                <table class="data-table">
                    <caption>表2 | 用于评估在未见过物体、环境和背景维度上受控分布变化的自然语言指令。</caption>
                    
                    <thead>
                        <tr>
                            <th>任务组</th>
                            <th>任务</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>未见过的物体 (简单)</td>
                            <td>拿起香蕉，将香蕉移动到可乐罐附近，将橙色罐子移动到香蕉附近，拿起奥利奥，将奥利奥移动到苹果附近，将红牛罐移动到奥利奥附近，拿起梨，拿起椰子水，将梨移动到椰子水附近，将百事可乐罐移动到梨附近</td>
                        </tr>
                        <tr>
                            <td>未见过的物体 (困难)</td>
                            <td>拿起冷萃咖啡罐，拿起大的橙色盘子，拿起咀嚼玩具，拿起大的网球，拿起小鸟装饰品，拿起鱼形玩具，拿起姜味柠檬康普茶，拿起鸡蛋分离器，拿起手表，拿起绿色的雪碧罐，拿起蓝色的超细纤维布，拿起黄色的梨，拿起椒盐脆饼袋，拿起消毒湿巾，拿起菠萝味水，拿起绿色的杯子，拿起腌菜零食，拿起小的蓝色盘子，拿起小的橙色擀面杖，拿起章鱼玩具，拿起猫薄荷玩具</td>
                        </tr>
                        <tr>
                            <td>未见过的背景 (简单)</td>
                            <td>拿起绿色的墨西哥胡椒薯片袋，拿起橙色罐子，拿起百事可乐罐，拿起七喜罐，拿起苹果，拿起蓝色的薯片袋，拿起橙子，拿起七喜罐，将橙子移动到水槽附近，拿起可乐罐，拿起海绵，拿起蓝莓味的RXBAR</td>
                        </tr>
                        <tr>
                            <td>未见过的背景 (困难)</td>
                            <td>拿起手表，拿起鸡蛋分离器，拿起绿色的雪碧罐，拿起蓝色的超细纤维布，拿起黄色的梨，拿起椒盐脆饼袋，拿起消毒湿巾，拿起菠萝味水，拿起绿色的杯子，拿起腌菜零食，拿起小的蓝色盘子，拿起小的橙色擀面杖，拿起章鱼玩具，拿起猫薄荷玩具，拿起瑞典鱼软糖袋，拿起大的绿色擀面杖，拿起黑色的太阳镜</td>
                        </tr>
                        <tr>
                            <td>未见过的环境 (简单)</td>
                            <td>拿起可乐罐，拿起苹果，拿起蓝莓味的RXBAR，将苹果移动到可乐罐附近，将蓝莓味的RXBAR移动到苹果附近，将可乐罐移动到蓝莓味的RXBAR附近，拿起蓝色的塑料瓶，拿起海绵，拿起蓝色的薯片袋，将海绵移动到蓝色的塑料瓶附近，将蓝色的薯片袋移动到海绵附近，将蓝色的塑料瓶移动到蓝色的薯片袋附近，将可乐罐移动到白色的马克杯附近，将海绵移动到白色的马克杯附近，将可乐罐移动到黄色的碗附近，将海绵移动到黄色的碗附近，将可乐罐移动到绿色的布附近，将海绵移动到绿色的布附近，将可乐罐移动到盘子附近，将海绵移动到盘子附近，将可乐罐移动到勺子附近，将海绵移动到勺子附近，将可乐罐移动到橙色的杯子附近，将海绵移动到橙色的杯子附近，拿起白色的马克杯，拿起黄色的碗，拿起绿色的布，将白色的马克杯移动到海绵附近，将黄色的碗移动到海绵附近，将绿色的布移动到海绵附近，拿起盘子，拿起勺子，拿起橙色的杯子，将盘子移动到海绵附近，将勺子移动到海绵附近，将橙色的杯子移动到海绵附近，将可乐罐放入水槽，将可乐罐丢入水槽，将可乐罐推入水槽，将海绵放入水槽，将海绵丢入水槽，将海绵推入水槽，将绿色的布放入水槽，将绿色的布丢入水槽，将绿色的布推入水槽</td>
                        </tr>
                        <tr>
                            <td>未见过的环境 (困难)</td>
                            <td>拿起可乐罐，拿起苹果，拿起蓝莓味的RXBAR，将苹果移动到可乐罐附近，将蓝莓味的RXBAR移动到苹果附近，将可乐罐移动到蓝莓味的RXBAR附近，将可乐罐移动到订书机附近，将苹果移动到订书机附近，将可乐罐移动到键盘附近，将苹果移动到键盘附近，将可乐罐移动到纸巾盒附近，将苹果移动到纸巾盒附近，将可乐罐移动到纸张附近，将苹果移动到纸张附近，将可乐罐移动到鼠标附近，将苹果移动到鼠标附近，将可乐罐移动到书本附近，将苹果移动到书本附近，拿起记号笔，拿起订书机，拿起鼠标，将记号笔移动到苹果附近，将订书机移动到苹果附近，将鼠标移动到苹果附近，将可乐罐向左推，将可乐罐向右推，将海绵向左推，将海绵向右推，将纸巾盒向左推，将纸巾盒向右推，指向可乐罐，指向海绵，指向纸巾盒</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <p>如图 4 和附录表 4 所示，RT-2 模型在已知任务上的表现与 RT-1 相似，其他基准线的表现要差一些。在各种泛化实验中，RT-2 模型与基线之间的差异最为明显，这表明视觉语言行动模型的优势在于从其大规模预训练数据中转移出更具一般性的视觉和语义概念。在这里，平均而言，RT-2 的两个版本的表现相似，导致相对于下一个基线 RT-1 和 MOO 约有 2 倍的提升，以及相对于其他基线约有 6 倍的提升。PaLM-E 版本的 RT-2 在更困难的一般化场景下似乎比 RT-2-PaLI-X 表现得更好，但在较容易的场景下表现较差，从而导致类似的整体性能。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig4.png" alt="Language-Table Performance">
                    <figcaption>图4 | 在已知训练任务以及对新对象、新背景和新环境的泛化测量中，RT-2 和基线的两个实例的整体性能。附录表 4 细节显示了完整结果。</figcaption>
                </figure>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_tab4.png" alt="Language-Table Performance">
                    <figcaption>表 4：RT-2 的两个实例以及基线在训练任务上的总体性能，以及评估测量泛化到新对象、新背景和新环境。</figcaption>
                </figure>
            </div>
            <p><strong>开源语言表格基准。</strong>为了提供一个使用开源基线和环境的额外比较点，我们利用了Lynch等人（2022）提供的开源Language-Table模拟环境。我们在几个预测任务上联合微调了一个较小的PaLI 3B模型，包括域内VQA任务，用于Language-Table数据集，并在仿真中评估所得到的策略。对于动作预测任务，我们将动作离散化并编码为文本格式“X Y”，其中X和Y范围从-10到+10，表示末端执行器的二维笛卡尔设定点增量。由于其尺寸减小，生成的模型可以以与其它基线相似的速度（5 Hz）进行推断。本实验的结果如表1所示。我们观察到，在我们的模型与基线相比时，性能显著提升，这表明基于视觉的语言模型预训练加上大型PaLI模型的表达能力可以在其他场景中受益，例如在这个例子中的机器人仿真。我们还在图5中展示了现实世界中样本外行为的质量，演示了新颖的推动任务和以前在这个环境中从未见过的目标物体。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig5.png" alt="Language-Table Performance">
                    <figcaption>图5 | 在语言表环境中出现的现实世界中的分布外行为。使用与表1中相同的RT-2-PaLI-3B模型检查点。 </figcaption>
                </figure>
            </div>
            <p>有关Language Table实验的更多详细信息，请参见附录B和D。</p>
            <div class="appendix-box">
                <h3>附录B 数据集</h3>
                <p>视觉语言数据集基于陈等人(2023 b)和Driess等人(2023)的数据集混合。这些数据的主要部分由WebLI数据集组成，该数据集包含约100亿对跨模态相似性得分最高的图像文本对，用于训练，共有109种语言。还包括许多其他标注和视觉问答数据集，有关数据集混合的更多信息，请参见Chen等人(2023 b)中的RT-2-PaLI-X和Driess等人(2023)中的RT-2-PaLM-E。在联合微调RT-2-PaLI-X时，我们没有使用陈等人(2023 a)中描述的Episodic WebLI数据集。机器人数据集基于 Brohan 等人(2022)的数据集。它由移动操作机器人收集的演示示例组成。每个演示都附带了来自七项技能之一的自然语言指令：“拾取对象”、“将对象移近物体”、“将对象竖直放置”、“敲倒对象”、“打开抽屉”、“关闭抽屉”、“将对象放入容器中”，以及“从容器中取出对象并将其放在台面上”。更多详细信息，请参见 Brohan 等人(2022)。RT-2-PaLI-X 对机器人数据集进行加权，使其占训练混合物的大约 50% 用于联合微调。RT-2-PaLM-E 在机器人数据集上的权重约为训练混合物的 66%。在表 1 中的“语言表格”部分，我们的模型是在林奇等人。 (2022) 的 Language-Table 数据集上训练的。 我们的模型还针对多个预测任务进行了联合微调：</p>
                <ol>
                    <li>给定两个连续的图像帧和文本指令，预测动作</li>
                    <li>给定图像帧，预测指令</li>
                    <li>给定图像帧，预测机器人臂的位置</li>
                    <li>给定图像帧，预测给定图像帧之间的时间步数</li>
                    <li>给定图像帧和指令，预测任务是否成功</li>
                </ol>
            </div>
            <h4>4.2. 我们能否观察并测量RT-2的涌现能力？</h4>
            <p>除了评估视觉-语言-动作模型的泛化能力之外，我们还希望评估这类模型在多大程度上能够通过从网络上迁移知识，实现超越机器人数据中所展示的新能力。我们称之为<strong>涌现性</strong>，因为它们通过大规模预训练而出现。我们不期望这种转移能够使新的机器人动作成为可能，但我们确实期望语义和视觉概念、包括关系和名词，在机器人数据中没有看到的情况下也能有效地转移。</p>
            <p><strong>定性评估。</strong> 首先，我们用我们的 RT-2-PaLI-X 模型来实验，以确定从视觉语言概念中转移的各种涌现能力。我们在图 2 中展示了这种交互的一些示例。我们通过探索发现，RT-2 在场景语境下具有新颖的理解能力和基本推理能力。例如，完成“把草莓放入正确的碗里”的任务需要对不仅仅是草莓和碗是什么有细微的理解，还需要在场景上下文中进行推理，知道草莓应该与相似的水果放在一起。对于“捡起即将掉下的包”的任务，RT-2 表现出物理理解能力，能够区分两个包并识别出置于危险位置的物体。所有这些测试中的相互作用都是机器人数据中从未见过的，这表明了从视觉语言数据到语义知识的迁移。</p>
            <p><strong>定量评估。</strong> 为了量化这些新兴能力，我们从之前的评估中选择了两个基线模型：RT-1 和 VC-1，并与我们的两个模型进行比较：RT-2-PaLI-X 和 RT-2-PaLM-E。为了减少实验的方差，我们在 A/B 测试框架（Fisher, 1936）下对所有方法进行了评估，在该框架下，四个模型在完全相同的条件下依次进行评估。我们根据推理和语义理解轴将 RT-2 的涌现能力分为三类（每种类别的示例请参见附录中的图 8）：</p>
            <ol>
                <li><strong>符号理解 (Symbolic Understanding):</strong> 它明确测试了 RT-2 策略是否从视觉语言预训练中转移出了机器人数据中不存在的语义知识。这一类别的示例指令包括“移动苹果到3”或“把可乐罐推到心上”。</li>
                <li><strong>推理 (Reasoning):</strong> 它展示了底层 VLM 各种推理方面的能力，以控制任务。这些任务需要视觉推理（“将苹果移到与自身颜色相同的杯子上”）、数学（“将X移动到两个加一的总和附近”）和多语言理解（“ mueve la manzana al vaso verde”）。</li>
                <li><strong>人类识别 (Human Recognition):</strong> 包括诸如“将可乐罐移到戴眼镜的人旁边”这样的任务，以证明对人类的理解和识别能力。</li>
            </ol>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig8.png" alt="Language-Table Performance">
                    <figcaption>图8：使用 RT-2 研究其涌现能力的一些评估场景的概述。它们侧重于三个大类，即（a）推理、(b)符号理解以及 (c)人类识别。可视化指令是完整指令的一个子集，完整指令列在附录F.2中。 </figcaption>
                </figure>
            </div>
            <p>我们在图6(a)中展示了实验结果，所有数值结果在附录H.2中。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig6a.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图6(a)：RT-2与两个基线在各种紧急技能评估上的性能比较（图8）</figcaption>
                </figure>
            </div>
            <div class="appendix-box">
                <h3>附录H.2. Emergent Evaluation, for Section 4.2</h3>
                <p>表5列出了我们所有的定量涌现性评估结果。 我们发现，与 RT-1 相比，RT-2 在这些新指令上的性能提高了 2 到 3 倍，而且不需要额外的机器人演示。 这证明了我们的方法如何利用预训练来提高在大规模视觉语言数据集上进行推理的能力。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./RT-2_figs/RT-2_tab5.png" alt="Language-Table Performance">
                        <figcaption>表5：RT-2 和基线在定量新兴评估上的性能。</figcaption>
                    </figure>
                </div>
            </div>
            <p>我们观察到，我们的 VLA 模型在所有类别上都显著优于基线，其中我们的最佳 RT-2-PaLI-X 模型比下一个最佳基线（RT-1）平均成功率高出三倍多。我们还注意到，虽然基于 PaLI-X 的较大模型在平均符号理解、推理和人脸识别方面表现更好，但基于 PaLM-E 的较小模型在涉及数学推理的任务上具有优势。我们将这一有趣的结果归因于 PaLM-E 中使用的预训练混合不同，从而产生了一个在数学计算方面比主要基于视觉预训练的 PaLI-X 更加擅长的模型。</p>
            <h4>4.3. 参数数量和其他设计决策如何影响泛化？</h4>
            <p>出于模型大小的灵活性考虑（由于PaLM-E的特性，RT-2-PaLM-E仅限于某些大小的PaLM和ViT模型），我们使用了RT-2-PaLI-X模型进行比较。具体来说，我们比较了两个不同的模型大小：5B和55B，以及三种不同的训练方案：从头开始训练，不使用预训练的VLM权重；只使用机器人动作数据微调预训练模型；以及联合微调（co-training with fine-tuning），这是本工作中主要使用的训练方法，其中我们同时使用原始VLM训练数据和机器人数据对VLM进行微调。因为我们主要关注这些模型的泛化能力，所以我们去掉了这个实验集中的已知任务评估。</p>
            <p>在图6b和附录表6中展示了消融实验的结果。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig6b.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图6(b)：对RT-2-PaLI-X的消融研究，展示了参数数量和训练策略对泛化的影响。</figcaption>
                </figure>
            </div>
            <div class="appendix-box">
                <h3>附录H.3. Size and Training Ablations, for Section 4.3</h3>
                <p>表6详细说明了模型大小和训练方法对消融的影响。我们发现，无论是在哪方面，模型大小都对性能有重要影响，联合微调优于微调，而微调又优于从头开始训练。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./RT-2_figs/RT-2_tab6.png" alt="Language-Table Performance">
                        <figcaption>表6：RT-2的消融展示了参数数量和训练策略对泛化的影响。</figcaption>
                    </figure>
                </div>
            </div>
            <p>首先，我们观察到从头开始训练一个非常大的模型，即使对于5B模型也会导致性能很差。考虑到这个结果，我们在从头开始训练时决定跳过更大的55B PaLI-X模型的评估。其次，我们注意到联合微调模型（无论其大小如何）比仅使用机器人数据对其进行简单微调能够获得更好的泛化性能。我们认为这是因为保持原始数据在训练中的微调部分周围，使模型不会忘记先前在VLM训练期间学到的概念。最后，不出所料，我们发现模型尺寸的增加会导致更好的泛化性能。</p>
            <h4>4.4. RT-2 是否可以像视觉语言模型一样展示思维链推理的迹象？</h4>
            <p>受 LLM 中的思维链提示方法（Wei等人，2022）的启发，我们对带有 PaLM-E 的 RT-2 进行了微调，仅用几百个梯度步骤来提高它共同利用语言和行动的能力，希望这能引发更复杂的推理行为。我们在数据中添加了一个额外的“计划”步骤，首先用自然语言描述机器人即将采取的动作的目的，然后是实际动作标记，例如：“指令：我饿了。计划：拿一包巧克力棒。动作：1 128 124 136 121 158 111255。”这种数据增强方案在视觉推理 (VQA) 数据集和生成操作的数据集之间架起了一座桥梁。</p>
            <p>我们定性观察到，采用思维链推理的RT-2能够响应更复杂的指令，这得益于它首先获得了用自然语言规划行动的空间。这一发现为以下观点提供了初步证据：将大型语言模型或视觉语言模型作为规划器（Ahn等人，2022；Driess等人，2023）与底层策略整合到单一视觉语言行动模型中具有可行性。图7和附录I展示了采用思维链推理的RT-2的运行实例。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./RT-2_figs/RT-2_fig7.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图7：使用链式推理的RT-2推出，其中RT-2生成计划和行动。 </figcaption>
                </figure>
            </div>
            <div class="appendix-box">
                <h3>附录I. Additional Chain-Of-Thought Reasoning Results</h3>
                <p>我们在图 10 中提供了使用 RT-2-PaLM-E 完成的思维链推理推演的更多示例，如第 4.4 节所述。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./RT-2_figs/RT-2_fig10.png" alt="Language-Table Performance">
                        <figcaption>图10：具有思维链推理的RT-2的其他示例</figcaption>
                    </figure>
                </div>
            </div>
            <h3>5. Limitations</h3>
            <p>尽管 RT-2 展示了很有前途的一般化特性，但这种方法存在多个局限性。首先，虽然我们证明通过 VLM 进行基于 Web 的预训练可以提高对语义和视觉概念的一般化能力，但机器人不会通过包含这种额外经验来获得执行新动作的能力。模型的物理技能仍然局限于机器人数据中看到的技能分布（见附录G），但它学会了以新的方式部署这些技能。我们认为这是由于数据集在技能轴上不够多样化所致。未来研究的一个令人兴奋的方向是研究如何通过收集人类视频等新的数据收集范例来学习新的技能。</p>
            <div class="appendix-box">
                <h3>附录G. Example Failure Cases</h3>
                <p>在图9中，我们提供了语言表设置中一种值得注意的失败类型的示例，其中RT-2模型不能推广到未见过的对象动力学。在这种情况下，尽管该模型能够正确地关注语言指令并移动到第一个正确的对象，但它无法控制这些对象具有挑战性的动力学，这些对象的动力学与在此环境中看到的小型积木对象集（Lynch等人，2022）大不相同。然后，钢笔会从桌子上滚下来（图9左），而香蕉的质心远离机器人接触的位置（图9右）。我们注意到，推动动力学通常很难预测和控制（Yu等人，2016）。我们假设通过进一步扩展包含各种环境和物体的数据集，可能可以在机器人-环境相互作用动力学方面实现更大的泛化——例如，在这种情况下，数据集包括类似类型的各种更复杂的推动力学（Dasari等人，2019）。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./RT-2_figs/RT-2_fig9.png" alt="Language-Table Performance">
                        <figcaption>图9：定性示例失败案例在现实世界中未能推广到看不见的对象动力学。</figcaption>
                    </figure>
                </div>
            </div>
            <p>此外，尽管在定性和定量的涌现评估中，RT-2 在现实世界操作任务中的表现令人鼓舞，但我们仍然发现了许多值得注意的失败案例。例如，在当前的数据集构成和训练方法下，RT-2 在：</p>
            <ul type="disc">
                <li>通过特定部分（如手柄）抓住物体</li>
                <li>机器人数据中没有见过的新动作，比如用毛巾擦或使用工具</li>
                <li>精细的动作，比如叠毛巾</li>
                <li>需要多层间接推理的扩展推理</li>
            </ul>
            <p>其次，虽然我们已经展示了实时运行大型 VLA 模型的能力，但这些模型的计算成本很高。由于这些方法被应用于需要高频控制的环境，实时推理可能会成为主要瓶颈。未来研究的一个令人兴奋的方向是探索量化和蒸馏技术，这可能使这些模型以更高的速率或在更低成本的硬件上运行。这也与另一个当前的局限性有关，即只有少数通用可用的 VLM 模型可用于创建 RT-2。我们希望更多的开源模型能够提供（例如：https://llava-vl.github.io/），私有模型可以开放他们的微调 API，这是构建 VLA 模型的充分要求。</p>
            <h3>6. Conclusions</h3>
            <p>在这篇论文中，我们描述了如何通过结合视觉语言模型预训练与机器人数据来训练视觉语言行动 (VLA) 模型。然后，我们基于 PaLM-E 和 PaLI-X 提出了两个 VLA 实现，我们称之为 RT-2-PaLM-E 和 RT-2-PaLI-X。这些模型与机器人轨迹数据共同微调以输出机器人动作，这些动作表示为文本标记。我们展示了我们的方法导致非常高效的机器人策略，并且更重要的是，导致显著更好的泛化性能和从Web 规模的视觉语言预训练继承的能力。我们认为这种简单而通用的方法展示了机器人直接从更好的视觉语言模型中受益的前景，这使机器人学习领域处于一个可以进一步改进的有战略意义的位置，并随着其他领域的进步而改进。</p>
        </section>

        <!-- ====================================================================== -->
        <!-- PART 3: 论文重点代码/实现解析 (Code & Implementation Analysis)         -->
        <!-- ====================================================================== -->
        <!-- 这是我为你解析的部分。未来你可以将自己的代码分析放在这里。 -->
        <section id="code-analysis">
            <h2>论文重点实现解析</h2>
            <div class="code-analysis">
                <h3>1. 动作的文本化 (Action Tokenization)</h3>
                <p>这是实现VLA模型的关键。论文将一个7维的机器人动作（6自由度位移 + 1个夹爪状态）和一个终止命令，全部离散化并映射为文本标记。</p>
                <pre><code><span class="comment"># 动作空间 (Action Space)</span>
                - 6-DoF position/rotation displacement (末端执行器6自由度位移)
                - Gripper extension (夹爪开合度)
                - Terminate episode (终止指令)

                <span class="comment"># 离散化 (Discretization)</span>
                - 每个连续维度被均匀地划分为 <span class="value">256</span> 个桶 (bins)。
                - 因此，一个完整的机器人动作可以表示为 <span class="value">8</span> 个整数。

                <span class="comment"># 文本格式 (Text Representation)</span>
                - 动作向量被转换成一个由空格分隔的字符串。
                - 例如: "<span class="token">terminate</span> <span class="token">Δpos_x</span> <span class="token">Δpos_y</span> <span class="token">Δpos_z</span> <span class="token">Δrot_x</span> <span class="token">Δrot_y</span> <span class="token">Δrot_z</span> <span class="token">gripper_state</span>"
                - 实际输出可能像这样: "<span class="value">1 128 91 241 5 101 127 255</span>"
                </code></pre>
                                
                                <h3>2. 联合微调 (Co-finetuning)</h3>
                                <p>为了让模型既不忘记从网络数据中学到的通用知识，又能学会机器人控制，训练时采用了联合微调策略。</p>
                                <pre><code><span class="comment"># 训练数据混合 (Training Data Mix)</span>
                - 机器人轨迹数据 (Robot Trajectories)
                - 网络规模的视觉语言数据 (Web-scale VQA, captioning data, etc.)

                <span class="comment"># 关键技术 (Key Technique)</span>
                - 在每个训练批次中，同时包含机器人数据和网络数据。
                - 增加机器人数据的采样权重 (e.g., <span class="value">50%</span> for RT-2-PaLI-X, <span class="value">66%</span> for RT-2-PaLM-E)，以确保模型充分学习控制策略。
                </code></pre>

                                <h3>3. 模型输入与输出约束</h3>
                                <p>模型以标准的视觉问答 (VQA) 格式接收任务。</p>
                                <pre><code><span class="comment"># 输入格式 (Input Format)</span>
                - 图像: [robot_camera_image]
                - 文本提示: "问: 机器人应该采取什么行动来 [任务说明]? 答:"
                - 示例: "问: 机器人应该采取什么行动来 <strong>捡起苹果</strong>? 答:"

                <span class="comment"># 输出约束 (Output Constraining)</span>
                - 在执行机器人任务时，模型的解码词汇表被限制为仅包含 <span class="value">256</span> 个有效动作标记，确保生成合法的动作指令。
                - 在回答普通VQA问题时，则可以使用完整的词汇表。
                </code></pre>

            </div>
        </section>

    </div>

</body>
</html>