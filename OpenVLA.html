<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>论文笔记 | OpenVLA: An Open-Source Vision-Language-Action Model</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=Roboto+Mono&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #005f73;
            --secondary-color: #0a9396;
            --background-color: #f8f9fa;
            --text-color: #212529;
            --heading-font: 'Noto Sans SC', sans-serif;
            --body-font: 'Noto Sans SC', sans-serif;
            --code-font: 'Roboto Mono', monospace;
            --border-color: #dee2e6;
            --card-bg: #ffffff;
            --shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        }

        body {
            font-family: var(--body-font);
            line-height: 1.8;
            color: var(--text-color);
            background-color: var(--background-color);
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 850px;
            margin: 0 auto;
            background-color: var(--card-bg);
            padding: 30px 50px;
            border-radius: 12px;
            box-shadow: var(--shadow);
        }

        .paper-title {
            font-family: var(--heading-font);
            font-weight: 700;
            font-size: 2.2em;
            color: var(--primary-color);
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 5px;
        }

        .paper-subtitle {
            font-family: var(--heading-font);
            font-size: 1.2em;
            color: #6c757d;
            margin-top: 0;
            margin-bottom: 40px;
        }

        h2 {
            font-family: var(--heading-font);
            font-size: 1.8em;
            color: var(--primary-color);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 8px;
            margin-top: 50px;
        }

        h3 {
            font-family: var(--heading-font);
            font-size: 1.4em;
            color: var(--secondary-color);
            margin-top: 30px;
        }

        h4 {
            font-family: var(--heading-font);
            font-size: 1.2em;
            color: var(--secondary-color);
            margin-top: 25px;
        }

        h5 {
            font-family: var(--heading-font);
            font-size: 1.0em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        p, li {
            font-size: 1em;
            text-align: justify;
        }
        
        strong {
            color: var(--primary-color);
        }

        /* 重点总结区域样式 */
        .summary-box {
            background-color: #eef7f8;
            border-left: 5px solid var(--secondary-color);
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .summary-box h3 {
            margin-top: 0;
            color: var(--primary-color);
        }

        .summary-box ul {
            padding-left: 20px;
        }

        .summary-box li {
            margin-bottom: 10px;
        }

        /* 代码/实现解析区域样式 */
        .code-analysis pre {
            background-color: #282c34;
            color: #abb2bf;
            font-family: var(--code-font);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.5;
            font-size: 0.9em;
        }
        
        .code-analysis code .token{
            color: #61afef; /* 蓝色，用于变量/标记 */
        }
        .code-analysis code .comment{
            color: #5c6370; /* 灰色，用于注释 */
        }
        .code-analysis code .value{
            color: #98c379; /* 绿色，用于值 */
        }

        /* 图像和表格说明文字样式 */
        .figure-caption, .table-caption {
            background-color: #f1f3f5;
            border: 1px solid var(--border-color);
            padding: 15px;
            margin: 20px auto;
            border-radius: 8px;
            font-style: italic;
            color: #495057;
            text-align: center;
        }
        .figure-caption img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .img-small {
            width: 30%; /* 图片宽度为容器的 30% */
            max-width: 100%; /* 安全措施，确保不会超出 */
        }

        .img-medium {
            width: 50%; /* 图片宽度为容器的 50% */
            max-width: 100%;
        }

        .img-large {
            width: 75%; /* 图片宽度为容器的 75% */
            max-width: 100%;
        }

        blockquote {
            border-left: 4px solid var(--border-color);
            padding-left: 20px;
            color: #6c757d;
            margin-left: 0;
        }

        .appendix-box {
            background-color: #f1f3f5; /* 浅灰色背景 */
            border-left: 5px solid #adb5bd; /* 中性灰色边框 */
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .appendix-box h3 {
            margin-top: 0;
            color: #495057; /* 深灰色标题，以在灰色背景上保持清晰 */
        }

        .data-table {
            width: 100%;
            border-collapse: collapse; /* 合并边框 */
            margin: 30px 0;
            font-size: 0.95em;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden; /* 确保圆角生效 */
        }

        .data-table caption {
            caption-side: bottom; /* 将标题放在表格下方，与图片说明保持一致 */
            margin-top: 10px;    /* 与表格的间距 */
            padding: 5px;
            font-size: 0.9em;
            font-style: italic;
            color: #6c757d;      /* 使用柔和的灰色 */
            text-align: center;
        }

        .data-table thead tr {
            background-color: var(--primary-color);
            color: #ffffff;
            text-align: left;
            font-weight: bold;
        }

        .data-table th, .data-table td {
            padding: 12px 15px;
            border-bottom: 1px solid var(--border-color);
        }

        .data-table tbody tr {
            background-color: #ffffff;
        }

        /* 斑马条纹，增加可读性 */
        .data-table tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }

        .data-table tbody tr:last-of-type {
            border-bottom: 2px solid var(--primary-color);
        }

        /* 任务组列加粗，突出显示 */
        .data-table td:first-child {
            font-weight: bold;
            color: var(--primary-color);
        }
    </style>
</head>
<body>

    <div class="container">
        
        <h1 class="paper-title">OpenVLA: An Open-Source Vision-Language-Action Model</h1>
        <p class="paper-subtitle"></p>

        <!-- ====================================================================== -->
        <!-- PART 2: 论文重点总结 (Key Takeaways)                                   -->
        <!-- ====================================================================== -->
        <!-- 这是我为你总结的部分。未来你可以将自己的总结放在这里。 -->
        <section id="summary">
            <h2>论文重点总结</h2>
            <div class="summary-box">
                <h3>核心思想与贡献</h3>
                <p>OpenVLA的核心贡献是推出了一个拥有7B参数的开源视觉-语言-动作（VLA）模型，用于通用的机器人操作。它旨在解决现有VLA模型大多是闭源且难以获取的问题，从而推动机器人学的广泛研究和应用。</p>
                <ul>
                    <li><strong>实现了顶尖的性能:</strong> OpenVLA在通用的机器人操控任务上取得了新的技术水平。尽管其参数量远小于55B的闭源模型RT-2-X，但在包含29个任务的评估中，OpenVLA的绝对任务成功率比RT-2-X高出16.5%。</li>
                    <li><strong>创新的模型架构与大规模训练:</strong> OpenVLA基于Llama 2语言模型，并结合了一个融合DINOv2和SigLIP预训练特征的视觉编码器。这种架构得益于多样化的数据和新颖的模型组件，并利用了包含97万个真实世界机器人演示的Open X-Embodiment数据集进行训练。</li>
                    <li><strong>高效适应新任务的能力:</strong> 论文首次系统地研究了如何高效地对VLA模型进行微调。研究表明，OpenVLA可以通过参数高效的微调方法（如LoRA）在消费级GPU上快速适应新的机器人和任务，解决了模型适配性的关键问题。</li>
                    <li><strong>全面的开源生态系统:</strong> 除了模型本身，研究者还完全开源了其PyTorch训练流程、数据、权重和微调代码。这一举措为社区提供了宝贵的资源，便于其他研究者在此基础上进行复现、探索和创新。</li>
                    <li><strong>解决了实际部署的可及性:</strong> 通过模型量化技术，OpenVLA可以在消费级硬件（如单个RTX 4090 GPU）上高效运行，而性能没有明显下降。这大大降低了研究和使用先进VLA模型的门槛。</li>
                </ul>
            </div>
        </section>

        <!-- ====================================================================== -->
        <!-- PART 1: 论文全文翻译 (Full Translation)                                -->
        <!-- ====================================================================== -->

        <section id="translation">
            <h2>论文全文翻译</h2>
            <div class="figure-caption">
                <!-- 你可以用img标签在这里插入图片 -->
                <img src="./OpenVLA_figs/OpenVLA_fig1.png" alt="RT-2 Overview">
                图1：我们提出了OpenVLA，这是一个7B参数的开源视觉语言动作模型（VLA），在来自Open X-Embodiment数据集[1]的97万个机器人演示中进行了训练。OpenVLA为通用机器人操作策略设定了新的状态艺术。它支持控制多个机器人，并可以通过参数高效的微调快速适应新机器人领域。OpenVLA检查点和PyTorch训练流程是完全开源的，可以从HuggingFace下载并进行微调。
            </div>

            <h3>Abstract</h3>
            <p>在互联网规模的视觉语言数据和多样化的机器人演示相结合的基础上预训练大型政策，有可能改变我们教授机器人新技能的方式：而不是从头开始训练新的行为，我们可以对这些视觉语言动作（VLA）模型进行微调以获得稳健、可泛化用于视觉运动控制的策略。然而，广泛采用VLA对于机器人来说一直具有挑战性，因为现有的VLA大多封闭且公众难以访问，并且先前的工作未能探索为新任务高效地微调VLA的方法，而这是广泛采用VLA的关键部分。为了应对这些挑战，我们引入了OpenVLA，这是一个参数量为7B的开源VLA，它是在一个由97万多个真实世界机器人演示组成的多样化集合上训练的。OpenVLA基于Llama 2语言模型以及一个结合了DINOv2和SigLIP预训练特征的视觉编码器。由于增加了数据多样性并添加了新的模型组件，OpenVLA在通用操纵方面表现出色，在29个任务和多种机器人实现上绝对任务成功率比封闭模型RT-2-X（55B）高出16.5%，参数数量仅为后者的七分之一。此外，我们还表明可以有效地将OpenVLA应用于新的设置，尤其是强大的泛化能力在涉及多个对象的多任务环境中表现良好，并且在从头开始表达的模仿学习方法，如Diffusion Policy中表现出色，比其高出20.4％。我们还探索了计算效率；作为单独的贡献，我们表明OpenVLA可以通过LoRA方法在消费者GPU上进行微调，并通过量化高效地提供服务而不会影响下游的成功率。最后，我们发布了模型检查点、微调笔记本和我们的PyTorch代码库，该代码库内置支持在Open X-Embodiment数据集上大规模训练VLAs。</p>
            
            <h3>1. Introduction</h3>
            <p>学习型机器人操作策略的一个关键弱点是它们无法泛化到训练数据之外：虽然现有的针对特定技能或语言指令进行培训的策略具有将行为扩展到新初始条件（例如物体位置或照明）的能力，但它们缺乏对场景干扰物或新型对象的鲁棒性，并且难以执行未见过的任务指令。然而，在机器人领域之外，现有用于视觉和语言的基础模型，如CLIP、SigLIP和Llama 2等，能够实现这些类型的泛化以及更多功能，源于其互联网规模预训练数据集捕获的先验知识。尽管为机器人复制这种规模的预训练仍然是一个开放挑战——即使最大的机器人操作数据集也只有10万至100万个示例——这种不平衡表明了一个机会：使用现有用于视觉和语言的基础模型作为训练机器人策略的核心构建块，该策略可以泛化到超出其训练数据的对象、场景和任务。 </p>
            <p>为了实现这一目标，现有工作已经探索了将预训练的语言和视觉语言模型集成到机器人表示学习中[12-14]以及模块化系统中的任务规划和执行组件中[15、16]。最近，它们被用于直接学习视觉语言行动模型[VLAs；1、7、17、18]进行控制。VLAs为使用互联网规模数据进行预训练的视觉语言基础模型在机器人上的应用提供了一种直接的实例化方法，可以直接微调视觉条件化的语言模型（VLM），如PaLI [19、20]来生成机器人控制动作。通过利用强大的基于互联网规模数据的预训练模型，RT-2 [7]等VLAs展示了令人印象深刻的结果，并且能够泛化到新的对象和任务上，从而设定通用型机器人策略的新标准。然而，现有的VLAs存在两个关键原因阻碍其广泛使用：1）当前模型[1、7、17、18]是封闭式的，限制了对模型架构、训练程序和数据混合的可见性，2）现有工作没有提供部署和适应VLAs到新机器人、环境和任务的最佳实践——尤其是商用硬件（例如消费级GPU）。我们认为，为了发展未来研究和开发的基础，机器人需要开源的一般性VLAs，支持有效的微调和适应，类似于围绕开源语言模型的现有生态系统[21-24]。 </p>
            <p>为此，我们引入了OpenVLA，这是一个7B的开源VLA，它为通用机器人操作策略建立了新的基准。OpenVLA包括一个预训练的视觉条件语言模型主干，该主干能够捕获多个粒度级别的视觉特征，并在包含97万条来自Open-X Embodiment[1]数据集的机器人操作轨迹的大规模、多样化数据集中进行微调——这个数据集涵盖了广泛的机器人实现、任务和场景。由于增加了数据多样性以及新模型组件，OpenVLA在WidowX和Google Robot两种机器人实现上的29项评估任务中，相对于55B的RT-2-X模型[1, 7]（先前的最先进的VLA）取得了绝对成功率提高16.5%的表现。此外，我们还研究了针对VLAs的有效微调策略，这是前人工作中未探索的新贡献，涉及7种多样化的操作任务，涵盖从物体抓取到清洁桌子等行为。我们发现，经过微调的OpenVLA策略明显优于经过微调的预训练策略，如Octo[5]。与基于diffusion的从头模仿学习相比[3]，在包含多个物体的多任务环境中，OpenVLA在那些需要将语言指令与具体行为相结合的任务上，展现出了显著的进步。</p>
            <div class="appendix-box">
                <p>OpenVLA使用多个预训练模型组件：SigLIP [9] 和DinoV2 [25] 视觉编码器和aLlama 2 [10] 语言模型主干。对于所有三个模型，权重是开放的，但不是他们的训练数据或代码。我们发布培训数据、代码和模型权重以在这些组件之上重现OpenVLA。</p>
            </div>
    
            <h3>2. Related Work</h3>
            <p><strong>视觉语言模型 </strong>视觉条件语言模型（VLM）是通过在互联网规模的数据上训练，以从输入图像和语言提示中生成自然语言的视觉语言模型。它们被广泛应用于各种应用，如视觉问答 [28 - 31] 和对象定位 [32, 33]。<strong>最近视觉语言模型的关键进步之一是将预训练的视觉编码器特征与预训练的语言模型直接构建起来，从而创建强大的多模态模型。</strong>早期的工作探索了各种跨注意力视觉和语言特征的架构 [37 - 41] ，而新的开源视觉语言模型 [20, 42 - 44] 已经收敛到一个简单的“切片作为令牌”的方法，在其中预训练的视觉变压器的切片特征被当作令牌，并投影到语言模型的输入空间。这种简单性使得很容易重新利用现有的工具来大规模培训语言模型进行视觉条件语言模型的培训。我们在工作中使用这些工具来扩展 VLA 的培训，并且特别使用 Karamcheti 等人的预训练主干 [44]，因为它们是从多分辨率视觉特征中训练出来的，融合了 DINOV2 [25] 中低级的空间信息以及 SigLIP [9] 中高级语义，以帮助视觉泛化。 </p>
            <p><strong>通用机器人策略 </strong>近年来，机器人领域的工作朝着训练多任务“通用型”机器人策略的方向发展。这些策略在大型多样化的机器人数据集上进行训练，涵盖许多不同的机器人实现方式。值得注意的是，Octo [5] 训练了一个通用的策略，该策略可以控制多个机器人，并允许对新的机器人设置进行灵活的微调。这些方法与OpenVLA的不同之处在于模型架构。类似Octo 的先前工作通常将预训练组件（如语言嵌入或视觉编码器）与其他从头开始初始化的模型组件组合在一起，在策略训练过程中学习将其“缝合”在一起。不同于这些工作，OpenVLA采用了一种更端到端的方法，直接通过将它们视为语言模型词汇中的令牌来微调VLM以生成机器人动作。我们的实验评估表明，这种简单但可扩展的管道在性能和泛化能力方面显著优于之前的通用策略。 </p>
            <p><strong>视觉语言动作模型 </strong>已经有许多工作探索了VLM在机器人中的应用，例如用于视觉状态表示[12、13]、对象检测[67]、高级规划[16]和提供反馈信号[68-71]。其他的工作直接将VLM集成到端到端的视觉运动控制策略中[14、15]，但需要对策略架构进行大量结构化处理或要求校准相机，这限制了它们的应用范围。最近的一些工作已经探索了与我们类似的方法，并直接微调大型预训练VLM以预测机器人动作[1、7、17、18、72-74]。这些模型通常被称为视觉语言动作模型（VLAs），因为它们直接将机器人控制动作融合到VLM骨架中。这种做法有三个关键好处：</p>
            <ol>
                <li>它在大规模的互联网规模视觉语言数据集上执行预训练的视觉和语言组件的对齐</li>
                <li>使用通用架构而不是专门为机器人控制定制的架构允许我们利用现代VLM训练背后的可扩展基础设施[75-77]并实现对新机器人设置的高效微调</li>
                <li>为机器人从快速改进的VLM中受益提供了直接途径</li>
            </ol>
            <p>现有的VLAs要么专注于单个机器人或模拟环境上的训练和评估[72-74、78]，因此缺乏一般性，要么是封闭式的并且不支持高效的重新调整以适应新的机器人设置[1、7、17、18]。最相关的是RT-2-X[1]在Open X-Embodiment数据集上训练了一个55B的VLA策略，并展示了最先进的通用操纵策略性能。然而，我们的工作在多个重要方面与RT-2-X不同：</p>
            <ol>
                <li>通过结合具有更丰富的机器人预训练数据集的强大的开放VLM骨干网络，OpenVLA在我们的实验中优于RT-2-X，并且大小小一个数量级</li>
                <li>我们彻底研究了在新目标设置下对OpenVLA模型进行微调，而RT-2-X没有研究微调设置</li>
                <li>我们第一个证明现代参数高效微调和量化方法对于VLAs的有效性</li>
                <li>OpenVLA是第一个开源的一般性VLAs，因此支持未来关于VLAs训练、数据混合、目标和推理的研究</li>
            </ol>  

            <h3>3. The OpenVLA Model</h3>
            <p> 我们介绍了OpenVLA模型，这是一个在Open X-Embodiment数据集上训练的7BVLA。关于开发VLA模型的最佳实践有很多问题，例如使用哪些最佳模型骨架、数据集和超参数进行训练。下面，我们详细描述了开发OpenVLA的方法，并总结我们的关键学习成果。具体来说，首先提供了一个现代VLM的简要概述，这些VLM构成了OpenVLA的基础（第3.1节）；然后描述了我们的基本训练配方和数据集（第3.2节和第3.3节）；讨论了一些关键的设计决策（第3.4节）；并提供了用于训练和推理使用的基础设施细节（第3.5节）。</p>
            <h4>3.1. Preliminaries: Vision-Language Models</h4>
            <p> 最近大多数VLM的架构（见图2）由三个主要部分组成： （1）一个视觉编码器，将图像输入映射到多个“图像块嵌入”， (2) 一个投影器，它从视觉编码器输出中提取嵌入，并将其映射到语言模型的输入空间，以及 (3) 一个大型语言模型(LLM)主干。在VLM训练期间，该模型与各种互联网来源中的配对或交错的视觉和语言数据一起进行端到端训练，以预测下一个文本令牌的目标。</p>
            <div class="figure-caption">
                <img src="./OpenVLA_figs/OpenVLA_fig2.png" alt="RT-2 Overview">
                <strong>图2：OpenVLA模型架构。</strong>给定图像观察和语言指令，该模型预测7维机器人控制动作。该架构由三个关键组件组成：（1）一个视觉编码器，它将Dino V2 [25] 和SigLIP [79] 特征串联起来；（2）一个投影机，用于将视觉特征映射到语言嵌入空间；以及（3）LLM主干，即一个7B的Llama 2 [10] 。 
            </div>
            <p> 在这项工作中，我们基于Prismatic-7B VLM [44]构建。Prismatic遵循上述描述的相同标准架构，具有一个600M的视觉编码器、一个小的2层MLP投影仪和一个7B的LLaMA 2语言模型主干[10]。值得注意的是，Prismatic使用了一个两部分的视觉编码器，由预训练的SigLIP [79] 和DinoV2 [25] 模型组成。输入图像块分别通过两个编码器，并将产生的特征向量按通道进行拼接。与更常见的视觉编码器如CLIP [80] 或仅使用SigLIP 的编码器不同，添加的DinoV2 特征已被证明对改进的空间推理是有帮助的[44] ，这对于机器人控制特别有用。 </p>
            <p>SigLIP、DinoV2和LLaMA 2没有透露其训练数据的细节，这些数据可能包括分别来自互联网的数万亿个图像-文本对、仅图像和仅文本的数据。Prismatic VLM在这些组件之上进行了微调，使用了包含大约100万张图像-文本对和纯文本数据样本的LLaVA 1.5数据混合物[43]，其中包含来自开源数据集的总计约100万个图像-文本对和纯文本数据样本[29, 42, 81–83]。 </p>

            <h4>3.2. OpenVLA Training Procedure</h4>
            <p>为了训练OpenVLA，我们对预训练的Prismatic-7B VLM主干进行微调（见图2），用于机器人动作预测。<strong>我们将动作预测问题表述为“视觉语言”任务，输的入观察图像和自然语言任务指令被映射到预测机器人动作的字符串中。</strong>为了让VLM的语言模型主干能够预测机器人动作，我们通过将连续的机器人动作映射到语言模型词典中的离散令牌来表示输出空间中的动作。遵循Brohan等人[7]的做法，我们分别将每个机器人动作维度划分为256个桶之一。对于每个动作维度，我们将间隔均匀地划分成训练数据中第1个百分位数与第99个百分位数之间的区间。使用百分位数而不是Brohan等人[7]使用的最小值和最大值边界允许我们忽略可能大大扩展分段间隔并降低我们动作分段有效粒度的数据中的异常动作。 </p>
            <p>使用这种离散化，我们为N维机器人动作获得N个离散整数∈ [0 ... 255]。不幸的是，OpenVLA语言骨干的分词器Llama tokenizer[10]仅保留了在微调期间引入的新令牌的100个“特殊令牌”，这对我们行动离散化的256个令牌来说太少了。相反，我们再次选择简单的方法，并遵循Brohan等人[7]的做法，通过简单地将Llama tokenizer词汇表中使用的最少的256个令牌（对应于最后256个令牌）覆盖我们的行动令牌来实现这一目标。一旦将动作处理成一个令牌序列，OpenVLA就会被训练成标准的下一个令牌预测目标，只评估预测的动作令牌的交叉熵损失。我们在第3节讨论实施此训练程序的关键设计决策。接下来，我们将描述用于OpenVLA培训的机器人数据集。 </p>
            
            <h4>3.3. Training Data</h4>
            <p>构建OpenVLA训练数据集的目标是捕获大量机器人形态、场景和任务的多样性。这使得最终模型能够控制各种各样的机器人，并且允许对新的机器人设置进行高效的微调。我们利用Open X-Embodiment数据集（OpenX）作为基础来整理我们的训练数据集。当时，完整的OpenX数据集包括超过70个独立的机器人数据集，有超过2百万个机器人轨迹，这些数据被整合成一个统一且易于使用的格式，在一个大型社区的努力下完成。为了使在该数据上进行训练可行，我们对原始数据集进行了多个步骤的数据整理。 </p>
            <p>这个分类的目标是确保（1）所有训练数据集的输入和输出空间的一致性，以及（2）最终训练混合物中实体、任务和场景的平衡组合。为了应对（1），我们遵循[1, 5]并限制我们的训练数据集仅包含至少一个第三人称相机的操纵数据集，并使用单臂末端执行器控制。对于（2），我们利用Octo [5]的数据混合权重对通过第一轮筛选的所有数据集进行加权。Octo 原则上会降低或删除多样性较低的数据集，并提高具有更大任务和场景多样性的数据集；有关详细信息，请参阅Octo 模型团队等人的文章[5]。 </p>
            <p>我们还尝试将几个额外的数据集添加到我们的训练混合中，这些数据集是在Octo发布后添加到OpenX中的，包括DROID数据集（尽管在保守的混合权重为10%的情况下），虽然在实践中，我们发现DROID上的动作令牌精度在整个训练过程中保持较低水平，这表明未来可能需要更大的混合权重或模型来适应其多样性。为了不损害最终模型的质量，我们在最后三分之一的训练中移除了DROID。我们在附录A中提供了所用数据集和混合权重的完整概述。</p>
            <div class="appendix-box">
                <h3>附录A Data Mixture Details</h3>
                <p>我们列出了我们的数据混合物列表，如表3所示。该混合物主要遵循[5]，并添加了少量其他数据集。 </p>
                <div class="figure-caption">
                    <figure>
                        <img src="./OpenVLA_figs/tab3.png" alt="Language-Table Performance" class="img-medium">
                        <figcaption> 表 3：使用来自Open X-Embodiment 数据集的训练数据混合，遵循[5]并添加一些内容。 </figcaption>
                    </figure>
                </div>
            </div>
            
            <h4>3.4 OpenVLA Design Decisions</h4>
            <p>在开发OpenVLA模型时，我们在开始最终的模型训练运行之前，在较小规模的实验中探索了各种设计决策。具体来说，我们使用BridgeData V2[6]对我们的初始实验进行了OpenVLA模型的训练和评估，而不是在完整的OpenX混合，以提高迭代速度并降低计算成本。我们总结了这些探索的关键经验教训如下。</P>
            <p><strong>VLM Backbone. </strong>最初，我们尝试了多个VLM主干。除了Prismatic [44]之外，我们还测试了对机器人动作预测进行微调的IDEFICS-1 [84] 和LLaVA [85] 。我们发现，在场景中只有一个对象的任务上，LLaVA和IDEFICS-1 表现相当，但在涉及多个对象且需要策略操纵正确对象（即语言指令中指定的对象）的任务中，LLaVA表现更强的语言扎根能力。具体来说，与在BridgeData V2 沉浸环境中五个语言扎根任务上的平均绝对成功率相比，微调后的Prismatic VLM策略提高了35%，而LLaVA策略则提高了约10% 的绝对成功率。在简单单个对象任务和多对象、语言扎根任务上，经过进一步改进的微调Prismatic VLM策略的表现优于LLaVA 策略。我们归因于这种性能差异是由于融合的SigLIP-DinoV2 主干提供的增强的空间推理能力（见第3.1节）。除了性能提升外，Prismatic 还提供了一个模块化和易于使用的代码库，因此最终选择了它作为OpenVLA 模型的主干。</p>
            <p><strong>Image Resolution. </strong>输入图像的分辨率对VLA训练所需的计算量有显著影响，因为高分辨率图像会导致更多的图像块令牌和更长的上下文长度，从而导致训练计算成比例地增加。我们比较了使用224×224像素和384×384像素输入的VLAs，但在我们的评估中没有发现性能差异，而后者需要三倍的时间进行训练。因此，我们最终选择将OpenVLA模型的分辨率设置为224×224像素。请注意，在许多VLM基准上，提高分辨率确实可以改善性能（例如，[44]、[86]、[87]），但我们目前还没有看到这种趋势。</p>
            <p><strong>Fine-Tuning Vision Encoder. </strong>微调视觉编码器。先前的工作发现，在VLM训练期间冻结视觉编码器通常会导致更高的性能[44]。直观地，一个被冻结的视觉编码器可能更好地保留从其互联网规模预训练中学习到的稳健特征。然而，我们发现，在VLA训练期间微调视觉编码器对于良好的VLA性能至关重要。我们假设预先训练过的视觉主干可能无法捕获场景重要部分的足够精细的空间细节以实现精确的机器人控制。</p>
            <p><strong>Training Epochs. </strong>典型的LLM或VLM训练运行最多通过其训练数据集完成一到两个轮次的训练。相比之下，我们发现对VLA训练来说迭代次数需要更多，直到训练动作令牌准确率超过95%，机器人性能才持续提高。我们的最终训练运行完成了27个轮次的数据集训练。</p>
            <p><strong>Learning Rate. </strong>学习率。我们在VLA训练中对学习率进行了多个数量级的扫描，并使用固定的学习率为2e-5（与VLM预训练中的相同）取得了最佳结果。\我们没有发现学习率warmup可以提供好处。</p>

            <h4>3.5 Infrastructure for Training and Inference</h4>
            <p>最终的OpenVLA模型在由64个A100 GPU组成的集群上训练了14天，或总共使用了21,500个A100小时。在推理过程中，当以bfloat16精度加载时（即未进行量化），OpenVLA需要15GB的GPU内存，并且在单个NVIDIA RTX 4090 GPU上运行约6Hz（不包括编译、推测解码或其他推理加速技巧）。我们可以通过量化进一步减少推理过程中的OpenVLA内存足迹，而不会影响实际机器人任务中的性能，如第5节所示。我们在图6中报告了各种消费级和服务器级GPU上的推理速度。为了方便起见，我们实现了一个远程VLA推理服务器，允许实时远程流式传输动作预测到机器人——无需访问强大的本地计算设备来控制机器人。我们将此远程推理解决方案作为我们的开源代码发布的一部分公开发布（第4节）。</p>

            <h3>4. The OpenVLA Codebase</h3>
            <p> 与我们的模型一起，我们发布了OpenVLA代码库，这是一个用于训练VLA模型的模块化PyTorch代码库（请参阅https：// openvla.github.io）。它从在单个GPU上微调VLAs扩展到在多节点GPU集群上训练十亿参数的VLAs，并支持现代大型Transformer模型训练的技术，例如自动混合精度（AMP、PyTorch [75]）、Flash Attention [76] 和完全分片数据并行性 (FSDP、Zhao 等人 [77])。开箱即用，OpenVLA 的代码库支持在 Open X 数据集上进行训练，并与 Hugging Face 的 [21] AutoModel 类集成，支持 LoRA 细调 [26] 和量化模型推理 [27, 88]。</p>
            
            <h3>5. Experiments</h3>
            <p>我们的实验评估目标是测试OpenVLA是否能够作为强大的多机器人控制策略出箱即用，以及作为针对新机器人任务进行微调的良好初始化。具体来说，我们希望回答以下问题： </p>
            <ol>
                <li>当在多个机器人和各种通用化类型上评估时，OpenVLA与先前的一般性机器人策略相比如何？</li>
                <li>在新的机器人设置和任务上，OpenVLA是否可以有效地微调，并且它与最先进的数据高效模仿学习方法相比如何？</li>
                <li>我们能否使用参数高效的微调和量化来减少OpenVLA模型的训练和推理计算要求，并使其更加可用？性能与计算之间的权衡是什么？</li>
            </ol>
            
            <h4>5.1 Direct Evaluations on Multiple Robot Platforms</h4>
            <p><strong>机器人设置和任务。</strong>我们评估了OpenVLA在两个机器人实现上的性能：“开箱即用”的BridgeData V2评估中的WidowX机器人（见图1，左）以及RT-1和RT-2评估中的移动操纵机器人（“谷歌机器人”；见图1，中）。这两个平台已经在以前的工作中广泛用于评估通用型机器人策略[1、2、5、7]。我们在每个环境中定义了一个全面的评价任务集，涵盖了各种一般化轴，如视觉（未知背景、干扰对象、物体的颜色/外观）、运动（未知对象的位置/方向）、物理（未知对象的大小/形状）和语义（未知目标对象、指令和来自互联网的概念）的一般化。我们也评估了多对象场景的语言条件能力，测试政策是否可以正确地操纵用户提示指定的目标对象。请参阅图3和图4底部行中的桥数据V2和谷歌机器人的示例任务图像。总体而言，在桥数据V2实验中，我们对每种方法进行了170次卷积（17个任务，每个任务有10次试运行），而在谷歌机器人实验中，则为60次卷积（12个任务，每个任务有5次试运行）。所有任务及其与训练数据的不同之处的详细分解见附录B。附录B。本节和后续各节中的所有评估均采用A/B评估，使用相同的任务以及相同的初始机器人状态和对象状态进行比较，以确保公平的对比。</p>
            <div class="appendix-box">
                <h3>附录B Evaluation Tasks and Detailed Results</h3>
                <p> 在本节中，我们提供了有关第5.1节讨论的BridgeData V2 WidowX和Google机器人评估以及第5.2节讨论的Franka-Tabletop和Franka-DROID微调评估的更多细节。</p>
                <h4>B.1 BridgeData V2 WidowX Evaluation Details</h4>
                <p>这里我们特别关注第5.1节中讨论的BridgeData V2评估。</p>
                <h5>B.1.1 BridgeData V2 Evaluation Tasks</h5>
                <p>如第5.1节所述，我们对每个通用机器人操作策略在17个任务上进行了评估，每个任务有10次试验。在此部分中，我们将提供任务类别和个别任务的详细信息。</p>
                <p> 总共，我们评估了五个视觉泛化任务、两个运动泛化任务、三个物理泛化任务、四个语义泛化任务和三个语言接地任务。请注意，所有我们评估的任务都引入了一些形式的分布偏移，因为我们无法获得原始数据集中的确切对象（其他分布偏移自然地出现在我们复制最初在不同位置构建的真实世界测试环境中；有关此类分布偏移的详细讨论，请参见附录B.1.2）。总共17个任务如图7所示。每个滚动都是标记为失败（0）或成功（1）。在一些更困难的任务中，我们记录部分成功（0.5）；我们在任务描述中描述了获得部分学分的条件。下面，我们按照图7所示的顺序描述每个任务：</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./OpenVLA_figs/fig7.png" alt="Language-Table Performance" class="img-large">
                        <figcaption> 图7：BridgeData V2 WidowX机器人评估任务。我们对每种通用机器人策略进行4种类型的分布外（OOD）泛化任务的评估：视觉、运动、物理和语义（如第5.1节所定义）。每一对图像显示了开始状态以及机器人完成任务后的示例结束状态。我们还通过改变提示，同时固定初始状态，并测试策略是否可以接近正确的目标对象来严格评估底部三行所示的三个任务中的语言接地。</figcaption>
                    </figure>
                </div>
                <ol>
                    <li><strong>将茄子放入锅中（简易版）：</strong>机器人的目标是拿起茄子并将其放入锅中。这是一个视觉泛化任务，因为我们使用了一个手工制作的纸制锅，其外观与原始BridgeData V2训练数据集中的锅不同（因为无法获得原始的锅）。与其他16个任务不同，对于这个特定的任务，在展开策略之前，我们直接将机器人的末端执行器置于茄子上方；因此，我们将此称为“放茄子入锅”的“简易版”。 </li>
                    <li><strong>将茄子放入锅中：</strong>这是与上述任务相同的任务，不同之处在于机器人的末端执行器不是直接在茄子上方初始化。相反，我们在所有展开过程中固定其位置进行初始化，这意味着机器人必须首先水平地抓住茄子才能操作它。（注意：以下描述的其他任务也适用于此）这是一项视觉泛化任务，原因与上面相同。</li>
                    <li><strong>将杯子从柜台放入水槽：</strong>机器人的目标是拿起厨房柜台或烘干架上的粉色杯子，并将其放在右侧的水槽中。这是一个视觉概括任务，因为我们使用的是粉色杯子而不是蓝色杯子（在原始BridgeData V2数据集中使用了蓝色杯子，但我们发现我们评估的所有方法都无法可靠地操纵它——最有可能是因为杯子的颜色与水槽的颜色融为一体）。</li>
                    <li><strong>将茄子放入锅中（有杂乱）：</strong>这是一个与“将茄子放入锅中”的任务相同的任务，只是由于存在几个干扰对象而更加困难。它是一个视觉概括任务，原因与正常“将茄子放入锅中”任务中的相同，并且在场景中未见的干扰对象的情况下，其难度更大。当机器人朝向正确的目标物体移动时，会得到部分分数（满分1分）。</li>
                    <li><strong>将黄色玉米放在粉红色盘子上：</strong>机器人的目标是拿起黄色的玉米并将其放在粉红色的盘子上。由于场景中存在未见过的干扰对象，例如水槽后部柜台上的绿色恐龙，这是一个视觉泛化任务。当机器人朝向正确的目标物体移动时，会得到部分分数（满分1分中的0.5分）。</li>
                    <li><strong>搬运茄子：</strong>机器人的目标是抓住并举起茄子。这是一个运动泛化任务，因为茄子在未见的位置和/或方向被初始化，并且机器人被迫移动到其训练分布之外的位置和/或方向，并经常进行远距离抓取以完成任务。（注意：在原始BridgeData V2演示中，在此环境中没有展示远距离抓取；请参阅附录B.1.2的详细信息。）我们发现这个看似简单的任务对许多策略来说具有欺骗性的挑战性。当机器人与茄子接触时，将获得部分分数（0.5分中的1分）。 </li>
                    <li><strong>将胡萝卜放在盘子上（高度变化）：</strong>机器人的目标是拿起胡萝卜并将其放在黄色的盘子上。这是一个运动泛化任务，因为盘子从水槽底部的正常位置升高了，并且机器人必须调整其轨迹以正确地将胡萝卜放置在升高的平台上（而不会撞到盘子）。当机器人抓住胡萝卜并与之接触时，会奖励部分分数（满分1分中的0.5分）。 </li>
                    <li><strong>将胡萝卜放在盘子上：</strong>这是与上述任务相同的任务，只是盘子处于其正常位置（水槽或烘干架底部）。我们将其视为物理一般化任务，因为胡萝卜的大小和形状不同于原始BridgeData V2数据集中的一个，它更短更窄。（请注意，上面列出的前一版本的任务也从技术上讲是物理一般化任务，因为它涉及相同的胡萝卜，但我们将其归类为“运动一般化”，因为那是重点所在。） </li>
                    <li><strong>翻转锅：</strong>机器人的目标是操纵锅，使其在剧集结束时朝上放在水槽中。这是一个物理概括任务，因为这个锅的大小和形状与原始BridgeData V2培训演示中的不同（我们使用的锅更宽更短）。 </li>
                    <li><strong>提升AAA电池：</strong>机器人的目标是简单地抓住AAA电池并将其提升到空中。这被认为是物理通用化任务，因为电池比在BridgeData V2训练演示中看到的目标对象小得多且薄得多；请参阅附录B.1.2的详细信息。（请注意，这个目标对象在本环境中的原始BridgeData V2演示中不存在，因此这也是一个“语义通用化”的实例，但我们将它归类为“物理通用化”，因为它是这里的主要焦点）。 </li>
                    <li><strong>将头骨移动到干燥架上：</strong>机器人的目标是抓住头骨玩具并将其放入水槽左侧的黄色干燥架中。这是一个语义概括任务，因为头骨是一个未见的目标对象（在BridgeData V2训练演示中不出现）。 </li>
                    <li><strong>提升白色胶带：</strong>机器人的目标是抓住并提升白色的胶带卷到空中。这是一个语义泛化任务，因为白色的胶带卷是一个未见的目标对象（在BridgeData V2训练演示中没有出现）。请注意，这个任务也可以被视为“物理泛化”，因为它与训练演示中的物体形状不同；大多数策略难以抓取带有这种环形结构的物体，并且它们经常直接将机器人末端执行器移动到中心区域。 </li>
                    <li><strong>将紫色葡萄从锅中取出：</strong>机器人的目标是抓住钢锅中的紫色葡萄并将其移出（通过提起它或在锅外的任何地方掉落）。这是一个语义概括任务，因为这是未见过的语言指令；机器人从未在原始BridgeData V2训练数据集中看到过这个任务。 </li>
                    <li><strong>将蓝色杯子放在粉色杯子上：</strong>机器人的目标是抓住蓝色的杯子，并安全地将其放在粉色的杯子上面。这是一个语义概括任务，因为这是机器人从未见过的语言指令；在原始BridgeData V2训练数据集中，机器人从未在这个环境中看到过这个任务。当机器人抓住蓝色的杯子并用蓝色的杯子触碰粉色的杯子时，会得到部分分数（0.5分中的1分）。 </li>
                    <li><strong>将{茄子，红色瓶子}放入锅中：</strong>这是一个语言接地任务。机器人的目标是将指定的目标对象放入锅中。在场景中同时存在茄子和红色瓶子。我们进行配对评估：对于相同的初始状态，我们在一个回合中提示政策瞄准茄子，在下一个回合中则瞄准红色瓶子。我们用同一组5个初始状态测试每个方法，对于两种目标物体都使用相同的5个初始状态集。当机器人朝向正确的目标物体移动时，会得到部分分数（满分1分中的0.5分）。 </li>
                    <li><strong>拿起奶酪和红辣椒：</strong>这是一个语言接地任务。机器人的目标是抓住并举起指定的目标物体。我们按照上面的任务描述进行配对评估。当机器人朝向正确的目标物体移动时，会得到部分分数（满分1分中的0.5分）。 </li>
                    <li><strong>将{蓝色杯子，粉色杯子}放在盘子上：</strong>这是一个语言接地任务。机器人的目标是抓住指定的目标物体并将其放到盘子上。我们按照其他语言接地任务中描述的进行配对评估。当机器人朝向正确的目标物体移动时，会得到部分分数（满分1分中的0.5分）。 </li>
                </ol>
                <h5>B.1.2 Comparing Evaluation Tasks to Original BridgeData V2 Training Data</h5>
                <p> 我们在原始的BridgeData V2数据集[6]中使用的下水道环境中进行评估。我们复制环境以匹配原始环境，同时对机器人相对于下水道的位置以及相机相对于场景的位置进行了粗略估计。由于原始数据集中的这些位置的精确测量不足，我们无法重现确切的环境设置，并且由于机器人、水槽和相机的位置略有不同，自然分布偏移随之产生。此外，由于我们在训练演示收集地点之外评估机器人策略，因此其他自然分布偏移也随之产生。例如，照明条件和背景（例如，水槽后面的可见区域）不可避免地与在训练数据集中看到的不同。此外，我们无法获得原始BridgeData V2数据集所使用的精确对象集合，因此在训练时间和测试时间使用对象之间存在分布偏移。 </p>
                <p> 尽管面临这些挑战，我们发现某些通用政策，例如OpenVLA和RT-2-X仍然可以泛化并可靠地完成各种任务“即插即用”。其他通用政策，如RT-1-X和Octo也可以完成一些任务，尽管它们在我们的BridgeData V2评估套件中测试更困难的泛化任务时会遇到困难。 </p>
                <p>原始的BridgeData V2数据集包括在特定的接收环境中演示了以下七个任务：“翻转锅子”，“将胡萝卜放在盘子上”，“从柜台（或干燥架）中取出杯子放入水槽”，“将茄子放入锅中”，“将刀具放在砧板上”，“将勺子放入锅中”和“将杠杆垂直于前方”。请参见图8，以获取所有这些任务的所有样本图像。请注意，在此环境中收集的所有训练演示都是初始化的，即机器人末端执行器在每个剧集开始时直接位于目标对象上方。（但是，在BridgeData V2数据集中并非所有环境都如此；在某些其他环境中，机器人被初始化为远离目标对象的位置，因此它必须首先水平地抓住物体才能对其进行操作）。 </p>
                <div class="figure-caption">
                    <figure>
                        <img src="./OpenVLA_figs/fig8.png" alt="Language-Table Performance">
                        <figcaption> 图8：原始BridgeData V2接收器环境任务。来自原始BridgeData V2数据集的接收器环境中样本演示的图像显示，该环境中的所有演示都是初始化的，使得机器人的末端执行器直接位于目标对象上方。请注意，这些初始状态与我们在图7中展示的BridgeData V2评估任务中使用的初始状态不同。在我们的评估中，我们总是将机器人末端执行器初始化到水槽上方的一个固定位置，而不是将其直接定位在目标对象上方（除了一个任务：“把茄子放入锅里（简易版）”）。 </figcaption>
                    </figure>
                </div>
                <p> 在我们的BridgeData V2评估套件中，仅有一个任务——“将茄子放入锅（简易版）”——初始化时机器人末端执行器直接悬停在目标对象上方；在其余的16个任务中，末端执行器被初始化为固定位置上方的水槽上，使得机器人必须水平地向物体移动。这种初始条件与我们在评估套件中引入的各种类型的OOD泛化分布偏移相结合，挑战了通用策略，并且需要很高的鲁棒性才能成功完成任务。因此，RT-1-X和Octo等政策的成功率低于先前工作的报告值。然而，我们发现其他政策如RT-2-X和OpenVLA仍然能够相对有效地应对这些分布偏移和挑战。 </p>
                <h5>B.1.3 Detailed BridgeData V2 Evaluation Results</h5>
            </div>
            <p><strong>比较。</strong>我们比较了OpenVLA的性能与三个先前的一般性操纵策略：RT-1-X [1]，RT-2-X [1] 和Octo [5] 。 RT-1-X（350万参数）和Octo（930万参数）是基于从OpenX数据集子集中训练的Transformer策略；Octo是开源操纵策略中的最先进的模型。 RT-2-X（55亿个参数）是一个最先进的、闭源的VLA，它利用互联网预训练的视觉和语言骨干。 </p>
            <p> 结果总结在图3中，BridgeData V2评估和图4中的谷歌机器人评估（表4和表6的任务分解）。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./OpenVLA_figs/fig3.png" alt="Language-Table Performance">
                    <figcaption> 图3：BridgeData V2 WidowX机器人评估任务和结果。我们对OpenVLA和以前的最先进的通用机器人策略进行了全面的任务评估，这些任务涵盖了几个方面的泛化能力，以及专门评估语言调节能力的任务。OpenVLA在所有类别中都取得了最高的整体性能，并且除了语义泛化之外，在所有类别中甚至超过了闭源模型RT-2-X。平均成功率±StdErr是在每个方法上总共170次卷出计算得出的。详细的结果见表4。 </figcaption>
                </figure>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./OpenVLA_figs/fig4.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图4：谷歌机器人评估结果。我们在RT-1和RT-2评估中使用的移动机械臂上对通用机器人策略进行了分布内和分布外（OOD）任务的评估[2，7]。我们发现Open-VLA和RT-2-X取得了可比的表现，并且在总体上显著优于RT-1-X和Octo。平均成功率±StdErr是在每个方法总共60次卷出中计算得出的。详见表6中的详细结果。 </figcaption>
                </figure>
            </div>
            <p>我们发现RT-1-X和Octo在测试任务上表现不佳，经常无法操纵正确的物体，尤其是在干扰物存在的情况下，并且有时导致机器人无目的地挥动手臂。请注意我们的评估比那些先前工作所进行的评估更广泛地测试了泛化程度，以挑战互联网预训练的VLA模型。因此，没有互联网预训练的模型性能较低是预期的。RT-2-X明显优于RT-1-X和Octo，证明大型、预先训练过的VLM对机器人的好处。 </p>
            <p>值得注意的是，OpenVLA在Google机器人评估中与RT-2-X相当，并且在BridgeData V2评估中显著优于RT-2-X（尽管其参数量级小一个数量级）。定性上，我们发现RT-2-X和OpenVLA都比其他测试模型表现出更稳健的行为，例如当有干扰对象时接近正确的对象、正确地将机器人的末端执行器对准目标对象的取向以及甚至从错误中恢复过来（如不安全抓握物体）等。如https://openvla.github.io所示，在语义泛化任务中，RT-2-X的表现更高，这并不奇怪，因为其使用了规模更大的互联网预训练数据，并且通过同时使用机器人动作数据和互联网预训练数据进行联合微调来更好地保留预训练知识，而不是仅进行基于机器人数据的微调，如OpenVLA。然而，在BridgeData V2和Google机器人评估中，OpenVLA在所有其他任务类别中的表现与RT-2-X相当或更好。性能差异可以归因于多种因素的组合：我们为OpenVLA精心挑选了一个更大的训练数据集（有97万条轨迹，而RT-2-X只有35万条）；我们在训练数据集中进行了更仔细的清理，并且例如过滤了Bridge数据集中的所有零动作（见附录C进行详细讨论）；并且OpenVLA使用一个融合视觉编码器，该编码器结合了预训练的语义和空间特征。请参阅附录D对这些组件的消融分析。 </p>
            <h4>5.2 Data-Efficient Adaptation to New Robot Setups</h4>
            <p>先前的工作主要侧重于直接评估“开箱即用”的VLAs [1，7，16] ，而对有效微调VLA模型以适应新任务和机器人设置的研究则很少涉及，但这是广泛采用它们的关键。在本节中，我们研究了OpenVLA快速适应新的真实世界机器人设置的能力。（请参阅附录E中的模拟微调实验。） </p>
            <p><strong>机器人设置和任务。</strong>我们测试了OpenVLA模型的简单微调配方：对所有模型参数进行完全微调，使用包含10到150个目标任务演示的小数据集（见图5；我们在第5.3节中探索参数高效的微调方法）。我们在两个设置下测试OpenVLA：Franka-Tabletop，一个固定在桌子上的弗兰卡Emika Panda七自由度机器人手臂；以及Franka-DROID，来自最近发布的DROID数据集的弗兰卡机器人手臂设置[11]，安装在可移动的站立式办公桌上。设置使用了分别非阻塞控制器5赫兹和15赫兹。我们选择Franka机器人手臂作为我们的精细调整实验的目标实现，因为它们广泛用于机器人学习社区，并且可能是OpenVLA精细调整的“目标”。我们测试不同控制频率的设置以测试OpenVLA对各种用例的适用性。 </p>
            <div class="figure-caption">
                <figure>
                    <img src="./OpenVLA_figs/fig5.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图5：适应新的机器人设置。我们评估了从头开始在七个Franka Emika Panda任务（每个任务有10-150个演示）上训练的最先进的扩散策略，以及通用机器人政策Octo和OpenVLA对相同数据进行微调的结果。扩散策略在狭窄的单一指令任务中表现出色，而Octo和OpenVLA则在涉及多个指令和干扰物对象的各种精细调整任务中表现更好。总体而言，OpenVLA在两种设置下都取得了最高的综合性能，这表明它是一个有效的默认值，用于学习下游任务上的策略。平均成功率±标准误差是在每种方法的129次滚动测试（99次针对Franka-Tabletop任务和30次针对Franka-DROID任务）中计算得出的。详细结果见表7。 </figcaption>
                </figure>
            </div>
            <p><strong>比较。</strong>我们与Diffusion Policy [3] 进行比较，这是一种最先进的数据高效模仿学习方法，从头开始训练。我们也与Diffusion Policy (匹配)进行比较，这是Diffusion Policy的一个版本，它与OpenVLA的输入和输出规格相匹配。此外，我们评估Octo [5] 在目标数据集上微调的结果，因为它目前是支持微调的最佳通用策略（RT-2-X的微调不通过其推理API支持）。我们还对同一目标数据集上的OpenVLA进行微调，并将得到的策略称为OpenVLA。最后，作为一项消融实验，我们将直接在目标机器人设置中对底层Prismatic VLM进行微调——而不是对预先训练好的OpenX/OpenVLA模型进行微调——以评估大规模机器人预训练的好处。 </p>
            <div class="appendix-box">
                <p>全扩散策略使用具有图像和本体感觉状态的两步观察历史，并通过预测T个未来动作来执行后退视距控制，然后在预测下一个块之前以开环方式执行前X个动作（对于15赫兹控制，我们设置T = 16，X = 8，就像DROID之前的工程一样；对于5赫兹控制，我们将块大小减少到T = 8，X = 3）。它也是第5.2节中唯一一种预测绝对笛卡尔坐标来控制机器人的方法；所有其他方法都使用相对位置控制。扩散策略(匹配)使用单张图片作为输入，没有本体感觉信息也没有观察历史，并且不进行动作分段地预测一个单一的相对位置控制动作。 </p>
            </div>
            <p>我们将在附录中的表7中展示结果。我们发现，Diffusion Policy的两个版本在“将胡萝卜放入碗中”和“将玉米倒入锅中”等更窄单指令任务上与Octo和OpenVLA的一般政策竞争或表现更好，但预训练的一般政策在涉及多个对象并在场景中需要语言条件的更多样化的微调任务上表现更好。对于Octo和OpenVLA，OpenX预训练使模型更好地适应这些多样化的任务，在这些任务中，语言接地很重要；我们在Scratch(从头开始)上的较低性能中看到了这一点。 </p>
            <p>总体而言，我们发现OpenVLA的平均性能最高。值得注意的是，大多数先前的工作仅在狭窄单指令或多样多指令任务中表现出强大的性能，导致成功率差异很大。OpenVLA是唯一一种在所有测试任务中至少达到50％成功率的方法，这表明它可能是模仿学习任务中的一个强有力的默认选项，特别是如果它们涉及多种语言指令集。对于更窄但高度灵巧的任务，Diffusion Policy仍然显示出更平滑和精确的轨迹；将动作块和时间平滑结合起来，如Diffusion Policy所实现的那样，可能有助于OpenVLA达到相同的灵巧度，并且可能是未来工作的有前途的方向（见第6节对当前局限性的详细讨论）。 </p>
        
        </section>

        <!-- ====================================================================== -->
        <!-- PART 3: 论文重点代码/实现解析 (Code & Implementation Analysis)         -->
        <!-- ====================================================================== -->
        <!-- 这是我为你解析的部分。未来你可以将自己的代码分析放在这里。 -->
        <section id="code-analysis">
            <h2>论文重点实现解析</h2>
            <div class="code-analysis">
                <h3>1. 动作的文本化 (Action Tokenization)</h3>
                <p>这是实现VLA模型的关键。论文将一个7维的机器人动作（6自由度位移 + 1个夹爪状态）和一个终止命令，全部离散化并映射为文本标记。</p>
                <pre><code><span class="comment"># 动作空间 (Action Space)</span>
                - 6-DoF position/rotation displacement (末端执行器6自由度位移)
                - Gripper extension (夹爪开合度)
                - Terminate episode (终止指令)

                <span class="comment"># 离散化 (Discretization)</span>
                - 每个连续维度被均匀地划分为 <span class="value">256</span> 个桶 (bins)。
                - 因此，一个完整的机器人动作可以表示为 <span class="value">8</span> 个整数。

                <span class="comment"># 文本格式 (Text Representation)</span>
                - 动作向量被转换成一个由空格分隔的字符串。
                - 例如: "<span class="token">terminate</span> <span class="token">Δpos_x</span> <span class="token">Δpos_y</span> <span class="token">Δpos_z</span> <span class="token">Δrot_x</span> <span class="token">Δrot_y</span> <span class="token">Δrot_z</span> <span class="token">gripper_state</span>"
                - 实际输出可能像这样: "<span class="value">1 128 91 241 5 101 127 255</span>"
                </code></pre>
                                
                                <h3>2. 联合微调 (Co-finetuning)</h3>
                                <p>为了让模型既不忘记从网络数据中学到的通用知识，又能学会机器人控制，训练时采用了联合微调策略。</p>
                                <pre><code><span class="comment"># 训练数据混合 (Training Data Mix)</span>
                - 机器人轨迹数据 (Robot Trajectories)
                - 网络规模的视觉语言数据 (Web-scale VQA, captioning data, etc.)

                <span class="comment"># 关键技术 (Key Technique)</span>
                - 在每个训练批次中，同时包含机器人数据和网络数据。
                - 增加机器人数据的采样权重 (e.g., <span class="value">50%</span> for RT-2-PaLI-X, <span class="value">66%</span> for RT-2-PaLM-E)，以确保模型充分学习控制策略。
                </code></pre>

                                <h3>3. 模型输入与输出约束</h3>
                                <p>模型以标准的视觉问答 (VQA) 格式接收任务。</p>
                                <pre><code><span class="comment"># 输入格式 (Input Format)</span>
                - 图像: [robot_camera_image]
                - 文本提示: "问: 机器人应该采取什么行动来 [任务说明]? 答:"
                - 示例: "问: 机器人应该采取什么行动来 <strong>捡起苹果</strong>? 答:"

                <span class="comment"># 输出约束 (Output Constraining)</span>
                - 在执行机器人任务时，模型的解码词汇表被限制为仅包含 <span class="value">256</span> 个有效动作标记，确保生成合法的动作指令。
                - 在回答普通VQA问题时，则可以使用完整的词汇表。
                </code></pre>

            </div>
        </section>
    </div>

</body>
</html>