<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>论文笔记 | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=Roboto+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/python.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
        :root {
            --primary-color: #005f73;
            --secondary-color: #0a9396;
            --background-color: #f8f9fa;
            --text-color: #212529;
            --heading-font: 'Noto Sans SC', sans-serif;
            --body-font: 'Noto Sans SC', sans-serif;
            --code-font: 'Roboto Mono', monospace;
            --border-color: #dee2e6;
            --card-bg: #ffffff;
            --shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        }

        body {
            font-family: var(--body-font);
            line-height: 1.8;
            color: var(--text-color);
            background-color: var(--background-color);
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 850px;
            margin: 0 auto;
            background-color: var(--card-bg);
            padding: 30px 30px;
            border-radius: 12px;
            box-shadow: var(--shadow);
        }

        .paper-title {
            font-family: var(--heading-font);
            font-weight: 700;
            font-size: 2.2em;
            color: var(--primary-color);
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 5px;
        }

        .paper-subtitle {
            font-family: var(--heading-font);
            font-size: 1.2em;
            color: #6c757d;
            margin-top: 0;
            margin-bottom: 40px;
        }

        h2 {
            font-family: var(--heading-font);
            font-size: 1.8em;
            color: var(--primary-color);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 8px;
            margin-top: 50px;
        }

        h3 {
            font-family: var(--heading-font);
            font-size: 1.4em;
            color: var(--secondary-color);
            margin-top: 30px;
        }

        h4 {
            font-family: var(--heading-font);
            font-size: 1em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        h5 {
            font-family: var(--heading-font);
            font-size: 1.0em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        p, li {
            font-size: 1em;
            text-align: justify;
        }
        
        strong {
            color: var(--primary-color);
        }

        /* 重点总结区域样式 */
        .summary-box {
            background-color: #eef7f8;
            border-left: 5px solid var(--secondary-color);
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .summary-box h3 {
            margin-top: 0;
            color: var(--primary-color);
        }

        .summary-box ul {
            padding-left: 20px;
        }

        .summary-box li {
            margin-bottom: 10px;
        }

        /* 代码/实现解析区域样式 */
        .code-analysis pre {
            background-color: #282c34;
            color: #abb2bf;
            font-family: var(--code-font);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.5;
            font-size: 0.9em;
        }
        
        .code-analysis code .token{
            color: #61afef; /* 蓝色，用于变量/标记 */
        }
        .code-analysis code .comment{
            color: #5c6370; /* 灰色，用于注释 */
        }
        .code-analysis code .value{
            color: #98c379; /* 绿色，用于值 */
        }

        /* 图像和表格说明文字样式 */
        .figure-caption, .table-caption {
            background-color: #f1f3f5;
            border: 1px solid var(--border-color);
            padding: 10px;
            margin: 20px auto;
            border-radius: 8px;
            font-style: italic;
            color: #495057;
            text-align: center;
        }
        .figure-caption img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .img-small {
            width: 30%; /* 图片宽度为容器的 30% */
            max-width: 100%; /* 安全措施，确保不会超出 */
        }

        .img-medium {
            width: 50%; /* 图片宽度为容器的 50% */
            max-width: 100%;
        }

        .img-large {
            width: 75%; /* 图片宽度为容器的 75% */
            max-width: 100%;
        }

        blockquote {
            border-left: 4px solid var(--border-color);
            padding-left: 20px;
            color: #6c757d;
            margin-left: 0;
        }

        .appendix-box {
            background-color: #f1f3f5; /* 浅灰色背景 */
            border-left: 5px solid #adb5bd; /* 中性灰色边框 */
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .appendix-box h3 {
            margin-top: 0;
            color: #495057; /* 深灰色标题，以在灰色背景上保持清晰 */
        }

        .data-table {
            width: 100%;
            border-collapse: collapse; /* 合并边框 */
            margin: 30px 0;
            font-size: 0.95em;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden; /* 确保圆角生效 */
        }

        .data-table caption {
            caption-side: bottom; /* 将标题放在表格下方，与图片说明保持一致 */
            margin-top: 10px;    /* 与表格的间距 */
            padding: 5px;
            font-size: 0.9em;
            font-style: italic;
            color: #6c757d;      /* 使用柔和的灰色 */
            text-align: center;
        }

        .data-table thead tr {
            background-color: var(--primary-color);
            color: #ffffff;
            text-align: left;
            font-weight: bold;
        }

        .data-table th, .data-table td {
            padding: 12px 15px;
            border-bottom: 1px solid var(--border-color);
        }

        .data-table tbody tr {
            background-color: #ffffff;
        }

        /* 斑马条纹，增加可读性 */
        .data-table tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }

        .data-table tbody tr:last-of-type {
            border-bottom: 2px solid var(--primary-color);
        }

        /* 任务组列加粗，突出显示 */
        .data-table td:first-child {
            font-weight: bold;
            color: var(--primary-color);
        }

        pre code.hljs {
            font-family: Consolas; /* 设置字体 */
            font-size: 16px;                        /* 设置字号 */
            background-color: #000000;              /* 可选：背景色 */
            padding: 1em;                           /* 可选：内边距 */
            display: block;                         /* 确保是块级元素 */
            white-space: pre-wrap;                  /* 保留空格和换行 */
            word-wrap: break-word;                  /* 长单词自动换行 */
        }
    </style>
</head>
<body>

    <div class="container">
        
        <h1 class="paper-title">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h1>
        <p class="paper-subtitle"></p>

        <!-- ====================================================================== -->
        <!-- PART 2: 论文重点总结 (Key Takeaways)                                   -->
        <!-- ====================================================================== -->
        <!-- 这是我为你总结的部分。未来你可以将自己的总结放在这里。 -->
        <section id="summary">
            <h2>论文重点总结</h2>
            <div class="summary-box">
                <h3>核心思想与贡献</h3>
                <p>Swin Transformer 是一篇具有里程碑意义的论文，它成功地将 Transformer 架构从自然语言处理（NLP）领域推广为计算机视觉（CV）领域的通用骨干网络，在各大主流视觉任务上均取得了顶尖的性能。</p>
                <p>论文旨在解决<strong>标准 Vision Transformer (ViT) 应用于计算机视觉时的两大挑战：</strong></p>
                <ul>
                    <li><strong>多尺度问题：</strong>视觉任务中的物体尺寸变化非常大，而 ViT 使用固定大小的图像块（Patch），难以适应这种变化。</li>
                    <li><strong>计算复杂度问题：</strong>ViT 的自注意力计算复杂度与图像尺寸的平方成正比，对于高分辨率图像（如语义分割、目标检测所需）来说，计算量过大。</li>
                </ul>
                <p><strong>Transformer 的两大核心创新：</strong> </p>
                <ul>
                    <li><strong>分层架构 (Hierarchical Architecture)：</strong>类似于卷积神经网络（CNN）的工作方式，Swin Transformer 通过一种名为 Patch Merging 的模块，在网络加深的过程中逐步合并相邻的图像块，从而减小特征图的分辨率、增加感受野。这种设计使其能够产生不同尺度的特征图，可以方便地与现有的下游任务框架（如用于目标检测的 FPN、用于分割的 U-Net）结合，成为一个通用的视觉骨干网络。</li>
                    <li><strong>移动窗口自注意力 (Shifted Window based Self-Attention, W-MSA/SW-MSA)：</strong>
                        <ul>
                            <li><strong>常规窗口自注意力 (W-MSA)：</strong>为了降低计算量，模型不再计算全局的自注意力，而是将图像划分为多个不重叠的局部窗口（Window），只在每个窗口内部计算自注意力。由于窗口大小固定，计算复杂度从图像尺寸的二次方下降为线性关系。</li>
                            <li><strong>移动窗口自注意力 (SW-MSA)：</strong>仅使用 W-MSA 会导致窗口之间缺乏信息交流，限制了模型的表征能力。为此，Swin Transformer 采用了一种巧妙的 “移动窗口” 机制。在连续的两个 Transformer 模块中，第二个模块的窗口划分会相对于前一个模块进行位移（通常是半个窗口大小）。这种设计使得上一层中原本不相邻的图像块，在新的窗口中得以互动，从而实现了跨窗口的信息交流，大大增强了模型的性能。</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </section>

        

        <!-- ====================================================================== -->
        <!-- PART 1: 论文全文翻译 (Full Translation)                                -->
        <!-- ====================================================================== -->
        <!-- 这是你提供的笔记内容，我已经帮你格式化好了。 -->
        <section id="translation">
            <h2>论文全文翻译</h2>

            <h3>Abstract</h3>
            <p>本文提出了一种名为 Swin Transformer 的新型视觉 Transformer，它能够作为一个通用的计算机视觉骨干网络。将 Transformer 从语言领域适配到视觉领域存在诸多挑战，这些挑战源于两个领域之间的差异，例如视觉实体的尺度变化范围大，以及与文本中的单词相比，图像中的像素分辨率要高得多。为了解决这些差异，我们提出了一种分层式 Transformer，其表示是通过“移动窗口”（Shifted Windows）计算的。移动窗口方案通过将自注意力计算限制在不重叠的局部窗口内来提高效率，同时允许跨窗口连接。这种分层架构具有在不同尺度上建模的灵活性，并且其计算复杂度与图像大小呈线性关系。这些特性使得 Swin Transformer 能够兼容广泛的视觉任务，包括图像分类（在 ImageNet-1K 上达到 87.3% 的 Top-1 准确率）和密集预测任务，如目标检测（在 COCO test-dev 上取得 58.7 的 box AP 和 51.1 的 mask AP）和语义分割（在 ADE20K val 上取得 53.5 的 mIoU）。其性能在 COCO 上以 +2.7 box AP 和 +2.6 mask AP 的巨大优势，以及在 ADE20K 上以 +3.2 mIoU 的优势，超越了之前的最先进水平，展示了基于 Transformer 的模型作为视觉骨干网络的潜力。这种分层设计和移动窗口方法也被证明对全 MLP 架构是有益的。代码和模型已在 https://github.com/microsoft/Swin-Transformer 公开。</p>
            
            <h3>1. Introduction</h3>
            <p>长期以来，计算机视觉中的建模一直由卷积神经网络（CNNs）主导。从 AlexNet 及其在 ImageNet 图像分类挑战赛上的革命性表现开始，CNN 架构通过更大的规模、更广泛的连接以及更复杂的卷积形式而变得越来越强大。随着 CNN 作为各种视觉任务的骨干网络，这些架构上的进步带来了性能的提升，从而广泛地推动了整个领域的发展。</p>
            <p>另一方面，自然语言处理（NLP）中网络架构的演变走了一条不同的道路，如今流行的架构是 Transformer。Transformer 为序列建模和转导任务而设计，其显著特点是使用注意力机制来建模数据中的长程依赖关系。它在语言领域的巨大成功促使研究人员探索其在计算机视觉领域的应用，最近在某些特定任务，特别是图像分类和联合视觉-语言建模上，已经显示出有希望的结果。</p>
            <p>在本文中，我们寻求扩展 Transformer 的适用性，使其能够作为一个通用的骨干网络，就像它在 NLP 中和 CNN 在视觉中所扮演的角色一样。我们观察到，将其在语言领域的高性能迁移到视觉领域所面临的重大挑战，可以由两种模态之间的差异来解释。其中一个差异涉及尺度。与作为语言 Transformer 处理基本元素的单词标记不同，视觉元素的尺度可以有很大的变化，这个问题在目标检测等任务中受到了关注。在现有的基于 Transformer 的模型中，所有标记都是固定尺度的，这一特性不适合这些视觉应用。另一个差异是图像中的像素分辨率远高于文本段落中的单词数量。存在许多视觉任务，如语义分割，需要在像素级别进行密集预测，这对于在高分辨率图像上运行的 Transformer 来说是难以处理的，因为其自注意力的计算复杂度与图像大小成二次方关系。</p>
            <p>为了克服这些问题，我们提出了一种名为 Swin Transformer 的通用 Transformer 骨干网络，它构建了分层的特征图，并且对图像大小具有线性的计算复杂度。如图 1(a) 所示，Swin Transformer 通过从小型图像块（灰色轮廓）开始，并在更深的 Transformer 层中逐渐合并相邻的图像块，来构建一个分层的表示。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/fig1.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图1: 
                        <ul>
                            <li>(a) 我们提出的 Swin Transformer 通过在更深层中合并图像块（灰色部分显示）来构建分层的特征图，并且由于自注意力计算仅在每个局部窗口内（红色部分显示），其计算复杂度与输入图像大小呈线性关系。因此，它可以作为图像分类和密集识别任务的通用骨干网络。</li>
                            <li>(b) 相比之下，以前的视觉 Transformer 产生单一低分辨率的特征图，并且由于全局计算自注意力，其计算复杂度与输入图像大小呈二次方关系。</li>
                        </ul>
                    </figcaption>
                </figure>
            </div>
            <p>有了这些分层的特征图，Swin Transformer 模型可以方便地利用先进的技术进行密集预测，例如特征金字塔网络（FPN） 或 U-Net。线性的计算复杂度是通过在划分图像的不重叠窗口内局部计算自注意力来实现的（红色轮廓）。每个窗口中的图像块数量是固定的，因此复杂度与图像大小呈线性关系。这些优点使得 Swin Transformer 适合作为各种视觉任务的通用骨干网络，而之前的基于 Transformer 的架构产生的是单一分辨率的特征图，并且具有二次方的复杂度。</p>
            <p>Swin Transformer 的一个关键设计元素是其在连续自注意力层之间窗口分区的移动，如图 2 所示。移动的窗口连接了前一层的窗口，提供了它们之间的连接，从而显著增强了建模能力（见表 4）。这个策略在实际延迟方面也是高效的：一个窗口内的所有查询块共享相同的键集¹，这有助于硬件中的内存访问。相比之下，早期的基于滑动窗口的自注意力方法 由于不同查询像素有不同的键集，在通用硬件上延迟较低²。我们的实验表明，我们提出的移动窗口方法比滑动窗口方法的延迟低得多，但在建模能力上是相似的（见表 5 和 6）。移动窗口方法也被证明对全 MLP 架构 是有益的。</p>
            <div class="appendix-box">
                <p>¹ 查询和键是自注意力层中的投影向量。</p>
                <p>² 虽然有高效的方法在通用硬件上实现基于滑动窗口的卷积层，这得益于其共享的核权重，但对于基于滑动窗口的自注意力层来说，在实践中实现高效的内存访问是困难的。</p>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/fig2.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图2: 在所提出的Swin Transformer架构中，采用移位窗口方法计算自注意力的示意图。在第l层（左图），采用常规的窗口划分方案，并在每个窗口内计算自注意力。在下一层l+1（右图），窗口划分被移位，从而形成新的窗口。新窗口中的自注意力计算跨越了第l层中先前窗口的边界，实现了它们之间的连接。
                    </figcaption>
                </figure>
            </div>
            <p>微调代码和预训练模型可在 https://github.com/google-research/vision_transformer 获取</p>

            <h3>2. Related Work</h3>
            <p><strong>CNN 及其变体</strong> CNN 在整个计算机视觉领域都是标准的网络模型。虽然 CNN 已经存在了几十年，但直到 AlexNet 的引入，CNN 才得以起飞并成为主流。从那时起，更深、更有效的卷积神经网络被提出来，以进一步推动计算机视觉领域的深度学习浪潮，例如 VGG、GoogleNet、ResNet、DenseNet、HRNet 和 EfficientNet。除了这些架构上的进步，还有很多工作致力于改进单个卷积层，例如深度可分离卷积和可变形卷积。虽然 CNN 及其变体仍然是计算机视觉应用的主要骨干架构，但我们强调了类似 Transformer 的架构在视觉和语言之间统一建模方面的巨大潜力。我们的工作在几个基本的视觉识别任务上取得了强大的性能，我们希望它能为一个建模转变做出贡献。</p>
            <p><strong>基于自注意力的骨干架构</strong> 同样受到 NLP 领域自注意力层和 Transformer 架构成功的启发，一些工作采用自注意力层来替换流行的 ResNet 中的部分或全部空间卷积层。在这些工作中，自注意力是在每个像素的局部窗口内计算的，以加快优化，它们比对应的 ResNet 架构取得了稍微更好的准确率/FLOPs 权衡。然而，它们昂贵的内存访问导致其实际延迟明显大于卷积网络。我们建议在连续的层之间移动窗口，而不是使用滑动窗口，这允许在通用硬件中更高效地实现。</p>
            <p><strong>用于补充 CNN 的自注意力/Transformer</strong> 另一条工作路线是用自注意力层或 Transformer 来增强标准的 CNN 架构。自注意力层可以通过提供编码远距离依赖或异构交互的能力来补充骨干网络或头部网络。最近，Transformer 中的编码器-解码器设计已被应用于目标检测和实例分割任务。我们的工作探索了 Transformer 在基本视觉特征提取中的应用，并与这些工作是互补的。</p>
            <p><strong>基于 Transformer 的视觉骨干网络</strong> 与我们工作最相关的是 Vision Transformer (ViT) 及其后续工作。ViT 的开创性工作直接将 Transformer 架构应用于不重叠的中等大小图像块上进行图像分类。与卷积网络相比，它在图像分类上取得了令人印象深刻的速度-准确率权衡。虽然 ViT 需要大规模训练数据集（即 JFT-300M）才能表现良好，但 DeiT 引入了几种训练策略，使得 ViT 在使用较小的 ImageNet-1K 数据集时也能有效。ViT 在图像分类上的结果是令人鼓舞的，但由于其低分辨率的特征图和计算复杂度随图像大小二次增加，其架构不适合用作密集视觉任务的通用骨干网络，或者当输入图像分辨率高时。有一些工作通过直接上采样或反卷积将 ViT 模型应用于密集视觉任务，如目标检测和语义分割，但性能相对较低。与我们的工作同时进行的一些研究修改了 ViT 架构以获得更好的图像分类效果。根据经验，我们发现我们的 Swin Transformer 架构在图像分类上，在这些方法中取得了最佳的速度-准确率权衡，即使我们的工作侧重于通用性能而非专门针对分类。另一个同期的工作 探索了类似的思路，在 Transformer 上构建多分辨率特征图。它的复杂度仍然是图像大小的二次方，而我们的是线性的，并且我们的方法在局部操作，这在建模视觉信号中的高相关性方面已被证明是有益的。我们的方法既高效又有效，在 COCO 目标检测和 ADE20K 语义分割上都取得了最先进的准确率。</p>
                        
            <h3>3. Method</h3>
            <h4>3.1. Overall Architecture</h4>
            <p>Swin Transformer 架构的概述如图 3 所示，其中展示了微型版本（Swin-T）。它首先通过一个像 ViT 一样的切片模块（patch splitting module）将输入的 RGB 图像分割成不重叠的图像块。每个图像块被视为一个“令牌”（token），其特征被设置为原始像素 RGB 值的拼接。在我们的实现中，我们使用 4 × 4 的块大小，因此每个块的特征维度是 4 × 4 × 3 = 48。一个线性嵌入层（linear embedding layer）被应用于这个原始值特征，以将其投影到任意维度（表示为 C）。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/fig3.png" alt="Language-Table Performance">
                    <figcaption>图3: (a) Swin Transformer(Swin-T)的架构；(b) 两个连续的Swin Transformer模块（符号表示见式(3)）。W-MSA和SW-MSA分别是具有常规窗口和移位窗口配置的多头自注意力模块。
                    </figcaption>
                </figure>
            </div>
            <p>在这些图像块令牌上应用了几个带有改进自注意力计算的 Transformer 模块（Swin Transformer 模块）。Transformer 模块保持了令牌的数量 (\(H/4 × W/4\))，并与线性嵌入层一起被称为“阶段 1”。</p>
            <p>为了产生层次化表示，随着网络加深，令牌的数量通过图像块合并层（patch merging layers）来减少。第一个图像块合并层将每组 2 × 2 相邻图像块的特征连接起来，并在 4C 维的连接特征上应用一个线性层。这将令牌数量减少了 2 × 2 = 4 倍（分辨率的 2 倍下采样），输出维度被设置为 2C。之后应用 Swin Transformer 模块进行特征转换，分辨率保持在 \(H/8 × W/8\)。这个图像块合并和特征转换的第一个模块被称为“阶段 2”。这个过程重复两次，作为“阶段 3”和“阶段 4”，输出分辨率分别为 \(H/16 × W/16\) 和 \(H/32 × W/32\)。这些阶段共同产生一个层次化的表示，其特征图分辨率与典型的卷积网络（例如 VGG 和 ResNet）相同。因此，我们提出的架构可以方便地替换现有方法中用于各种视觉任务的骨干网络。</p>
            <p>Swin Transformer 模块 Swin Transformer 是通过将标准 Transformer 模块中的标准多头自注意力（MSA）模块替换为基于移动窗口的模块（在 3.2 节中描述）而构建的，其他层保持不变。如图 3(b) 所示，一个 Swin Transformer 模块由一个基于移动窗口的 MSA 模块组成，后面跟着一个中间带有 GELU 非线性的 2 层 MLP。在每个 MSA 模块和每个 MLP 之前都应用了一个 LayerNorm (LN) 层，并且在每个模块之后都应用了一个残差连接。</p>

            <h4>3.2. Shifted Window based Self-Attention</h4>
            <p>标准的 Transformer 架构 及其在图像分类中的应用[11] 都进行全局自注意力计算，即计算一个令牌与所有其他令牌之间的关系。全局计算导致了与令牌数量成二次方的复杂度，使其不适合于许多需要大量令牌进行密集预测或表示高分辨率图像的视觉问题。</p>
            <p><strong>非重叠窗口中的自注意力</strong> 为了高效建模，我们建议在局部窗口内计算自注意力。这些窗口被安排成以不重叠的方式均匀地划分图像。假设每个窗口包含 \(M × M\) 个图像块，一个全局 MSA 模块和一个基于窗口的模块在一个 \(h × w\) 个图像块的图像上的计算复杂度³分别为：</p>
            <p>$$\Omega(MSA)=4hwC^2+2(hw)^2C$$</p>
            <p>$$\Omega(W-MSA)=4hwC^2+2M^2hwC$$</p>
            <p>其中前者对于图像块数量 hw 是二次的，而后者在 M 固定时（默认设置为 7）是线性的。对于大的 hw，全局自注意力计算通常是不可负担的，而基于窗口的自注意力是可扩展的。</p>
            <div class="appendix-box">
                <p>³ 我们在确定复杂度时省略了 SoftMax 的计算。</p>
            </div>
            <p><strong>连续模块中的移动窗口分区</strong> 基于窗口的自注意力模块缺乏跨窗口的连接，这限制了其建模能力。为了在保持非重叠窗口高效计算的同时引入跨窗口连接，我们提出了一种移动窗口分区方法，该方法在连续的 Swin Transformer 模块中交替使用两种分区配置。</p>
            <p>如图 2 所示，第一个模块使用常规的窗口分区策略，从左上角像素开始，将 8 × 8 的特征图均匀地划分为 2 × 2 个大小为 4 × 4 (M = 4) 的窗口。然后，下一个模块采用一个从前一层移位的窗口配置，通过将窗口从规则分区的窗口位移 (\(⌊M/2⌋, ⌊M/2⌋\)) 个像素来实现。</p>
            <p>通过移动窗口分区方法，连续的 Swin Transformer 模块计算如下：</p>
            <p>$$ẑˡ = W-MSA(LN(zˡ⁻¹)) + zˡ⁻¹$$</p>
            <p>$$zˡ = MLP(LN(ẑˡ)) + ẑˡ$$</p>
            <p>$$ẑˡ⁺¹ = SW-MSA(LN(zˡ)) + zˡ$$</p>
            <p>$$zˡ⁺¹ = MLP(LN(ẑˡ⁺¹)) + ẑˡ⁺¹$$</p>
            <p>其中 ẑˡ 和 zˡ 分别表示块 l 的 (S)WMSA 模块和 MLP 模块的输出特征；W-MSA 和 SW-MSA 分别表示使用常规和移动窗口分区配置的基于窗口的多头自注意力。</p>
            <p>移动窗口分区方法引入了前一层相邻不重叠窗口之间的连接，并被发现在图像分类、目标检测和语义分割中是有效的，如表 4 所示。</p>
            <p><strong>移动配置的高效批处理计算</strong> 移动窗口分区的一个问题是它会导致更多的窗口，从 \(⌈h/M⌉ × ⌈w/M⌉\) 到 \((⌈h/M⌉+1) × (⌈w/M⌉+1)\) 个，并且一些窗口会比 \(M × M\) 小⁴。一个简单的解决方案是将较小的窗口填充到 \(M × M\) 的大小，并在计算注意力时屏蔽掉填充的值。当常规分区的窗口数量很少时，例如 2 × 2，这个简单解决方案带来的计算增加是可观的（2 × 2 → 3 × 3，增加了 2.25 倍）。在这里，我们提出了一种更高效的批处理计算方法，通过向左上方向循环移位，如图 4 所示。经过这次移位后，一个批处理的窗口可能由几个在特征图中不相邻的子窗口组成，因此采用了一个掩蔽机制，将自注意力的计算限制在每个子窗口内。通过循环移位，批处理窗口的数量与常规窗口分区的数量保持相同，因此也是高效的。这种方法的低延迟如表 5 所示。</p>
            <div class="appendix-box">
                <p>⁴ 为了使窗口大小 (M, M) 能被特征图大小 (h, w) 整除，如果需要，会在特征图的右下角进行填充。</p>
            </div>            
            <p><strong>相对位置偏置</strong> 在计算自注意力时，我们遵循[10] 的做法，在计算相似度时为每个头加入一个相对位置偏置 \(B ∈ ℝ^{M²×M²}\)：</p>
            <p>$$Attention(Q, K, V) = SoftMax(QKᵀ/\sqrt d + B)V$$</p>
            <p>其中 \(Q, K, V ∈ ℝ^{M²×d}\) 是查询、键和值矩阵；d 是查询/键的维度，\(M²\) 是一个窗口中的图像块数量。由于沿每个轴的相对位置在 \([-M+1, M-1]\) 范围内，我们参数化一个更小尺寸的偏置矩阵 \(B̂ ∈ ℝ^{(2M-1)×(2M-1)}\)，B 中的值取自 B̂。</p>
            <p>我们观察到，相比于没有这个偏置项或使用绝对位置嵌入的对应模型，有显著的改进，如表 4 所示。进一步像[11] 中那样向输入添加绝对位置嵌入会略微降低性能，因此在我们的实现中没有采用。</p>
            <p>预训练中学习到的相对位置偏置也可以用于初始化一个模型，以便用不同的窗口大小进行微调，方法是双三次插值。</p>

            <h4>3.3. Architecture Variants</h4>
            <p>我们构建了我们的基础模型，称为 Swin-B，其模型大小和计算复杂度与 ViT-B/DeiT-B 相似。我们还引入了 Swin-T、Swin-S 和 Swin-L，它们分别是大约 0.25 倍、0.5 倍和 2 倍模型大小和计算复杂度的版本。请注意，Swin-T 和 Swin-S 的复杂度分别与 ResNet-50 (DeiT-S) 和 ResNet-101 相似。窗口大小默认设置为 M = 7。每个头的查询维度为 d = 32，每个 MLP 的扩展层 α = 4，适用于所有实验。这些模型变体的架构超参数是：</p>
            <ul>
                <li>Swin-T: C = 96, layer numbers = {2, 2, 6, 2}</li>
                <li>Swin-S: C = 96, layer numbers ={2, 2, 18, 2}</li>
                <li>Swin-B: C = 128, layer numbers ={2, 2, 18, 2}</li>
                <li>Swin-L: C = 192, layer numbers ={2, 2, 18, 2}</li>
            </ul>
            <p>其中 C 是第一阶段隐藏层的通道数。模型大小、理论计算复杂度 (FLOPs) 和 ImageNet 图像分类模型变体的吞吐量列在表 1 中。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/tab1.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>表1. 不同骨干网络在ImageNet-1K分类任务上的比较。吞吐量的测量采用[68]的GitHub仓库和V100 GPU，参考[63]。
                    </figcaption>
                </figure>
            </div>

            <h3>4. Experiments</h3>
            <p>我们在 ImageNet-1K 图像分类、COCO 目标检测 和 ADE20K 语义分割 上进行了实验。在接下来的部分，我们首先将提出的 Swin Transformer 架构与之前在三个任务上的最先进技术进行比较。然后，我们对 Swin Transformer 的重要设计元素进行消融研究。</p>
            <h4>4.1. Image Classification on ImageNet-1K</h4>
            <p><strong>设置</strong> 对于图像分类，我们在 ImageNet-1K 上对提出的 Swin Transformer 进行基准测试，该数据集包含 128 万张训练图像和 5 万张验证图像，来自 1000 个类别。报告的是单个裁剪图像的 top-1 准确率。我们考虑两种训练设置：</p>
            <ul>
                <li><strong>常规 ImageNet-1K 训练。</strong> 这个设置主要遵循。我们使用 AdamW 优化器进行 300 个周期的训练，使用余弦衰减学习率调度器和 20 个周期的线性热身。批处理大小为 1024，初始学习率为 0.001，权重衰减为 0.05。我们包含了 中的大多数增强和正则化策略，除了重复增强 和 EMA，它们没有提升性能。注意，这与 相反，其中重复增强对于稳定 ViT 的训练至关重要。</li>
                <li><strong>在 ImageNet-22K 上预训练并在 ImageNet-1K 上微调。</strong> 我们还在更大的 ImageNet-22K 数据集上进行预训练，该数据集包含 1420 万张图像和 22K 个类别。我们使用 AdamW 优化器进行 90 个周期的训练，使用线性衰减学习率调度器和 5 个周期的线性热身。批处理大小为 4096，初始学习率为 0.001，权重衰减为 0.01。在 ImageNet-1K 微调中，我们训练模型 30 个周期，批处理大小为 1024，恒定学习率为 10⁻⁵，权重衰减为 10⁻⁸。</li>
            </ul>
            <p><strong>常规 ImageNet-1K 训练结果</strong> 表 1(a) 展示了与其他骨干网络的比较，包括基于 Transformer 和基于 ConvNet 的网络，使用常规 ImageNet-1K 训练。与之前的最先进的基于 Transformer 的架构，即 DeiT 相比，Swin Transformers 在相似的复杂度下显著超过了对应的 DeiT 架构：Swin-T (81.3%) 比 DeiT-S (79.8%) 在 224² 输入上高出 +1.5%，Swin-B (83.3%/84.5%) 比 DeiT-B (81.8%/83.1%) 在 224²/384² 输入上分别高出 +1.5%/1.4%。与最先进的 ConvNets，即 RegNet 和 EfficientNet 相比，Swin Transformer 实现了稍好的速度-准确率权衡。注意到虽然 RegNet 和 EfficientNet 是通过彻底的架构搜索获得的，但提出的 Swin Transformer 是从标准 Transformer 改编而来的，并且有进一步改进的巨大潜力。</p>
            <p><strong>ImageNet-22K 预训练结果</strong> 我们还预训练了更大容量的 Swin-B 和 Swin-L 在 ImageNet-22K 上。在 ImageNet-1K 图像分类上微调的结果如表 1(b) 所示。对于 Swin-B，ImageNet-22K 预训练比从头开始在 ImageNet-1K 上训练带来了 1.8%~1.9% 的增益。与之前 ImageNet-22K 预训练的最佳结果相比，我们的模型取得了明显更好的速度-准确率权衡：Swin-B 获得了 86.4% 的 top-1 准确率，比相似推理吞吐量的 ViT (84.7 vs. 85.9 images/sec) 高出 2.4%，并且 FLOPs 稍低 (47.0G vs. 55.4G)。更大的 Swin-L 模型达到了 87.3% 的 top-1 准确率，比 Swin-B 模型高出 +0.9%。</p>
            
            <h4>4.2. Object Detection on COCO</h4>
            <p><strong>设置</strong> 目标检测和实例分割实验在 COCO 2017 上进行，该数据集包含 118K 训练、5K 验证和 20K 测试开发图像。消融研究使用验证集进行，并在测试开发集上报告系统级比较。对于消融研究，我们考虑四种典型的目标检测框架：Cascade Mask R-CNN、ATSS、RepPoints v2 和 Sparse RCNN in mmdetection。对于这四种框架，我们使用相同的设置：多尺度训练（将输入大小调整为短边在 480 到 800 之间，长边最多为 1333），AdamW 优化器（初始学习率 0.0001，权重衰减 0.05，批处理大小 16），和 3x 调度（36 个周期）。对于系统级比较，我们采用了一个改进的 HTC（表示为 HTC++），带有 instaboost、更强的多尺度训练、6x 调度（72 个周期）、soft-NMS 和 ImageNet-22K 预训练模型作为初始化。我们比较我们的 Swin Transformer 和标准的 ConvNets，即 ResNe(X)t，以及之前的 Transformer 网络，即 DeiT。比较是通过只改变骨干网络而其他设置不变来进行的。请注意，虽然 Swin Transformer 和 ResNe(X)t 由于其分层的特征图可以直接应用于所有上述框架，但 DeiT 只产生单一分辨率的特征图，不能直接应用。为了公平比较，我们遵循 的做法，使用反卷积层为 DeiT 构建分层特征图。</p>
            <p><strong>与 ResNe(X)t 的比较</strong> 表 2(a) 列出了 Swin-T 和 ResNet-50 在四种目标检测框架上的结果。我们的 Swin-T 架构比 ResNet-50 带来了持续的 +3.4~4.2 box AP 增益，模型大小、FLOPs 和延迟略大。表 2(b) 使用 Cascade Mask R-CNN 在不同模型容量下比较 Swin Transformer 和 ResNe(X)t。Swin Transformer 取得了 51.9 box AP 和 45.0 mask AP 的高检测准确率，这比具有相似模型大小、FLOPs 和延迟的 ResNeXt101-64x4d 高出 +3.6 box AP 和 +3.3 mask AP。在一个使用改进 HTC 框架的更高基线上，52.3 box AP 和 46.0 mask AP，Swin Transformer 的增益也很高，分别为 +4.1 box AP 和 +3.1 mask AP（见表 2(c)）。关于推理速度，虽然 ResNe(X)t 是由高度优化的 Cudnn 函数构建的，但我们的架构是用并非都经过良好优化的内置 PyTorch 函数实现的。彻底的核优化超出了本文的范围。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/tab2.png" alt="Language-Table Performance">
                    <figcaption>图2：COCO目标检测与实例分割结果。†表示使用了额外的解卷积层以生成分层特征图。*表示多尺度测试。</figcaption>
                </figure>
            </div>
            <p><strong>与 DeiT 的比较</strong> 使用 Cascade Mask R-CNN 框架的 DeiT-S 的性能如表 2(b) 所示。Swin-T 的结果比 DeiT-S 高出 +2.5 box AP 和 +2.3 mask AP，模型大小相似（86M vs. 80M），推理速度显著更高（15.3 FPS vs. 10.4 FPS）。DeiT 较低的推理速度主要是由于其对输入图像大小的二次方复杂度。</p>
            <p><strong>与之前最先进技术的比较</strong> 表 2(c) 比较了我们的最佳结果与之前的最先进模型。我们最好的模型在 COCO test-dev 上达到了 58.7 box AP 和 51.1 mask AP，超过了之前的最佳结果，分别高出 +2.7 box AP（Copy-paste 无外部数据）和 +2.6 mask AP (DetectoRS)。</p>

            <h4>4.3. Semantic Segmentation on ADE20K</h4>
            <p><strong>设置</strong> ADE20K 是一个广泛使用的语义分割数据集，涵盖了 150 种语义类别。它总共有 25K 张图像，其中 20K 用于训练，2K 用于验证，另外 3K 用于测试。我们利用 UperNet 在 mmseg 中作为我们的基础框架，因其高效。更多细节在附录中介绍。</p>
            <p><strong>结果</strong> 表 3 列出了不同方法/骨干网络对的 mIoU、模型大小 (#param)、FLOPs 和 FPS。从这些结果可以看出，Swin-S 比具有相似计算成本的 DeiT-S 高出 +5.3 mIoU (49.3 vs. 44.0)。它也比 ResNet-101 高出 +4.4 mIoU，比 ResNeSt-101 高出 +2.4 mIoU。我们经过 ImageNet-22K 预训练的 Swin-L 模型在 val 集上达到了 53.5 mIoU，比之前的最佳模型高出 +3.2 mIoU (SETR 的 50.3 mIoU，其模型尺寸更大)。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/tab3.png" alt="Language-Table Performance">
                    <figcaption>表3：ADE20K val 和 test 集上的语义分割结果。‡ 表示使用额外的反卷积层来产生分层的特征图。‡ 表示模型在 ImageNet-22K 上进行了预训练。</figcaption>
                </figure>
            </div>

            <h4>4.4. Ablation Study</h4>
            <p>在本节中，我们对提出的 Swin Transformer 的重要设计元素进行消融研究，使用 ImageNet-1K 图像分类、COCO 目标检测上的 Cascade Mask R-CNN 和 ADE20K 语义分割上的 UperNet。</p>
            <p><strong>移动窗口</strong> 移动窗口方法在三个任务上的消融研究报告在表 4 中。采用移动窗口分区的 Swin-T 在 ImageNet-1K 上比在每个阶段都建立在单一窗口分区上的对应模型高出 +1.1% 的 top-1 准确率，在 COCO 上高出 +2.8 box AP/+2.2 mask AP，在 ADE20K 上高出 +2.8 mIoU。结果表明使用移动窗口来建立前一层窗口之间的连接是有效的。移动窗口的延迟开销也很小，如表 5 所示。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/tab4.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>表4：在三个基准测试中对移动窗口方法和不同位置嵌入方法的消融研究，使用 Swin-T 架构。w/o shifting：所有自注意力模块采用常规窗口分区，不移动；abs. pos.：ViT 的绝对位置嵌入项；rel. pos.：默认设置，带有一个额外的相对位置偏置项（见公式(4)）；app.：公式(4)中的第一个缩放点积项。</figcaption>
                </figure>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/tab5.png" alt="Language-Table Performance">
                    <figcaption>表5：在V100 GPU上不同自注意力计算方法和实现的真实速度。</figcaption>
                </figure>
            </div>
            <p><strong>相对位置偏置</strong> 表 4 比较了不同位置嵌入方法的比较。具有相对位置偏置的 Swin-T 在 ImageNet-1K 上比没有位置编码和有绝对位置嵌入的分别高出 +1.2%/+0.8% 的 top-1 准确率，在 COCO 上高出 +1.3/+1.5 box AP 和 +1.1/+1.3 mask AP，在 ADE20K 上高出 +2.3/+2.9 mIoU，这表明了相对位置偏置的有效性。还要注意的是，虽然包含绝对位置嵌入可以提高图像分类准确率 (+0.4%)，但它会损害目标检测和语义分割（在 COCO 上 -0.2 box/mask AP，在 ADE20K 上 -0.6 mIoU）。虽然最近的 ViT/DeiT 模型在图像分类中放弃了平移不变性，即使它早已被证明对视觉建模至关重要，但我们发现，鼓励某些平移不变性的归纳偏置对于通用目的的视觉建模仍然是可取的，特别是对于目标检测和语义分割等密集预测任务。</p>
            <p><strong>不同的自注意力方法</strong> 表 5 比较了不同自注意力计算方法和实现的实际速度。我们的循环实现在硬件上比朴素的填充方法更高效，特别是在更深的阶段。总的来说，它在 Swin-T、Swin-S 和 Swin-B 上分别带来了 13%、18% 和 18% 的加速。基于提出的移动窗口方法构建的自注意力模块比在四个网络阶段上基于滑动窗口的朴素/核实现分别高效 40.8×/2.5×, 20.2×/2.5×, 9.3×/2.1× 和 7.6×/1.8×。总的来说，基于移动窗口构建的 Swin Transformer 架构比基于滑动窗口构建的变体在 Swin-T、Swin-S 和 Swin-B 上分别快 4.1/1.5, 4.0/1.5, 3.6/1.5 倍。表 6 比较了它们在三个任务上的准确率，显示它们在视觉建模中具有相似的准确率。与 Performer[15]（最快的 Transformer 架构之一，见）相比，提出的基于移动窗口的自注意力计算和整体 Swin Transformer 架构稍快（见表 5），同时使用 Swin-T 在 ImageNet-1K 上比 Performer 取得了 +2.3% 的 top-1 准确率（见表 6）。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/tab6.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>表6：在三个基准测试中使用不同自注意力计算方法的 Swin Transformer 的准确率。</figcaption>
                </figure>
            </div>

            <div class="appendix-box">
                <h4>附录A1. Detailed Architectures</h4>
                <p>详细的架构规格如表 7 所示，其中假设所有架构的输入图像尺寸均为 224x224。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./SwinTransformer_figs/tab7.png" alt="Language-Table Performance">
                        <figcaption>表7：详细架构规格。</figcaption>
                    </figure>
                </div>
                <ul>
                    <li>"Concat n x n" 指的是将一个 patch（图像块）中 n x n 个相邻特征进行拼接。该操作会导致特征图以 n 的倍率进行下采样。</li>
                    <li>"96-d" 表示一个输出维度为 96 的线性层。</li>
                    <li>"win. sz. 7 x 7" 表示一个窗口大小为 7x7 的多头自注意力模块。</li>
                </ul>

                <h4>附录A2. Detailed Experimental Settings</h4>
                <h4>附录A2.1. Image classification on ImageNet-1K</h4>
                <p>图像分类任务是通过在最后一个阶段的输出特征图上应用一个全局平均池化层，然后接一个线性分类器来完成的。我们发现，这种策略与 ViT 和 DeiT 中使用的额外 class token（类别令牌）一样准确。在评估时，我们报告的是使用单个裁切（single crop）的 Top-1 准确率。</p>
                <p><strong>常规 ImageNet-1K 训练</strong> 训练设置主要遵循。对于所有模型变体，我们采用 224² 的默认输入图像分辨率。对于其他分辨率，如 384²，我们微调在 224² 分辨率上训练好的模型，而不是从头开始训练，以减少 GPU 消耗。</p>
                <p>当使用 224² 输入从头训练时，我们采用 AdamW 优化器训练 300 个周期，使用带有 20 个周期线性预热（warm-up）的余弦衰减学习率调度器。批次大小为 1024，初始学习率为 0.001，权重衰减为 0.05，并使用最大范数为 1 的梯度裁剪。我们采纳了 中的大部分增强和正则化策略，包括 RandAugment、Mixup、Cutmix、随机擦除（random erasing） 和随机深度（stochastic depth），但没有使用重复增强（repeated augmentation） 和指数移动平均（Exponential Moving Average, EMA），因为它们并未提升性能。请注意，这与 相反，在 中，重复增强对于稳定 ViT 的训练至关重要。对于更大的模型，我们采用了递增的随机深度增强，即 Swin-T、Swin-S 和 Swin-B 的随机深度率分别为 0.2、0.3 和 0.5。</p>
                <p><strong>使用更大分辨率输入进行微调</strong> 我们使用 AdamW 优化器进行 30 个周期的微调，恒定学习率为 10⁻⁵，权重衰减为 10⁻⁸，并使用与第一阶段相同的数据增强和正则化方法，除了将随机深度率设置为 0.1。</p>
                <p><strong>ImageNet-22K 预训练</strong> 我们也在更大的 ImageNet-22K 数据集上进行预训练，该数据集包含 1420 万张图像和 22K 个类别。训练分两个阶段完成。第一阶段使用 224² 输入，我们采用 AdamW 优化器训练 90 个周期，使用带有 5 个周期线性预热的线性衰减学习率调度器。批次大小为 4096，初始学习率为 0.001，权重衰减为 0.01。在 ImageNet-1K 微调的第二阶段，我们使用 224²/384² 输入训练模型 30 个周期，批次大小为 1024，恒定学习率为 10⁻⁵，权重衰减为 10⁻⁸。</p>
                
                <h4>附录A2.2. Object detection on COCO</h4>
                <p>在消融研究中，我们考虑了 mmdetection 中的四种典型目标检测框架：Cascade Mask R-CNN、ATSS、RepPoints v2 和 Sparse RCNN。对于这四种框架，我们采用相同的设置：多尺度训练（调整输入尺寸，使短边在 480 到 800 之间，长边最多为 1333）、AdamW 优化器（初始学习率 0.0001，权重衰减 0.05，批次大小 16），以及 3x 调度（36 个周期，学习率在第 27 和 33 个周期时衰减 10 倍）。</p>
                <p>对于系统级比较，我们采用了一个改进的 HTC（表示为 HTC++），并结合了 instaboost、更强的多尺度训练（调整输入尺寸，使短边在 400 到 1400 之间，长边最多为 1600）、6x 调度（72 个周期，学习率在第 63 和 69 个周期时衰减 0.1 倍）、soft-NMS，以及在最后一个阶段的输出处附加一个额外的全局自注意力层，并使用 ImageNet-22K 预训练的模型作为初始化。</p>
            
                <h4>附录A2.3. Semantic segmentation on ADE20K</h4>
                <p>ADE20K 是一个广泛使用的语义分割数据集，涵盖了 150 个语义类别。它总共有 25K 张图像，其中 20K 用于训练，2K 用于验证，另外 3K 用于测试。我们利用 mmsegmentation 中的 UperNet 作为我们的基础框架，因其高效率。</p>
                <p>在训练中，我们使用 AdamW 优化器，初始学习率为 6 x 10⁻⁵，权重衰减为 0.01，一个使用线性学习率衰减的调度器，以及 1500 次迭代的线性预热。模型在 8 个 GPU 上进行训练，每个 GPU 2 张图像，共进行 160K 次迭代。对于数据增强，我们采用 mmsegmentation 中的默认设置，包括随机水平翻转、在 [0.5, 2.0] 比例范围内的随机缩放，以及随机光度失真。所有 Swin Transformer 模型都应用了比例为 0.2 的随机深度。Swin-T 和 Swin-S 在标准设置下使用 512x512 的输入进行训练。带有 ‡ 标记的 Swin-B 和 Swin-L 表示这些模型在 ImageNet-22K 上进行了预训练，并使用 640x640 的输入进行训练。</p>
                <p>在推理时，采用多尺度测试，使用的分辨率是训练分辨率的 [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] 倍。在报告测试分数时，遵循通用实践，将训练图像和验证图像都用于训练。</p>

                <h4>附录A3. More Experiments</h4>
                <h4>附录A3.1. Image classification with different input size</h4>
                <p>表 8 列出了 Swin Transformer 在不同输入图像尺寸（从 224² 到 384²）下的性能。总的来说，更大的输入分辨率会带来更好的 Top-1 准确率，但推理速度会变慢。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./SwinTransformer_figs/tab8.png" alt="Language-Table Performance" class="img-large">
                        <figcaption>表8：该表展示了 Swin-T, Swin-S, Swin-B 在 224², 256², 320², 384² 四种分辨率下的 Top-1 准确率和吞吐量（images/s）。</figcaption>
                    </figure>
                </div>

                <h4>附录A3.2. Different Optimizers for ResNe(X)t on COCO</h4>
                <p>表 9 比较了 ResNe(X)t 主干网络在 COCO 目标检测任务上使用 AdamW 和 SGD 优化器的性能。比较中使用了 Cascade Mask R-CNN 框架。虽然 SGD 是 Cascade Mask R-CNN 框架的默认优化器，但我们通常观察到，用 AdamW 优化器替换 SGD 可以提高准确率，特别是对于较小的主干网络。因此，在与我们提出的 Swin Transformer 架构进行比较时，我们为 ResNe(X)t 主干网络使用 AdamW。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./SwinTransformer_figs/tab9.png" alt="Language-Table Performance" class="img-large">
                        <figcaption>表9：在 COCO 目标检测上，使用 Cascade Mask R-CNN 框架对 ResNe(X)t 主干网络进行 SGD 和 AdamW 优化器的比较。</figcaption>
                    </figure>
                </div>

                <h4>附录A3.3. Swin MLP-Mixer</h4>
                <p>我们将所提出的分层设计和移动窗口方法应用于 MLP-Mixer 架构，我们称之为 Swin-Mixer。表 10 展示了 Swin-Mixer 与原始 MLP-Mixer 架构 和一个后续工作 ResMLP 的性能比较。Swin-Mixer 的性能显著优于 MLP-Mixer（81.3% vs. 76.4%），同时计算预算略小（10.4G vs. 12.7G）。与 ResMLP 相比，它也具有更好的速度-准确率权衡。这些结果表明，我们提出的分层设计和移动窗口方法是具有普适性的。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./SwinTransformer_figs/tab10.png" alt="Language-Table Performance" class="img-large">
                        <figcaption>表10：该表对比了 MLP-Mixer-B/16, ResMLP-S24, ResMLP-B24 和多个 Swin-Mixer 变体的性能，包括参数量、FLOPs、吞吐量和 Top-1 准确率。</figcaption>
                    </figure>
                </div>
            </div>

            <h3>5. Conclusion</h3>
            <p>本文提出了一种名为 Swin Transformer 的新型视觉 Transformer，它产生分层的特征表示，并且对输入图像大小具有线性的计算复杂度。Swin Transformer 在 COCO 目标检测和 ADE20K 语义分割上取得了最先进的性能，显著超过了之前最好的方法。我们希望 Swin Transformer 在各种视觉问题上的强大性能将鼓励对视觉和语言信号进行统一建模。作为 Swin Transformer 的一个关键元素，基于移动窗口的自注意力被证明在视觉问题上是有效和高效的，我们期待着研究它在自然语言处理中的应用。</p>
        </section>

        <!-- ====================================================================== -->
        <!-- PART 3: 论文重点代码/实现解析 (Code & Implementation Analysis)         -->
        <!-- ====================================================================== -->
        <!-- 这是我为你解析的部分。未来你可以将自己的代码分析放在这里。 -->
        <section id="code-analysis">
            <h2>论文重点实现解析</h2>
            <div class="code-analysis">
                <h3>MLP</h3>
                <pre><code class="python">
class Mlp(nn.Module):
    def __init__(
        self, 
        in_features, 
        hidden_features=None, 
        out_features=None, 
        act_layer=nn.GELU, 
        drop=0.
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
                </code></pre>

                <h3>Window Partition</h3>
                <pre><code class="python">
def window_partition(x, window_size):
    """
    功能：将 (B, H, W, C) 的特征图分割成多个窗口。
    Args:
        x: 输入特征图，形状为 (B, H, W, C) -> (批次大小, 高, 宽, 通道数)
        window_size (int): 窗口的大小（假设为正方形窗口）

    Returns:
        windows: 分割后的窗口，形状为 (num_windows*B, window_size, window_size, C)
    """
    # 1. 获取输入张量的维度
    B, H, W, C = x.shape
    
    # 2. 关键步骤：重塑张量以分离出窗口
    # 原始的 H 维度被看作 (H // window_size) 个块，每个块大小为 window_size
    # 原始的 W 维度也被看作 (W // window_size) 个块，每个块大小为 window_size
    # 结果形状变为 (B, H/ws, ws, W/ws, ws, C)
    # ws 是 window_size 的缩写
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    
    # 3. 交换维度，将所有窗口聚合在一起
    # 目标是将所有属于同一个窗口的像素聚合。
    # 原始索引: (0, 1, 2, 3, 4, 5) -> B, H/ws, ws, W/ws, ws, C
    # 交换后索引: (0, 1, 3, 2, 4, 5) -> B, H/ws, W/ws, ws, ws, C
    # 这样，维度 1 和 3 (H/ws, W/ws) 代表了窗口的网格索引，
    # 维度 2 和 4 (ws, ws) 代表了窗口内部的像素坐标。
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    
    # 4. 最后一次重塑，得到最终输出
    # 将批次大小(B)和窗口数量(H/ws * W/ws)合并到第一个维度。
    # 这样做的目的是为了让后续的 WindowAttention 模块可以一次性并行处理所有的窗口，
    # 就好像它们是一个巨大的批次一样，极大地提高了计算效率。
    # -1 会被自动计算为 B * (H/ws) * (W/ws)
    windows = windows.view(-1, window_size, window_size, C)
    
    return windows
                </code></pre>

                <h3>Window Reverse</h3>
                <pre><code class="python">
def window_reverse(windows, window_size, H, W):
    """
    功能：将分割后的窗口复原成原始的 (B, H, W, C) 特征图。
    Args:
        windows: 经过注意力计算后的窗口, 形状为 (num_windows*B, window_size, window_size, C)
        window_size (int): 窗口大小
        H (int): 原始图像的高度
        W (int): 原始图像的宽度

    Returns:
        x: 复原后的特征图, 形状为 (B, H, W, C)
    """
    # 1. 计算原始的批次大小 B
    # windows.shape[0] 是 num_windows * B
    # H * W / window_size / window_size 是每张图片包含的窗口数量 (num_windows_per_image)
    # 两者相除得到 B
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    
    # 2. 重塑张量，这是 partition 中第4步的逆操作
    # 将 (B * num_windows, ws, ws, C) 变回 (B, H/ws, W/ws, ws, ws, C)
    x = windows.view(
        B, H // window_size, W // window_size, window_size, window_size, -1
    )
    
    # 3. 交换维度，这是 partition 中第3步的逆操作
    # 原始索引: (0, 1, 2, 3, 4, 5) -> B, H/ws, W/ws, ws, ws, C
    # 交换后索引: (0, 1, 3, 2, 4, 5) -> B, H/ws, ws, W/ws, ws, C
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    
    # 4. 最后一次重塑，完全恢复原始形状
    # 这是 partition 中第2步的逆操作
    x = x.view(B, H, W, -1)
    
    return x
                </code></pre>

                <h3>Window Attention</h3>
                <pre><code class="python">
class WindowAttention(nn.Module):
    r""" 
    Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(
        self, 
        dim, 
        window_size, 
        num_heads, 
        qkv_bias=True, 
        qk_scale=None, 
        attn_drop=0., 
        proj_drop=0.
    ):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or (dim // num_heads) ** -0.5

        # --- 核心：相对位置偏置 (Relative Position Bias) ---
        # 1. 定义一个可学习的参数表，用于存储相对位置偏置
        # 对于一个大小为 (Wh, Ww) 的窗口，
        # 高度上的相对位置范围是 [-(Wh-1), Wh-1]，共 2*Wh-1 个值
        # 宽度上的相对位置范围是 [-(Ww-1), Ww-1]，共 2*Ww-1 个值
        # 所以总共有 (2*Wh-1)*(2*Ww-1) 种相对位置组合
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)
        )

        # 2. 计算窗口内每个 token 的成对相对位置索引
        # 这部分代码只在初始化时运行一次，生成一个固定的索引，后续直接使用。
        coords_h = torch.arange(window_size[0])
        coords_w = torch.arange(window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 形状: (2, Wh, Ww)
        coords_flatten = torch.flatten(coords, 1)  # 形状: (2, Wh*Ww)
        
        # 计算两两之间的坐标差，得到相对坐标
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 形状: (2, Wh*Ww, Wh*Ww)
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # 形状: (Wh*Ww, Wh*Ww, 2)
        
        # 将相对坐标转换为从0开始的索引
        relative_coords[:, :, 0] += window_size[0] - 1  # 高度偏移
        relative_coords[:, :, 1] += window_size[1] - 1  # 宽度偏移
        
        # 将二维索引 (dx, dy) 转换为一维索引，方便查表
        relative_coords[:, :, 0] *= 2 * window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # 形状: (Wh*Ww, Wh*Ww)
        
        # 将这个索引注册为 buffer，它会随模型移动（如.to(device)），但不是可训练参数
        self.register_buffer("relative_position_index", relative_position_index)

        # 定义 QKV 线性层和输出投影层
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x, mask=None):
        """
        Args:
            x: 输入特征，形状为 (num_windows*B, N, C)，其中 N = Wh*Ww
            mask: 可选的掩码，用于 shifted window (SW-MSA)，
                  形状为 (num_windows, Wh*Ww, Wh*Ww)
        """
        B_, N, C = x.shape
        
        # 1. 计算 Q, K, V
        # self.qkv(x) -> (B_, N, 3*C)
        # reshape -> (B_, N, 3, num_heads, C_per_head)
        # permute -> (3, B_, num_heads, N, C_per_head)
        qkv = self.qkv(x).reshape(
            B_, N, 3, self.num_heads, C // self.num_heads
        ).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # 2. 计算注意力分数 (Attention Score)
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1)) # attn 形状: (B_, num_heads, N, N)

        # 3. 添加相对位置偏置
        # 使用预先计算好的 relative_position_index 从偏置表 lookup
        relative_position_bias = 
            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(N, N, -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        # 将偏置加到注意力分数上，PyTorch 的广播机制会自动处理 B_ 维度
        attn = attn + relative_position_bias.unsqueeze(0)

        # 4. 应用掩码 (如果提供了)
        # 这在 SW-MSA (移动窗口自注意力) 中至关重要，用于阻止不应相互关注的区域进行信息交换
        if mask is not None:
            nW = mask.shape[0] # nW = num_windows
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        # 5. 计算加权后的 V
        # (attn @ v) -> (B_, num_heads, N, C_per_head)
        # transpose -> (B_, N, num_heads, C_per_head)
        # reshape -> (B_, N, C)
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        
        # 6. 输出投影
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops
                </code></pre>

                <h3>Embeddings and Softmax</h3>
                <pre><code class="python">
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model # 512

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
                </code></pre>
            <div class="appendix-box">
                <h4>嵌入机制的举例说明</h4>
                <pre><code class="python">
# 分词
word = 'Hi, 你好~'
tokens = tokenizer.tokenize(word)
print(f'{word} 分词: {tokens}')
# 输出：Hi, 你好~ 分词: ['Hi', ',', 'Ġ', 'ä½łå¥½', '~']

token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(f'{word} Token IDs: {token_ids}')
# 输出：Hi, 你好~ Token IDs: [13048, 11, 220, 108386, 93]

# 获取该单词的嵌入向量
word_embedding = embeddings.weight[token_ids]
print(f'{word} 的嵌入形状：{word_embedding.shape}')
# 输出：Hi, 你好~ 的嵌入形状：torch.Size([5, 1536])

print(f'{word} 的嵌入内容: {word_embedding}')
# 输出：
# Hi, 你好~ 的嵌入内容: tensor([[ 0.0261,  0.0048, -0.0043,  ...,  0.0193, -0.0493, -0.0020],
#         [-0.0303, -0.0159, -0.0107,  ..., -0.0198, -0.0020, -0.0129],
#         [-0.0236, -0.0254,  0.0325,  ..., -0.0317, -0.0082,  0.0137],
#         [ 0.0270,  0.0042,  0.0014,  ...,  0.0425, -0.0195,  0.0011],
#         [-0.0205, -0.0408, -0.0013,  ...,  0.0272, -0.0060,  0.0032]],
#        dtype=torch.bfloat16, grad_fn=<\IndexBackward0>)
                </code></pre>
                </div>
                <h3>SwinTransformer Block</h3>
                <pre><code class="python">
class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., 
                 drop_path=0.,act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 fused_window_process=False):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)
        self.fused_window_process = fused_window_process

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            if not self.fused_window_process:
                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
                # partition windows
                x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
            else:
                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)
        else:
            shifted_x = x
            # partition windows
            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C

        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)

        # reverse cyclic shift
        if self.shift_size > 0:
            if not self.fused_window_process:
                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
            else:
                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)
        else:
            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
            x = shifted_x
        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(x)

        # FFN
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops                

            </div>
        </section>

    </div>

</body>
</html>