<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>论文笔记 | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&family=Roboto+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/python.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
        :root {
            --primary-color: #005f73;
            --secondary-color: #0a9396;
            --background-color: #f8f9fa;
            --text-color: #212529;
            --heading-font: 'Noto Sans SC', sans-serif;
            --body-font: 'Noto Sans SC', sans-serif;
            --code-font: 'Roboto Mono', monospace;
            --border-color: #dee2e6;
            --card-bg: #ffffff;
            --shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        }

        body {
            font-family: var(--body-font);
            line-height: 1.8;
            color: var(--text-color);
            background-color: var(--background-color);
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 850px;
            margin: 0 auto;
            background-color: var(--card-bg);
            padding: 30px 30px;
            border-radius: 12px;
            box-shadow: var(--shadow);
        }

        .paper-title {
            font-family: var(--heading-font);
            font-weight: 700;
            font-size: 2.2em;
            color: var(--primary-color);
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 5px;
        }

        .paper-subtitle {
            font-family: var(--heading-font);
            font-size: 1.2em;
            color: #6c757d;
            margin-top: 0;
            margin-bottom: 40px;
        }

        h2 {
            font-family: var(--heading-font);
            font-size: 1.8em;
            color: var(--primary-color);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 8px;
            margin-top: 50px;
        }

        h3 {
            font-family: var(--heading-font);
            font-size: 1.4em;
            color: var(--secondary-color);
            margin-top: 30px;
        }

        h4 {
            font-family: var(--heading-font);
            font-size: 1em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        h5 {
            font-family: var(--heading-font);
            font-size: 1.0em;
            color: var(--secondary-color);
            margin-top: 20px;
        }

        p, li {
            font-size: 1em;
            text-align: justify;
        }
        
        strong {
            color: var(--primary-color);
        }

        /* 重点总结区域样式 */
        .summary-box {
            background-color: #eef7f8;
            border-left: 5px solid var(--secondary-color);
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .summary-box h3 {
            margin-top: 0;
            color: var(--primary-color);
        }

        .summary-box ul {
            padding-left: 20px;
        }

        .summary-box li {
            margin-bottom: 10px;
        }

        /* 代码/实现解析区域样式 */
        .code-analysis pre {
            background-color: #282c34;
            color: #abb2bf;
            font-family: var(--code-font);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.5;
            font-size: 0.9em;
        }
        
        .code-analysis code .token{
            color: #61afef; /* 蓝色，用于变量/标记 */
        }
        .code-analysis code .comment{
            color: #5c6370; /* 灰色，用于注释 */
        }
        .code-analysis code .value{
            color: #98c379; /* 绿色，用于值 */
        }

        /* 图像和表格说明文字样式 */
        .figure-caption, .table-caption {
            background-color: #f1f3f5;
            border: 1px solid var(--border-color);
            padding: 15px;
            margin: 20px auto;
            border-radius: 8px;
            font-style: italic;
            color: #495057;
            text-align: center;
        }
        .figure-caption img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        .img-small {
            width: 30%; /* 图片宽度为容器的 30% */
            max-width: 100%; /* 安全措施，确保不会超出 */
        }

        .img-medium {
            width: 50%; /* 图片宽度为容器的 50% */
            max-width: 100%;
        }

        .img-large {
            width: 75%; /* 图片宽度为容器的 75% */
            max-width: 100%;
        }

        blockquote {
            border-left: 4px solid var(--border-color);
            padding-left: 20px;
            color: #6c757d;
            margin-left: 0;
        }

        .appendix-box {
            background-color: #f1f3f5; /* 浅灰色背景 */
            border-left: 5px solid #adb5bd; /* 中性灰色边框 */
            padding: 20px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .appendix-box h3 {
            margin-top: 0;
            color: #495057; /* 深灰色标题，以在灰色背景上保持清晰 */
        }

        .data-table {
            width: 100%;
            border-collapse: collapse; /* 合并边框 */
            margin: 30px 0;
            font-size: 0.95em;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden; /* 确保圆角生效 */
        }

        .data-table caption {
            caption-side: bottom; /* 将标题放在表格下方，与图片说明保持一致 */
            margin-top: 10px;    /* 与表格的间距 */
            padding: 5px;
            font-size: 0.9em;
            font-style: italic;
            color: #6c757d;      /* 使用柔和的灰色 */
            text-align: center;
        }

        .data-table thead tr {
            background-color: var(--primary-color);
            color: #ffffff;
            text-align: left;
            font-weight: bold;
        }

        .data-table th, .data-table td {
            padding: 12px 15px;
            border-bottom: 1px solid var(--border-color);
        }

        .data-table tbody tr {
            background-color: #ffffff;
        }

        /* 斑马条纹，增加可读性 */
        .data-table tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }

        .data-table tbody tr:last-of-type {
            border-bottom: 2px solid var(--primary-color);
        }

        /* 任务组列加粗，突出显示 */
        .data-table td:first-child {
            font-weight: bold;
            color: var(--primary-color);
        }

        pre code.hljs {
            font-family: Consolas; /* 设置字体 */
            font-size: 16px;                        /* 设置字号 */
            background-color: #000000;              /* 可选：背景色 */
            padding: 1em;                           /* 可选：内边距 */
            display: block;                         /* 确保是块级元素 */
            white-space: pre-wrap;                  /* 保留空格和换行 */
            word-wrap: break-word;                  /* 长单词自动换行 */
        }
    </style>
</head>
<body>

    <div class="container">
        
        <h1 class="paper-title">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h1>
        <p class="paper-subtitle"></p>

        <!-- ====================================================================== -->
        <!-- PART 2: 论文重点总结 (Key Takeaways)                                   -->
        <!-- ====================================================================== -->
        <!-- 这是我为你总结的部分。未来你可以将自己的总结放在这里。 -->
        <section id="summary">
            <h2>论文重点总结</h2>
            <div class="summary-box">
                <h3>核心思想与贡献</h3>
                <p>Swin Transformer 是一篇具有里程碑意义的论文，它成功地将 Transformer 架构从自然语言处理（NLP）领域推广为计算机视觉（CV）领域的通用骨干网络，在各大主流视觉任务上均取得了顶尖的性能。</p>
                <p>论文旨在解决<strong>标准 Vision Transformer (ViT) 应用于计算机视觉时的两大挑战：</strong></p>
                <ul>
                    <li><strong>多尺度问题：</strong>视觉任务中的物体尺寸变化非常大，而 ViT 使用固定大小的图像块（Patch），难以适应这种变化。</li>
                    <li><strong>计算复杂度问题：</strong>ViT 的自注意力计算复杂度与图像尺寸的平方成正比，对于高分辨率图像（如语义分割、目标检测所需）来说，计算量过大。</li>
                </ul>
                <p><strong>Transformer 的两大核心创新：</strong> </p>
                <ul>
                    <li><strong>分层架构 (Hierarchical Architecture)：</strong>类似于卷积神经网络（CNN）的工作方式，Swin Transformer 通过一种名为 Patch Merging 的模块，在网络加深的过程中逐步合并相邻的图像块，从而减小特征图的分辨率、增加感受野。这种设计使其能够产生不同尺度的特征图，可以方便地与现有的下游任务框架（如用于目标检测的 FPN、用于分割的 U-Net）结合，成为一个通用的视觉骨干网络。</li>
                    <li><strong>移动窗口自注意力 (Shifted Window based Self-Attention, W-MSA/SW-MSA)：</strong>
                        <ul>
                            <li><strong>常规窗口自注意力 (W-MSA)：</strong>为了降低计算量，模型不再计算全局的自注意力，而是将图像划分为多个不重叠的局部窗口（Window），只在每个窗口内部计算自注意力。由于窗口大小固定，计算复杂度从图像尺寸的二次方下降为线性关系。</li>
                            <li><strong>移动窗口自注意力 (SW-MSA)：</strong>仅使用 W-MSA 会导致窗口之间缺乏信息交流，限制了模型的表征能力。为此，Swin Transformer 采用了一种巧妙的 “移动窗口” 机制。在连续的两个 Transformer 模块中，第二个模块的窗口划分会相对于前一个模块进行位移（通常是半个窗口大小）。这种设计使得上一层中原本不相邻的图像块，在新的窗口中得以互动，从而实现了跨窗口的信息交流，大大增强了模型的性能。</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </section>

        

        <!-- ====================================================================== -->
        <!-- PART 1: 论文全文翻译 (Full Translation)                                -->
        <!-- ====================================================================== -->
        <!-- 这是你提供的笔记内容，我已经帮你格式化好了。 -->
        <section id="translation">
            <h2>论文全文翻译</h2>

            <h3>Abstract</h3>
            <p>本文提出了一种名为 Swin Transformer 的新型视觉 Transformer，它能够作为一个通用的计算机视觉骨干网络。将 Transformer 从语言领域适配到视觉领域存在诸多挑战，这些挑战源于两个领域之间的差异，例如视觉实体的尺度变化范围大，以及与文本中的单词相比，图像中的像素分辨率要高得多。为了解决这些差异，我们提出了一种分层式 Transformer，其表示是通过“移动窗口”（Shifted Windows）计算的。移动窗口方案通过将自注意力计算限制在不重叠的局部窗口内来提高效率，同时允许跨窗口连接。这种分层架构具有在不同尺度上建模的灵活性，并且其计算复杂度与图像大小呈线性关系。这些特性使得 Swin Transformer 能够兼容广泛的视觉任务，包括图像分类（在 ImageNet-1K 上达到 87.3% 的 Top-1 准确率）和密集预测任务，如目标检测（在 COCO test-dev 上取得 58.7 的 box AP 和 51.1 的 mask AP）和语义分割（在 ADE20K val 上取得 53.5 的 mIoU）。其性能在 COCO 上以 +2.7 box AP 和 +2.6 mask AP 的巨大优势，以及在 ADE20K 上以 +3.2 mIoU 的优势，超越了之前的最先进水平，展示了基于 Transformer 的模型作为视觉骨干网络的潜力。这种分层设计和移动窗口方法也被证明对全 MLP 架构是有益的。代码和模型已在 https://github.com/microsoft/Swin-Transformer 公开。</p>
            
            <h3>1. Introduction</h3>
            <p>长期以来，计算机视觉中的建模一直由卷积神经网络（CNNs）主导。从 AlexNet 及其在 ImageNet 图像分类挑战赛上的革命性表现开始，CNN 架构通过更大的规模、更广泛的连接以及更复杂的卷积形式而变得越来越强大。随着 CNN 作为各种视觉任务的骨干网络，这些架构上的进步带来了性能的提升，从而广泛地推动了整个领域的发展。</p>
            <p>另一方面，自然语言处理（NLP）中网络架构的演变走了一条不同的道路，如今流行的架构是 Transformer。Transformer 为序列建模和转导任务而设计，其显著特点是使用注意力机制来建模数据中的长程依赖关系。它在语言领域的巨大成功促使研究人员探索其在计算机视觉领域的应用，最近在某些特定任务，特别是图像分类和联合视觉-语言建模上，已经显示出有希望的结果。</p>
            <p>在本文中，我们寻求扩展 Transformer 的适用性，使其能够作为一个通用的骨干网络，就像它在 NLP 中和 CNN 在视觉中所扮演的角色一样。我们观察到，将其在语言领域的高性能迁移到视觉领域所面临的重大挑战，可以由两种模态之间的差异来解释。其中一个差异涉及尺度。与作为语言 Transformer 处理基本元素的单词标记不同，视觉元素的尺度可以有很大的变化，这个问题在目标检测等任务中受到了关注。在现有的基于 Transformer 的模型中，所有标记都是固定尺度的，这一特性不适合这些视觉应用。另一个差异是图像中的像素分辨率远高于文本段落中的单词数量。存在许多视觉任务，如语义分割，需要在像素级别进行密集预测，这对于在高分辨率图像上运行的 Transformer 来说是难以处理的，因为其自注意力的计算复杂度与图像大小成二次方关系。</p>
            <p>为了克服这些问题，我们提出了一种名为 Swin Transformer 的通用 Transformer 骨干网络，它构建了分层的特征图，并且对图像大小具有线性的计算复杂度。如图 1(a) 所示，Swin Transformer 通过从小型图像块（灰色轮廓）开始，并在更深的 Transformer 层中逐渐合并相邻的图像块，来构建一个分层的表示。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./SwinTransformer_figs/fig1.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图1: 
                        <ul>
                            <li>(a) 我们提出的 Swin Transformer 通过在更深层中合并图像块（灰色部分显示）来构建分层的特征图，并且由于自注意力计算仅在每个局部窗口内（红色部分显示），其计算复杂度与输入图像大小呈线性关系。因此，它可以作为图像分类和密集识别任务的通用骨干网络。</li>
                            <li>(b) 相比之下，以前的视觉 Transformer 产生单一低分辨率的特征图，并且由于全局计算自注意力，其计算复杂度与输入图像大小呈二次方关系。</li>
                        </ul>
                    </figcaption>
                </figure>
            </div>
            <p>有了这些分层的特征图，Swin Transformer 模型可以方便地利用先进的技术进行密集预测，例如特征金字塔网络（FPN） 或 U-Net。线性的计算复杂度是通过在划分图像的不重叠窗口内局部计算自注意力来实现的（红色轮廓）。每个窗口中的图像块数量是固定的，因此复杂度与图像大小呈线性关系。这些优点使得 Swin Transformer 适合作为各种视觉任务的通用骨干网络，而之前的基于 Transformer 的架构产生的是单一分辨率的特征图，并且具有二次方的复杂度。</p>
            <p>Swin Transformer 的一个关键设计元素是其在连续自注意力层之间窗口分区的移动，如图 2 所示。移动的窗口连接了前一层的窗口，提供了它们之间的连接，从而显著增强了建模能力（见表 4）。这个策略在实际延迟方面也是高效的：一个窗口内的所有查询块共享相同的键集¹，这有助于硬件中的内存访问。相比之下，早期的基于滑动窗口的自注意力方法 由于不同查询像素有不同的键集，在通用硬件上延迟较低²。我们的实验表明，我们提出的移动窗口方法比滑动窗口方法的延迟低得多，但在建模能力上是相似的（见表 5 和 6）。移动窗口方法也被证明对全 MLP 架构 是有益的。</p>
            <div class="appendix-box">
                <p>¹ 查询和键是自注意力层中的投影向量。</p>
                <p>² 虽然有高效的方法在通用硬件上实现基于滑动窗口的卷积层，这得益于其共享的核权重，但对于基于滑动窗口的自注意力层来说，在实践中实现高效的内存访问是困难的。</p>
            </div>
            <p>微调代码和预训练模型可在 https://github.com/google-research/vision_transformer 获取</p>

            <h3>2. Related Work</h3>
            <p>Transformer由Vaswani等人（2017）为机器翻译提出，并自此成为许多NLP任务中的最先进方法。基于Transformer的大型模型通常在大型语料库上进行预训练，然后针对手头的任务进行微调：BERT（Devlin et al., 2019）使用去噪自监督预训练任务，而GPT系列工作则使用语言建模作为其预训练任务（Radford et al., 2018; 2019; Brown et al., 2020）。</p>
            <p><strong>将自注意力机制直接应用于图像，需要每个像素都关注到所有其他像素。由于其成本与像素数量成二次方关系，这种方法无法扩展到实际的输入尺寸。</strong>因此，为了在图像处理中应用Transformer，人们尝试了几种近似方法。Parmar等人（2018）仅在每个查询像素的局部邻域内而不是全局应用自注意力。这种局部多头点积自注意力块可以完全替代卷积（Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020）。在另一条研究路线中，稀疏Transformer（Child et al., 2019）采用了可扩展的全局自注意力近似方法，使其能应用于图像。另一种扩展注意力的方法是将其应用于不同大小的块中（Weissenborn et al., 2019），在极端情况下仅沿单个轴应用（Ho et al., 2019; Wang et al., 2020a）。许多这些专门的注意力架构在计算机视觉任务上展示了有希望的结果，但需要复杂的工程才能在硬件加速器上高效实现。</p>
            <p>与我们最相关的是Cordonnier等人（2020）的模型，该模型从输入图像中提取2×2大小的块，并在其上应用完整的自注意力。这个模型与ViT非常相似，但我们的工作更进一步，证明了大规模预训练使得标准Transformer能够与最先进的CNN相媲美（甚至更好）。此外，Cordonnier等人（2020）使用的是2×2像素的小块尺寸，这使得该模型仅适用于低分辨率图像，而我们则处理中等分辨率的图像。</p>
            <p>将卷积神经网络（CNN）与各种形式的自注意力相结合也引起了广泛关注，例如，通过增强图像分类的特征图（Bello et al., 2019），或通过自注意力进一步处理CNN的输出，用于目标检测（Hu et al., 2018; Carion et al., 2020）、视频处理（Wang et al., 2018; Sun et al., 2019）、图像分类（Wu et al., 2020）、无监督目标发现（Locatello et al., 2020）或统一的文本-视觉任务（Chen et al., 2020c; Lu et al., 2019; Li et al., 2019）。</p>
            <p>另一个最近的相关模型是图像GPT（iGPT）（Chen et al., 2020a），它在降低图像分辨率和色彩空间后将Transformer应用于图像像素。该模型以无监督方式作为生成模型进行训练，其产生的表示可以被微调或线性探测用于分类任务，在ImageNet上达到了72%的最高准确率。</p>
            <p>我们的工作加入了越来越多探索比标准ImageNet更大规模图像识别的论文行列。使用额外的数据源可以在标准基准上取得最先进的结果（Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020）。此外，Sun等人（2017）研究了CNN性能如何随数据集大小扩展，Kolesnikov等人（2020）和Djolonga等人（2020）则对从ImageNet-21k和JFT-300M等大规模数据集进行CNN迁移学习进行了实证探索。我们也关注后两个数据集，但我们训练的是Transformer，而不是先前工作中使用的基于ResNet的模型。</p>
            
            <h3>3. Method</h3>
            <p>在模型设计中，我们尽可能地遵循原始的Transformer（Vaswani et al., 2017）。这种有意简化的一个优势是，可扩展的NLP Transformer架构及其高效实现几乎可以开箱即用。</p>
            <h4>3.1. Vision Transformer (ViT)</h4>
            <p>模型的概览如图1所示。标准Transformer接收一个1D的标记嵌入序列作为输入。为了处理2D图像，我们将图像 \(x ∈ ℝ^{H×W×C}\) 重塑为一个扁平化的2D图像块序列 \(x_p ∈ ℝ^{N×(P²·C)}\)，其中(H, W)是原始图像的分辨率，C是通道数，(P, P)是每个图像块的分辨率，N = HW/P² 是产生的图像块数量，也作为Transformer的有效输入序列长度。Transformer在其所有层中使用恒定的潜向量大小D，因此我们将图像块展平并通过一个可训练的线性投影映射到D维（公式1）。我们称这个投影的输出为块嵌入（patch embeddings）。</p>
            <p>$$z_0=[x_{class};\ x_p^1E;\ x_p^2E;\ ...;\ x_p^NE]+E_{pos} $$</p>
            <p>$$E ∈ ℝ^{D×(P²·C)},\ E_{pos}∈ ℝ^{(N+1)×D}$$</p>

            <p>与BERT的[class]标记类似，我们在嵌入的块序列前添加一个可学习的嵌入(\(z_0^0 = x_{class}\))，其在Transformer编码器输出端的状态(\(z_L^0\))作为图像表示y（公式4）。在预训练和微调期间，一个分类头都附加在\(z_L^0\)上。该分类头在预训练时由一个带单隐藏层的MLP实现，在微调时由一个单线性层实现。</p>
            <p>$$y=LN(z_L^0)$$</p>
            <p>位置嵌入被添加到块嵌入中以保留位置信息。我们使用标准的可学习的1D位置嵌入，因为我们没有观察到使用更先进的2D感知位置嵌入会带来显著的性能提升（附录D.4）。最终的嵌入向量序列作为编码器的输入。</p>
            <div class="appendix-box">
                <h4>附录D.4 Positional Embedding</h4>
                <p>我们对使用位置嵌入编码空间信息的不同方式进行了消融研究。我们尝试了以下情况：</p>
                <ul>
                    <li><strong>无位置信息：</strong>将输入视为一袋子块（bag of patches）。</li>
                    <li><strong>一维位置嵌入：</strong>将输入视为光栅顺序的块序列（本文所有其他实验的默认设置）。</li>
                    <li><strong>二维位置嵌入：</strong>将输入视为二维的块网格。在这种情况下，会学习两组嵌入，分别对应X轴和Y轴，每组大小为D/2。然后，根据输入中块的坐标，我们将X和Y嵌入拼接起来，得到最终的位置嵌入。</li>
                    <li><strong>相对位置嵌入：</strong>考虑到块之间的相对距离，而不是它们的绝对位置来编码空间信息。为此，我们使用一维相对注意力，在其中定义所有可能的块对之间的相对距离。因此，对于给定的一组对（一个用作查询，另一个用作注意机制中的键/值），我们有一个偏移量 \(p_q-p_k\)，每个偏移量都与一个嵌入相关联。然后，我们简单地运行额外的注意力，使用原始查询（查询的内容），但使用相对位置嵌入作为键。然后，我们将来自相对注意力的对数几率作为偏差项添加到主注意力（基于内容的注意力）的对数几率中，然后再应用softmax。</li>
                </ul>
                <p>除了不同的空间信息编码方式外，我们还尝试了在模型中嵌入这些信息的不同方法。对于一维和二维位置嵌入，我们尝试了三种情况：(1) 在输入后和在将输入馈送到Transformer编码器（本论文中的所有其他实验默认使用）之前直接添加位置嵌入；(2) 在每个层开始时学习并添加位置嵌入；(3) 在每个层开始时将学习到的位置嵌入添加到输入中（各层共享）。</p>
                <p>表8总结了在ViT-B/16模型上进行的消融研究的结果。正如我们所看到的，尽管没有位置编码的模型与具有位置编码的模型之间的性能存在很大差距，但不同方式对位置信息进行编码之间几乎没有差异。我们推测，由于我们的Transformer 编码器是在像素级别的输入而不是像块级别这样的输入上运行的，因此如何编码空间信息的差异不那么重要。更准确地说，在像素级别的输入中，空间维度比原始像素级别的输入小得多，例如 14x14 与 224x224，学习在这个分辨率下表示空间关系对于这些不同的位置编码策略来说同样容易。尽管如此，网络学到的具体的位置嵌入相似性模式取决于训练超参数（图10）。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/tab8.png" alt="Language-Table Performance" class="img-large">
                        <figcaption>表8：在ImageNet 5-shot线性评估中，使用ViT-B/16模型对位置嵌入进行消融研究的结果。</figcaption>
                    </figure>
                </div>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig10.png" alt="Language-Table Performance">
                        <figcaption>图10：使用不同超参数训练的模型的位置嵌入。</figcaption>
                    </figure>
                </div>
            </div>
            <p>Transformer编码器（Vaswani et al., 2017）由交替的多头自注意力（MSA，见附录A）和MLP块（公式2, 3）构成。在每个块之前应用层归一化（LayerNorm, LN），每个块之后应用残差连接（Wang et al., 2019; Baevski & Auli, 2019）。</p>
            <p>$$z_l'=MSA(LN(z_{l-1}))+z_{l-1}$$</p>
            <p>$$z_l=MLP(LN(z_l'))+z_l'$$</p>
            <p>$$l=1...L$$</p>
            <div class="appendix-box">
                <h4>附录A Multihead Self-Attention</h4>
                <p>标准的“查询-键-值”(qkv)自注意力（SA, Vaswani et al., 2017）是神经网络架构中一个流行的构建模块。对于输入序列中的每个元素 \(z ∈ ℝ^{N×D}\)，我们计算序列中所有值 v 的加权和。注意力权重 \(A_{ij}\) 基于序列中两个元素及其各自的查询 \(q^i\) 和键 \(k^j\) 表示之间的成对相似度。</p>
                <p>$$[q,k,v]=zU_{qkv},\ U_{qkv}∈ ℝ^{D×3D_h}$$</p>
                <p>$$A=softmax(qk^T/\sqrt{D_h}),\ A∈ℝ^{N×N}$$</p>
                <p>$$SA(z)=Av$$</p>
                <p>多头自注意力（MSA）是SA的一种扩展，其中我们并行运行 k 个自注意力操作（称为“头”），并对它们拼接后的输出进行投影。为了在改变 k 时保持计算量和参数数量不变，\(D_h\)（公式5）通常被设置为 D/k。</p>
                <p>$$MSA(z)=[SA_1(z);SA_2(z);...SA_k(z)]U_{msa}$$</p>
                <p>$$U_{msa}∈ ℝ^{k×D_h×D}$$</p>
            </div>
            <p><strong>归纳偏置。</strong>我们注意到，Vision Transformer比CNN具有少得多的图像特异性归纳偏见。在CNN中，局部性、二维邻域结构和平移等变性被嵌入到模型的每一层中。而在ViT中，只有MLP层是局部和平移等变的，自注意力层则是全局的。二维邻域结构用得非常少：在模型开始时通过将图像切成块，以及在微调时为适应不同分辨率图像而调整位置嵌入时（如下所述）。除此之外，初始化时的位置嵌入不携带关于块的2D位置信息，所有块之间的空间关系都必须从头学习。</p>
            <p><strong>混合架构。</strong>作为原始图像块的替代方案，输入序列可以由CNN的特征图构成。在这种混合模型中，块嵌入投影E（公式1）应用于从CNN特征图中提取的块。在特殊情况下，块的空间大小可以是1x1，这意味着输入序列是通过简单地将特征图的空间维度展平并投影到Transformer维度来获得的。分类输入嵌入和位置嵌入如上所述添加。</p>

            <h4>3.2. Fine-Tuning and Higher Resolution</h4>
            <p>通常，我们在大数据集上预训练ViT，然后微调到（较小的）下游任务。为此，我们移除预训练的预测头，并附加一个零初始化的D×K前馈层，其中K是下游任务的类别数。以比预训练时更高的分辨率进行微调通常是有益的（Touvron et al., 2019; Kolesnikov et al., 2020）。当输入更高分辨率的图像时，我们保持块大小不变，这会导致更长的有效序列长度。Vision Transformer可以处理任意序列长度（受内存限制），但预训练的位置嵌入可能不再有意义。因此，我们根据它们在原始图像中的位置对预训练的位置嵌入进行2D插值。请注意，这种分辨率调整和块提取是ViT中唯一手动注入关于图像2D结构归纳偏见的地方。</p>
            
            <h3>4. Experiments</h3>
            <p>我们评估了ResNet、Vision Transformer（ViT）以及混合架构的表示学习能力。为了理解每个模型的数据需求，我们在不同规模的数据集上进行预训练，并评估了多个基准测试任务。当考虑到预训练的计算成本时，ViT的表现非常出色，在大多数识别基准上以更低的预训练成本达到了业界顶尖水平。最后，我们进行了一个使用自监督的小型实验，并表明自监督的ViT在未来有很大的潜力。</p>
            <h4>4.1. Setup</h4>
            <p><strong>数据集。</strong>为了探索模型的可扩展性，我们使用了包含1千个类别和130万张图像的ILSVRC-2012 ImageNet数据集（下文我们称之为ImageNet），其超集ImageNet-21k包含2.1万个类别和1400万张图像（Deng et al., 2009），以及包含1.8万个类别和3.03亿张高分辨率图像的JFT数据集（Sun et al., 2017）。我们遵循Kolesnikov等人（2020）的方法，对预训练数据集相对于下游任务的测试集进行了去重处理。我们将这些数据集上训练的模型迁移到几个基准测试任务上：带有原始验证标签的ImageNet和经过清理的ReaL标签（Beyer et al., 2020）、CIFAR-10/100（Krizhevsky, 2009）、Oxford-IIIT Pets（Parkhi et al., 2012）和Oxford Flowers-102（Nilsback & Zisserman, 2008）。对于这些数据集，预处理遵循Kolesnikov等人（2020）的方法。</p>
            <p>我们还在包含19个任务的VTAB分类套件上进行了评估（Zhai et al., 2019b）。VTAB评估的是向多样化任务的低数据量迁移能力，每个任务使用1000个训练样本。这些任务分为三组：自然 (Natural)——像上面提到的Pets、CIFAR等任务；专业 (Specialized)——医学和卫星图像；以及结构化 (Structured)——需要几何理解（如定位）的任务。</p>
            <p><strong>模型变体。</strong>我们基于用于BERT的模型配置来设置ViT的配置（Devlin et al., 2019），如表1所示。“Base”和“Large”模型直接从BERT中借鉴，我们还增加了一个更大的“Huge”模型。在下文中，我们使用简短的表示法来指明模型大小和输入块大小：例如，ViT-L/16表示使用16×16输入块大小的“Large”变体。注意，Transformer的序列长度与块大小的平方成反比，因此块大小较小的模型在计算上更昂贵。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/tab1.png" alt="Language-Table Performance">
                    <figcaption>表1：Vision Transformer模型变体的详细信息。</figcaption>
                </figure>
            </div>
            <p>对于基线CNN，我们使用ResNet（He et al., 2016），但将批归一化（Batch Normalization）层（Ioffe & Szegedy, 2015）替换为组归一化（Group Normalization）层（Wu & He, 2018），并使用了标准化的卷积（Qiao et al., 2019）。这些修改改善了迁移性能（Kolesnikov et al., 2020），我们将修改后的模型称为“ResNet (BiT)”。对于混合模型，我们将中间特征图输入到ViT中，块大小为一个“像素”。为了实验不同的序列长度，我们要么（i）取一个常规ResNet50的第4阶段的输出，要么（ii）移除第4阶段，在第3阶段放置相同数量的层（保持总层数不变），并取这个扩展后的第3阶段的输出。选项（ii）会产生一个4倍长的序列长度，和一个计算上更昂贵的ViT模型。</p>
            <p><strong>训练与微调。</strong>我们使用Adam（Kingma & Ba, 2015）优化器训练所有模型，包括ResNet，其中β₁=0.9，β₂=0.999，批大小为4096，并应用了0.1的高权重衰减，我们发现这对所有模型的迁移都很有用（附录D.1显示，与常见做法相反，在我们的设置中Adam比SGD对ResNet的效果略好）。我们使用线性的学习率预热（warmup）和衰减，详见附录B.1。对于微调，我们对所有模型使用带动量的SGD，批大小为512，详见附录B.1.1。</p>
            <div class="appendix-box">
                <h4>附录D.1 SGD VS. Adam for Resnets</h4>
                <p>ResNet通常使用SGD进行训练，我们使用Adam作为优化器是相当不寻常的。这里我们展示了促使我们做出这一选择的实验。具体来说，我们比较了使用SGD和Adam在JFT上预训练的两个ResNet——50x1和152x2——的微调性能。对于SGD，我们使用Kolesnikov等人（2020）推荐的超参数。结果呈现在表7中。在大多数数据集和平均性能上，Adam预训练优于SGD预训练。这证明了我们选择Adam作为在JFT上预训练ResNet的优化器的合理性。注意，绝对数值低于Kolesnikov等人（2020）报告的数值，因为我们只预训练了7个周期，而不是30个。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/tab7.png" alt="Language-Table Performance" class="img-large">
                        <figcaption>表7：使用Adam和SGD预训练的ResNet模型的微调性能</figcaption>
                    </figure>
                </div>
                <h4>附录B.1 Training</h4>
                <p>表3总结了我们不同模型的训练设置。我们发现在ImageNet上从头开始训练模型时，强正则化是关键。Dropout（如果使用）在除了qkv投影之外的每个密集层之后，以及在将位置嵌入添加到块嵌入之后直接应用。混合模型的训练设置与其ViT对应模型完全相同。最后，所有训练都是在224x224的分辨率下完成的。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/tab3.png" alt="Language-Table Performance">
                        <figcaption>表3：训练的超参数。所有模型均使用批量大小为4096进行训练，并采用10k步的学习率预热策略。对于ImageNet数据集，我们发现额外应用全局范数为1的梯度裁剪是有益的。训练分辨率为224。</figcaption>
                    </figure>
                </div>
                <h4>附录B.1.1 Fine-Tuning</h4>
                <p>我们使用动量为0.9的SGD对所有ViT模型进行微调。我们在学习率上进行了一个小的网格搜索，学习率范围见表4。为此，我们使用训练集中的小子集（Pets和Flowers为10%，CIFAR为2%，ImageNet为1%）作为开发集，并在剩余数据上进行训练。对于最终结果，我们在整个训练集上训练，并在相应的测试数据上进行评估。对于ResNet和混合模型的微调，我们使用完全相同的设置，唯一的例外是在ImageNet上，我们在学习率搜索中添加了另一个值0.06。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/tab4.png" alt="Language-Table Performance" class="img-large">
                        <figcaption>表4：微调的超参数。所有模型均采用余弦学习率衰减进行微调，批量大小为512，无权重衰减，并在全局范数为1处进行梯度裁剪。如未特别说明，微调分辨率为384。</figcaption>
                    </figure>
                </div>
            </div>
            <p>对于表2中的ImageNet结果，我们在更高的分辨率下进行微调：ViT-L/16为512，ViT-H/14为518，并且还使用了Polyak & Juditsky（1992）平均法，因子为0.9999（Ramachandran et al., 2019; Wang et al., 2020b）。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/tab2.png" alt="Language-Table Performance">
                    <figcaption>表2：与当前主流图像分类基准的对比。我们报告了在三个微调运行中平均得到的准确率的均值和标准差。在所有数据集上，使用JFT-300M数据集预训练的VisionTransformer模型均优于基于ResNet的基线模型，同时所需的预训练计算资源也显著减少。使用较小的公开ImageNet-21k数据集预训练的ViT模型同样表现良好。*Touvron等人（2020）报告的略微改进的88.5%结果。</figcaption>
                </figure>
            </div>
            <p><strong>指标 (Metrics)。</strong>我们通过小样本（few-shot）或微调准确率来报告在下游数据集上的结果。微调准确率捕捉了每个模型在相应数据集上微调后的性能。小样本准确率是通过解决一个正则化最小二乘回归问题获得的，该问题将一部分训练图像的（冻结）表示映射到\(\{-1, 1\}^K\)的目标向量。这种形式使我们能够以闭式解的形式恢复精确解。虽然我们主要关注微调性能，但我们有时会使用线性小样本准确率进行快速的即时评估，因为在这些情况下进行微调成本太高。</p>

            <h4>4.2. Comparison to State of the Art</h4>
            <p>我们首先将我们最大的模型——ViT-H/14和ViT-L/16——与文献中顶尖的CNN进行比较。第一个比较对象是Big Transfer (BiT)（Kolesnikov et al., 2020），它使用大型ResNet进行监督迁移学习。第二个是Noisy Student（Xie et al., 2020），这是一个在ImageNet和移除了标签的JFT-300M上使用半监督学习训练的大型EfficientNet。目前，Noisy Student在ImageNet上是业界顶尖，而BiT-L在本文报告的其他数据集上是顶尖。所有模型都在TPUv3硬件上进行了训练，我们报告了预训练它们所需的TPUv3-核心-天数，即用于训练的TPUv3核心数（每个芯片2个核心）乘以训练天数。</p>
            <p>表2显示了结果。在JFT-300M上预训练的较小的ViT-L/16模型在所有任务上都优于BiT-L（它也在同一数据集上预训练），同时训练所需的计算资源要少得多。更大的模型ViT-H/14进一步提升了性能，尤其是在更具挑战性的数据集上——ImageNet、CIFAR-100和VTAB套件。有趣的是，这个模型预训练所需的计算量仍然远少于之前的业界顶尖水平。然而，我们注意到预训练效率不仅受架构选择的影响，还受其他参数如训练计划、优化器、权重衰减等的影响。我们在4.4节中提供了不同架构在性能与计算成本方面的受控研究。最后，在较小的公开数据集ImageNet-21k上预训练的ViT-L/16模型在大多数数据集上也表现良好，同时预训练所需的资源更少：它可以使用一个标准的云TPUv3（8个核心）在大约30天内完成训练。</p>
            <p>图2分解了VTAB任务到它们各自的组，并与该基准上以前的SOTA方法进行比较：BiT，VIVI——一个在ImageNet和Youtube上共同训练的ResNet（Tschannen et al., 2020），以及S4L——在ImageNet上进行监督加半监督学习（Zhai et al., 2019a）。ViT-H/14在自然和结构化任务上优于BiT-R152x4和其他方法。在专业任务上，排名前两位的模型性能相似。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/fig2.png" alt="Language-Table Performance">
                    <figcaption>图2：VTAB在自然、专业和结构化任务组中的性能分解。</figcaption>
                </figure>
            </div>

            <h4>4.3. Pre-training Data Requirements</h4>
            <p>Vision Transformer在大型JFT-300M数据集上预训练时表现良好。与ResNet相比，它具有更少的视觉归纳偏见，那么数据集大小到底有多关键呢？我们进行了两组实验。</p>
            <p>首先，我们在规模递增的数据集上预训练ViT模型：ImageNet、ImageNet-21k和JFT-300M。为了提升在较小数据集上的性能，我们优化了三个基本的正则化参数——权重衰减、dropout和标签平滑。图3显示了微调到ImageNet后的结果（其他数据集上的结果见表5）²。当在最小的数据集ImageNet上预训练时，尽管有（适度的）正则化，ViT-Large模型的表现仍不如ViT-Base模型。在ImageNet-21k上预训练时，它们的性能相似。只有在JFT-300M上，我们才能看到更大模型的全部好处。图3还显示了不同大小的BiT模型所跨越的性能区域。BiT CNN在ImageNet上优于ViT，但在更大数据集上，ViT反超了。</p>
            <div class="appendix-box">
                <p>请注意，ImageNet预训练的模型也进行了微调，但是再次在ImageNet上。这是因为微调期间分辨率的增加会提高性能。</p>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/fig3.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图3：向ImageNet的迁移。当在小数据集上预训练时，大型ViT模型的表现比BiT ResNet（阴影区域）差，但当在更大数据集上预训练时，它们大放异彩。类似地，随着数据集的增长，较大的ViT变体也超越了较小的变体。</figcaption>
                </figure>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/tab5.png" alt="Language-Table Performance">
                    <figcaption>表5：在ImageNet、ImageNet-21k或JFT300M上预训练的Vision Transformer在各种数据集上的Top1准确率（百分比）。这些数值对应于正文图3。模型在384分辨率下进行微调。请注意，ImageNet的结果未使用额外的技术（Polyak平均和512分辨率图像）计算，这些技术用于实现表2中的结果。</figcaption>
                </figure>
            </div>
            <p>其次，我们在9M、30M和90M的随机子集以及完整的JFT-300M数据集上训练我们的模型。我们不对较小的子集进行额外的正则化，并对所有设置使用相同的超参数。这样，我们评估的是模型的内在属性，而不是正则化的效果。不过，我们确实使用了早停（early-stopping），并报告了训练期间达到的最佳验证准确率。为了节省计算，我们报告线性小样本准确率而不是完整的微调准确率。图4包含了结果。在较小的数据集上，与计算成本相当的ResNet相比，Vision Transformer更容易过拟合。例如，ViT-B/32比ResNet50稍快；它在9M子集上的表现要差得多，但在90M+子集上表现更好。ResNet152x2和ViT-L/16的情况也是如此。这一结果强化了这样一个直觉：卷积归纳偏见对于较小的数据集是有用的，但对于较大数据集，直接从数据中学习相关模式是充分的，甚至是有益的。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/fig4.png" alt="Language-Table Performance" class="img-large">
                    <figcaption>图4：在ImageNet上相对于预训练规模的线性小样本评估。ResNet在较小的预训练数据集上表现更好，但比ViT更早达到瓶颈，而ViT在更大的预训练数据上表现更好。ViT-b是ViT-B所有隐藏维度减半的版本。</figcaption>
                </figure>
            </div>
            <p>总的来说，ImageNet上的小样本结果（图4）以及VTAB上的低数据量结果（表2）对于极低数据量的迁移任务来说似乎很有希望。对ViT的小样本属性进行进一步分析是未来一个令人兴奋的研究方向。</p>

            <h4>4.4. Scaling Study</h4>
            <p>我们通过评估从JFT-300M迁移的性能，对不同模型进行了一项受控的扩展性研究。在这种设置下，数据大小不会成为模型性能的瓶颈，我们评估的是性能与每个模型预训练成本的关系。模型集包括：7个ResNet，R50x1、R50x2、R101x1、R152x1、R152x2，预训练7个周期，外加R152x2和R200x3预训练14个周期；6个Vision Transformer，ViT-B/32、B/16、L/32、L/16，预训练7个周期，外加L/16和H/14预训练14个周期；以及5个混合模型，R50+ViT-B/32、B/16、L/32、L/16，预训练7个周期，外加R50+ViT-L/16预训练14个周期（对于混合模型，模型名称末尾的数字代表ResNet主干网络中的总下采样率，而不是块大小）。</p>
            <p>图5包含了迁移性能与总预训练计算量的关系（关于计算成本的细节见附录D.5）。每个模型的详细结果在附录的表6中提供。可以观察到几个模式。首先，Vision Transformer在性能/计算权衡上主导了ResNet。ViT使用大约2-4倍少的计算量就能达到相同的性能（在5个数据集上平均）。其次，混合模型在小的计算预算下略微优于纯ViT，但对于更大的模型，这种差异消失了。这个结果有些出人意料，因为人们可能期望卷积局部特征处理在任何大小的ViT上都有帮助。第三，Vision Transformer在尝试的范围内似乎没有饱和，这为未来的扩展工作提供了动力。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/fig5.png" alt="Language-Table Performance">
                    <figcaption>图5：不同架构的性能与预训练计算量对比：Vision Transformer、ResNet和混合模型。Vision Transformer通常在相同的计算预算下优于ResNet。混合模型在较小的模型尺寸上改进了纯Transformer，但对于较大的模型，差距消失了。</figcaption>
                </figure>
            </div>
            <div class="appendix-box">
                <h4>附录D.5 Empirical Computational Costs</h4>
                <p>我们还对我们硬件上架构的实际速度感兴趣，这并不总能被理论FLOPs很好地预测，因为像通道宽度和缓存大小等细节会产生影响。为此，我们在TPUv3加速器上对感兴趣的主要模型进行了推理速度计时；推理和反向传播速度之间的差异是一个与模型无关的常数因子。</p>
                <p>图 12 (左) 显示了每个内核每秒可以处理多少张图像，这取决于不同的输入大小。 每个数据点都是在广泛的批量大小下测量的峰值性能。 如您所见，理论上的二次方平方扩展与图像大小相关联，仅在最大的模型中才刚刚开始出现在最大分辨率时。</p>
                <p>另一个感兴趣的量是每个模型可以放在一个核心上的最大批次大小，对于扩展到大型数据集来说，更大的批量大小更好。图 12 ( 右 ) 展示了同一组模型的这一指标。这表明在内存效率方面，大维特模型比残差网络模型有明显优势。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig12.png" alt="Language-Table Performance">
                        <figcaption>图12：左图：不同架构在各种输入尺寸下的实际运行时间。ViT模型的速度与类似的ResNet相当。右图：在不同架构和输入尺寸下，设备上可容纳的最大每核批量大小。显然，ViT模型具有更高的内存效率</figcaption>
                    </figure>
                </div>
            </div>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/tab6.png" alt="Language-Table Performance">
                    <figcaption>表6：模型规模扩展实验的详细结果。这些结果对应于主论文中的图5。我们展示了在多个数据集上的迁移准确率，以及预训练所需的计算量（以exa-FLOPs为单位）。</figcaption>
                </figure>
            </div>

            <h4>4.5. Inspecting Vision Transformer</h4>
            <p>为了开始理解Vision Transformer如何处理图像数据，我们分析了它的内部表示。Vision Transformer的第一层将展平的块线性投影到一个较低维的空间（公式1）。图7（左）显示了学习到的嵌入滤波器的顶层主成分。这些成分类似于用于对每个块内精细结构进行低维表示的合理基函数。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/fig7.png" alt="Language-Table Performance">
                    <figcaption>图7：左图：ViT-L/32模型中RGB值初始线性嵌入的滤波器。中图：ViT-L/32模型位置嵌入的相似性。方格显示了具有指定行和列的图像块的位置嵌入与其他所有图像块位置嵌入之间的余弦相似度。右图：不同网络深度和注意力头下被关注区域的大小。每个点表示在某一网络层上16个注意力头之一对所有图像的平均注意力距离。更多细节请参见附录D.7。</figcaption>
                </figure>
            </div>
            <p>投影之后，一个学习到的位置嵌入被添加到块表示中。图7（中）显示，模型学会了在位置嵌入的相似性中编码图像内的距离，即，位置上更近的块倾向于有更相似的位置嵌入。此外，行-列结构出现了；在同一行/列的块有相似的嵌入。最后，对于更大的网格，有时会出现正弦结构（附录D）。位置嵌入学会了表示2D图像拓扑，这解释了为什么手工制作的2D感知嵌入变体没有带来改进（附录D.4）。</p>
            <p>自注意力机制允许ViT即使在最底层也能整合整个图像的信息。我们研究了网络在多大程度上利用了这种能力。具体来说，我们根据注意力权重计算了信息被整合的平均图像空间距离（图7，右）。这个“注意力距离”类似于CNN中的感受野大小。我们发现，一些头在最底层就已经关注了大部分图像，这表明全局信息整合的能力确实被模型使用了。其他注意力头在底层一直保持着很小的注意力距离。这种高度局部化的注意力在应用了ResNet作为Transformer前置的混合模型中不那么明显（图7，右），这表明它可能起着与早期卷积层类似的功能。此外，注意力距离随着网络深度的增加而增加。总的来说，我们发现模型关注的是对于分类具有语义相关性的图像区域（图6）。</p>
            <div class="figure-caption">
                <figure>
                    <img src="./ViT_figs/fig6.png" alt="Language-Table Performance" class="img-small">
                    <figcaption>图6：从输出标记到输入空间的注意力代表性示例。详见附录D.7。</figcaption>
                </figure>
            </div>
            <div class="appendix-box">
                <h4>附录D.7 Attention Distance</h4>
                <p>为了理解ViT如何使用自注意力在整个图像上整合信息，我们分析了不同层级上注意力权重跨越的平均距离（图11）。这个“注意力距离”类似于CNN中的感受野大小。在较低层，不同头的平均注意力距离变化很大，一些头关注图像的大部分区域，而另一些则关注查询位置或其附近的小区域。随着深度的增加，所有头的注意力距离都会增加。在网络的后半部分，大多数头都会广泛地关注所有标记。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig11.png" alt="Language-Table Performance">
                        <figcaption>图11：由头和网络深度决定的注意力区域大小。通过计算128张示例图像的注意力距离，即对查询像素与所有其他像素之间的距离进行加权平均（权重为注意力权重）。每个点表示在某一层中16个头之一对所有图像的平均注意力距离。图像宽度为224像素。</figcaption>
                    </figure>
                </div>
            </div>

            <h4>4.6. Self-Supervision</h4>
            <p>Transformer在NLP任务中表现出令人印象深刻的性能。然而，它们的成功很大程度上不仅源于其出色的可扩展性，还源于大规模的自监督预训练（Devlin et al., 2019; Radford et al., 2018）。我们还对掩码块预测（masked patch prediction）进行了初步探索以用于自监督，模仿了BERT中使用的掩码语言建模任务。通过自监督预训练，我们较小的ViT-B/16模型在ImageNet上达到了79.9%的准确率，比从头训练显著提高了2%，但仍比监督预训练低4%。附录B.1.2包含更多细节。我们把对对比预训练（contrastive pre-training）的探索留给未来的工作（Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; Hénaff et al., 2020）。</p>
            <div class="appendix-box">
                <h4>附录B.1.2 Self-Supervision</h4>
                <p>我们采用掩码块预测（masked patch prediction）目标来进行初步的自监督实验。为此，我们破坏50%的块嵌入，方法是将其替换为一个可学习的[mask]嵌入（80%）、一个随机的其他块嵌入（10%）或保持原样（10%）。这个设置与Devlin等人（2019）用于语言的设置非常相似。最后，我们使用它们各自的块表示来预测每个被破坏块的3位平均颜色（即总共512种颜色）。</p>
                <p>我们在JFT上使用Adam优化器、基础学习率为2·10<sup>-4</sup>、10k步预热和余弦学习率衰减，对我们的自监督模型进行了1M步（约14个周期）的训练，批大小为4096。作为预训练的预测目标，我们尝试了以下设置：1）仅预测平均的3位颜色（即1个512类的预测），2）并行预测一个4×4下采样版本的16×16块的3位颜色（即16个512类的预测），3）使用L2损失对完整块进行回归（即对3个RGB通道的256个值进行回归）。令人惊讶的是，我们发现所有方法都效果不错，尽管L2略差。我们仅报告选项1）的最终结果，因为它显示出最好的小样本（few-shot）性能。我们也尝试了Devlin等人（2019）使用的15%的破坏率，但结果在我们的少样本指标上略差。</p>
                <p>最后，我们想指出，我们实例化的掩码块预测并不需要如此巨大的预训练量或像JFT这样的大型数据集，就能在ImageNet分类上带来类似的性能增益。也就是说，我们观察到在100k预训练步数后，下游性能的收益递减，并且在ImageNet上预训练时也看到了类似的增益。</p>
            </div>

            <h3>5. Conclusion</h3>
            <p>我们探索了将Transformer直接应用于图像识别。与以往在计算机视觉中使用自注意力的工作不同，除了初始的块提取步骤外，我们没有在架构中引入图像特异性的归纳偏见。相反，我们将图像解释为一系列块，并使用NLP中标准的Transformer编码器来处理它。这种简单而可扩展的策略，在与大规模数据集上的预训练相结合时，效果出奇地好。因此，Vision Transformer在许多图像分类数据集上达到或超过了现有技术的水平，同时预训练的成本相对较低。</p>
            <p>虽然这些初步结果令人鼓舞，但仍有许多挑战有待解决。一个挑战是将ViT应用于其他计算机视觉任务，如检测和分割。我们的结果，加上Carion等人（2020）的结果，表明了这一方法的前景。另一个挑战是继续探索自监督预训练方法。我们的初步实验显示了自监督预训练带来的改进，但在自监督和大规模监督预训练之间仍然存在很大差距。最后，进一步扩展ViT的规模可能会带来性能的提升。</p>
            
            <div class="appendix-box">
                <h4>附录D.2 Transformer Shape</h4>
                <p>我们对扩展Transformer架构的不同维度进行了消融研究，以找出哪些维度最适合扩展到非常大的模型。图8显示了不同配置在ImageNet上的5-shot性能。所有配置都基于一个具有8层、\(D=1024、D_{MLP}=2048\)和32的块大小的ViT模型，这是所有曲线的交点。我们可以看到，扩展深度带来了最大的改进，在64层之前都清晰可见。然而，在16层之后已经可以看到收益递减。有趣的是，扩展网络宽度似乎带来的变化最小。减小块大小从而增加有效序列长度，显示出无需引入新参数的惊人稳健改进。这些发现表明，计算量可能是比参数数量更好的性能预测指标，并且如果可能的话，扩展应优先考虑深度而非宽度。总的来说，我们发现按比例扩展所有维度会带来稳健的改进。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig8.png" alt="Language-Table Performance">
                        <figcaption>图8：视觉Transformer不同模型尺寸的缩放。</figcaption>
                    </figure>
                </div>
                <h4>附录D.3 Head Type and Class Token</h4>
                <p>为了尽可能接近原始的Transformer模型，我们使用了一个额外的[class]标记，它被作为图像表示。这个标记的输出然后通过一个带有tanh非线性单隐藏层的小型多层感知器（MLP）转换为类别预测。</p>
                <p>这个设计继承自文本的Transformer模型，我们在整个主论文中都使用它。最初尝试只使用图像块嵌入，对其进行全局平均池化（GAP），然后接一个线性分类器——就像ResNet的最终特征图一样——效果非常差。然而，我们发现这既不是因为额外的标记，也不是因为GAP操作。相反，性能差异完全可以用对不同学习率的需求来解释，参见图9。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig9.png" alt="Language-Table Performance">
                        <figcaption>图9：类标记（class-token）与全局平均池化分类器的比较。两者表现相似，但需要不同的学习率。</figcaption>
                    </figure>
                </div>
                <h4>附录D.6 Axial Attention</h4>
                <p>轴向注意力是一种简单而有效的技术，用于在组织为多维张量的大型输入上运行自注意力。其基本思想是执行多个注意力操作，每个操作沿着输入张量的一个轴进行，而不是在展平版本的输入上应用一维注意力。我们修改了ViT以二维形状处理输入，并加入了轴向Transformer块。我们还实现了AxialResNet作为基线模型。如图13所示，Axial-ViT模型在性能上优于其ViT-B对应模型，但计算成本更高。对于AxialResNet，尽管在准确性/计算权衡方面看起来合理，但其朴素实现在TPU上非常慢。</p>
                <div class="appendix-box">我们的实现基于 https://github.com/csrhddlam/axial-deeplab 中开源的 PyTorch 实现。在我们的实验中，我们重现了（Wang 等人，2020b）报告的准确率分数，然而，我们的实现与开源实现类似，在 TPU 上运行速度非常慢。因此，我们无法将其用于大规模的深入实验。这些可能通过精心优化的实现来解决。</div>
                <p>此外，我们修改了 ViT 以处理二维形状的输入，而不是一维的补丁序列，并且在其中加入了轴向变压器块，在这里我们有一个行自注意力加上一个MLP，然后是一个列自注意力加上一个MLP。</p>
                <p>图13展示了在 ImageNet 5shot linear 上，使用 JFT 数据集预训练时，Axial ResNet、Axial-ViT-B/32 和 Axial-ViT-B/16 的性能，以及计算成本（浮点运算次数和推理时间）。如我们所见，Axial-ViT-B/32 和 Axial-ViT-B/16 在性能上都优于其 ViT-B 对应模型，但代价是更多计算成本。这是因为，在轴对齐 ViT 模型中，每个带有全局自注意的 Transformer 块被两个轴对齐 Transformer 块所取代，一个是行自注意，另一个是列自注意，尽管在轴对齐情况下，自注意力作用于的序列长度较小，但每个轴对齐 ViT 块都有一个额外的全连接层 MLP。对于轴对齐 ResNet，虽然从精度/计算量权衡的角度来看似乎合理（图 13 左），但在 TPU 上，朴素实现非常缓慢（图 13 右）。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig13.png" alt="Language-Table Performance">
                        <figcaption>图13：基于轴向注意力（Axial-Attention）模型的性能，以ImageNet5-shot线性分类任务中的top-1准确率表示，并与它们的速度指标（以浮点运算次数FLOPs表示，左图）和推理时间（右图）进行对比。</figcaption>
                    </figure>
                </div>
                <h4>附录D.8 Attention Maps</h4>
                <p>为了计算注意力图，即将输出标记映射到输入空间（见图6和14），我们使用了Attention Rollout (Abnar & Zuidema, 2020)。简而言之，我们在所有头中平均 ViT-L/16 的注意力权重，然后递归地乘以所有层的权重矩阵。这解释了通过所有层对注意的混合。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/fig14.png" alt="Language-Table Performance">
                        <figcaption>图14：进一步示例注意力图，如图6所示（随机选择）。</figcaption>
                    </figure>
                </div>
                <h4>附录D.9 Objectnet Results</h4>
                <p>我们还在ObjectNet基准上评估了我们的旗舰模型ViT-H/14，遵循Kolesnikov等人（2020）的评估设置，取得了82.1%的top-5准确率和61.7%的top-1准确率。</p>
                <he>附录D.10 VTAB Breakdown</he>
                <p>表9显示了在VTAB-1k各项任务上取得的分数。</p>
                <div class="figure-caption">
                    <figure>
                        <img src="./ViT_figs/tab9.png" alt="Language-Table Performance">
                        <figcaption>表9：VTAB-1k在各任务上的性能分解。</figcaption>
                    </figure>
                </div>
            </div>
        
        </section>

        <!-- ====================================================================== -->
        <!-- PART 3: 论文重点代码/实现解析 (Code & Implementation Analysis)         -->
        <!-- ====================================================================== -->
        <!-- 这是我为你解析的部分。未来你可以将自己的代码分析放在这里。 -->
        <section id="code-analysis">
            <h2>论文重点实现解析</h2>
            <p>本文是对哈佛NLP团队实现的<a href="https://github.com/harvardnlp/annotated-transformer">Pytorch版Transformer</a>的源码解析。</p>
            <div class="code-analysis">
                <h4>LayerNorm</h4>
                <pre><code class="python">
class LayerNorm(nn.Module):
    "Construct a layernorm module (See citation for details)."

    def __init__(self, features, eps=1e-6):
        """
        features: d_model 512||1024
        """
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
                </code></pre>

                <a name="Attention"><h4>Attention</h4></a>
                <pre><code class="python">
def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
                </code></pre>

                <a name="Multi-Head Attention"><h4>Multi-Head Attention</h4></a>
                <pre><code class="python">
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h # 64
        self.h = h # 8
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = [
            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linears, (query, key, value))
        ]

        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(
            query, key, value, mask=mask, dropout=self.dropout
        )

        # 3) "Concat" using a view and apply a final linear.
        x = (
            x.transpose(1, 2)
            .contiguous()
            .view(nbatches, -1, self.h * self.d_k)
        )
        del query
        del key
        del value
        return self.linears[-1](x)
                </code></pre>

                <a name="Position-wise Feed-Forward Networks"><h4>Position-wise Feed-Forward Networks</h4></a>
                <pre><code class="python">
class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."

    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        d_model = 512
        d_ff = 2048
        """
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(self.w_1(x).relu()))
                </code></pre>

                <a name="Embeddings and Softmax"><h4>Embeddings and Softmax</h4></a>
                <pre><code class="python">
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model # 512

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
                </code></pre>
            <div class="appendix-box">
                <h4>嵌入机制的举例说明</h4>
                <pre><code class="python">
# 分词
word = 'Hi, 你好~'
tokens = tokenizer.tokenize(word)
print(f'{word} 分词: {tokens}')
# 输出：Hi, 你好~ 分词: ['Hi', ',', 'Ġ', 'ä½łå¥½', '~']

token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(f'{word} Token IDs: {token_ids}')
# 输出：Hi, 你好~ Token IDs: [13048, 11, 220, 108386, 93]

# 获取该单词的嵌入向量
word_embedding = embeddings.weight[token_ids]
print(f'{word} 的嵌入形状：{word_embedding.shape}')
# 输出：Hi, 你好~ 的嵌入形状：torch.Size([5, 1536])

print(f'{word} 的嵌入内容: {word_embedding}')
# 输出：
# Hi, 你好~ 的嵌入内容: tensor([[ 0.0261,  0.0048, -0.0043,  ...,  0.0193, -0.0493, -0.0020],
#         [-0.0303, -0.0159, -0.0107,  ..., -0.0198, -0.0020, -0.0129],
#         [-0.0236, -0.0254,  0.0325,  ..., -0.0317, -0.0082,  0.0137],
#         [ 0.0270,  0.0042,  0.0014,  ...,  0.0425, -0.0195,  0.0011],
#         [-0.0205, -0.0408, -0.0013,  ...,  0.0272, -0.0060,  0.0032]],
#        dtype=torch.bfloat16, grad_fn=<\IndexBackward0>)
                </code></pre>
                </div>

                <a name="Positional Encoding"><h4>Positional Encoding</h4></a>
                <pre><code class="python">
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term) # 偶数索引列填充
        pe[:, 1::2] = torch.cos(position * div_term) # 奇数索引列填充
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)
                </code></pre>

            </div>
        </section>

    </div>

</body>
</html>